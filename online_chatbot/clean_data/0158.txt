1 Hardening Random Forest Cyber Detectors Against Adversarial Attacks Giovanni Apruzzese, Mauro Andreolini, Michele Colajanni, Mirco Marchetti Department of Engineering ‚ÄúEnzo Ferrari‚Äù University of Modena and Reggio Emilia Modena, Italy {giovanni.apruzzese, mauro.andreolini, michele.colajanni, mirco.marchetti}@unimore.it ¬©2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Abstract‚ÄîMachine learning algorithms are effective in several applications, but they are not as much successful when applied to intrusion detection in cyber security. Due to the high sensitivity to their training data, cyber detectors based on machine learning are vulnerable to targeted adversarial attacks that involve the perturbation of initial samples. Existing defenses assume unrealistic scenarios; their results are underwhelming in non-adversarial set- tings; or they can be applied only to machine learning algorithms that perform poorly for cyber security. We present an original methodology for countering adversarial perturbations targeting intrusion detection systems based on random forests. As a practical application, we integrate the proposed defense method in a cyber detector analyzing network trafÔ¨Åc. The experimental results on millions of labelled network Ô¨Çows show that the new detector has a twofold value: it outperforms state-of-the-art detectors that are subject to adversarial attacks; it exhibits robust results both in adversarial and non-adversarial scenarios. Index Terms‚ÄîAdversarial samples, machine learning, random forest, intrusion detection, Ô¨Çow inspection, botnet I. INTRODUCTION T HE adoption of machine learning to support se- curity operators is an inevitable trend because of the continuous increment of network trafÔ¨Åc and so- phistication of the attacks [1]‚Äì[3]. Machine learning algorithms are employed with success in an increasing number of areas including image processing, speech and text recognition, social media marketing [4] and, more recently, in cyber security. Indeed, modern Network Intrusion Detection Systems (NIDS) are being increas- ingly enriched with machine learning (e.g., [1], [5], [6]) and deep learning algorithms (e.g., [6]‚Äì[8]). Even some commercial products (e.g. Darktrace or DragonÔ¨Çy Threat Sensor) integrate detectors based on machine learning. Despite these positive achievements, recent literature (e.g., [9]‚Äì[12]) highlights that existing machine learning techniques are vulnerable to the so called adversarial attacks. These malicious actions involve the production of samples designed to thwart the machine learning algorithm by inducing outputs favorable to the attacker. Similar vulnerabilities are critical in the cyber security domain because any undetected attack may compromise an entire organization. The problem of adversarial attacks against machine learning detectors is a relevant open issue. We propose a novel approach for hardening cyber detectors based on machine learning. We focus on the random forest algorithm due to its proven effectiveness for intrusion detection [13]‚Äì[18]; however, recent studies also highlight its vulnerability to adversarial perturba- tions [19]‚Äì[21]. Our solution is based on the observation that existing machine learning cyber detectors rely on excessively rigid classiÔ¨Åcation criteria: they are typically trained through class labels that separate samples in disjointed categories where each sample may be either malicious or benign. A similar approach cannot work in the cyber domain where each sample may present more vague attributes. For this reason, we leverage the idea of introducing some degree of Ô¨Çexibility in the training data set by using probability labels. The intuition is that a model that uses probability labels instead of hard class labels can be more resilient to adversarial perturbations, and can achieve comparable or even superior results even in the absence of attacks. Our methodology has several applications in all fuzzy scenarios characterizing cyber security that involve classiÔ¨Åers based on random forests. As a Ô¨Årst test case, in this paper we adopt it for devising botnet detectors based on network Ô¨Çows analyzers. We validate our approach through a large set of experiments, performed on a set of publicly available and labelled trafÔ¨Åc traces containing over 20 million network Ô¨Çows with benign and malicious samples of different malware families. These data sets capture the network behavior of medium-large enterprises and represent an appropriate setting for a realistic evaluation. The experi- mental results demonstrate that the proposed solution de- arXiv:1912.03790v1 [cs.CR] 9 Dec 2019 2 vises a detector with comparable or superior performance than state-of-the-art methods in scenarios that are not subject to adversarial attacks. Moreover, it signiÔ¨Åcantly improves the robustness of random forest models against adversarial attacks. Achieving both results is a fundamen- tal success for real contexts where we cannot anticipate whether a machine learning detector will be subject or not to adversarial attacks. Our promising results have room for further improvements, but we are conÔ¨Ådent that this paper represents a Ô¨Årst important step towards more robust cyber defensive platforms based on machine learning against adversarial attacks. The remainder of this paper is structured as follows. Section II introduces adversarial attacks and compares our paper against related work. Section III describes the proposed method. Section IV illustrates the scenario and the threat model considered in this paper. Section V presents the methodology and testbeds used for perfor- mance evaluation. Section VI discusses the experimental results. Section VII concludes the paper with some Ô¨Ånal remarks and possible extensions of this work. II. RELATED WORK The complexity of network attacks and the augment of daily trafÔ¨Åc requires security operators to rely on some machine learning support [1], [2]. These methods may detect anomalies and may even reveal attack vari- ants that are not recognizable through signature-based approaches [5], [22]. However, the success of novel defensive methods also induce the formulation of new offensive strategies. Today, the so called adversarial attacks represent a major limitation to the adoption of a fully autonomous cyber defence platform. We describe the main characteristics of adversarial attacks, and then compare our proposal with the state-of-the-art. Adversarial attacks are based on the generation of speciÔ¨Åc samples that induce a machine learning model to produce an output that is favorable to the attacker. This result is caused by the intrinsic sensitivity of machine learning models to their internal conÔ¨Åguration settings [1], [23], [24]. Early examples of adversarial attacks against spam Ô¨Åltering are proposed in [25]‚Äì[27]. These papers show that linear classiÔ¨Åers could be tricked by few carefully crafted changes in the text of spam emails without affecting the readability of the spam mes- sage. Another interesting example of adversarial attack against neural networks classiÔ¨Åers for image processing is presented in [28], where imperceptible perturbations to images used in the training phase can modify arbitrarily the model‚Äôs output. Adversarial attacks can be classiÔ¨Åed through the taxonomy inspired by [29] that considers the following two properties. InÔ¨Çuence determines whether an attack is performed at training-time or test-time. ‚Ä¢ Training-time: these attacks, also known as poison- ing attacks, manipulate the training dataset through the insertion or removal of speciÔ¨Åc samples, there- fore altering the decisions of the trained model. ‚Ä¢ Test-time: these attacks subvert the behavior of the detector through the injection of speciÔ¨Åc samples during its operational phase. Violation denotes the type of security violation, which can affect the availability or integrity of the system. ‚Ä¢ Integrity: often referred to as evasion attacks, the goal is increasing the false negative rate of the model by introducing malicious samples that are classiÔ¨Åed as benign. ‚Ä¢ Availability: these attacks tend to cause overwhelm- ing spikes of false alarms, inducing temporary shut- downs and/or recalibrations of the detector. There is extensive literature on adversarial perturba- tions against image processing (e.g., [9], [11], [12], [30]), while few papers consider adversarial attacks from a cyber security perspective (e.g., [18], [19], [21], [29], [31]). Several recent results demonstrate that adversarial attacks can represent a dangerous threat to any defen- sive system based on machine learning. For example, [10] and [32] consider the case of adversarial samples against PDF malware detectors based on Support Vector Machines (SVM), neural networks, and random forests. Other papers [21], [33], [34] highlight the problem of adversarial evasion for Android malware and spam detectors. Furthermore, the capability of a Generative Adversarial Network to thwart a Domain Generation Algorithm detector based on random forests is evaluated in [35]. More recently, [20] shows the fragility of a Ô¨Çow-based botnet detector relying on random forest against small adversarial perturbations. Although the threats posed by adversarial inputs are clear, the few existing solutions are not immediately applicable to real contexts. For example, [35] proposes to harden the classiÔ¨Åer through multiple re-training steps based on adversarial samples. This is an interesting theoretic solution with practical limitations because it requires the creation and continuous management of datasets with realistic adversarial samples. Moreover, [31] suggests to improve the robustness against evasion attacks by not considering the features that can be manipulated by an attacker. The problem of this approach is that it reduces accuracy in normal scenarios as shown in [18], [36]. On the other hand, our proposal is immediately applicable to real contexts as demonstrated by multiple experimental settings. 3 Defensive distillation may work in mitigating adver- sarial perturbations against image classiÔ¨Åcation [37], but this technique is built and evaluated only on neural network algorithms [38]. Although cyber detectors based on this algorithm exist and can be hardened through the original distillation proposal [39], in cybersecurity scenarios detectors based on random forests outperform those relying on neural networks and other supervised methods [13], [15]‚Äì[18], [40]. More recently, [41] eval- uates different classiÔ¨Åers for the speciÔ¨Åc problem of botnet detection and conÔ¨Årms that random forest yields the best results. Finally, [42] proposes a NIDS that inspects network Ô¨Çows through a random forest classiÔ¨Åer to identify botnets and obtains outstanding results with detection rates close to 0.99. For this reason, we devise an original formulation of the distillation technique that is speciÔ¨Åcally aimed at hardening random forest detec- tors, thus allowing to devise robust defensive schemes for cyber detection based on machine learning. Although a recent work [43] shows that it is possible to evade the defensive distillation, we observe that the considered threat model is unrealistic because it assumes an attacker with complete control of the detector: with similar privi- leges, attackers can (and most likely will) adopt measures much more invasive and disruptive than those based on adversarial perturbations. Other works on defenses against adversarial samples [44], [45] consider just SVM classiÔ¨Åers applied to malware analysis, which is out of the scope of this paper. We are not aware of other defen- sive mechanisms against evasion adversarial attacks that are applicable to random forest algorithms for network intrusion detection. Hence, we can conclude that the topic considered in this paper is a promising research theme, which we address through a novel approach that hardens random forest-based detectors through an original defensive distillation method. III. PROPOSED METHOD We propose a novel method that hardens machine learning detectors based on random forest against ad- versarial attacks. The idea comes from the observation that the excessively rigid classiÔ¨Åcation criteria learned by machine learning algorithms in the training phase are vulnerable to subtle adversarial perturbations. Indeed, existing detectors are trained through class labels that separate samples in disjointed categories where each sample may be either malicious or benign but not both. On the other hand, the cyber domain is more fuzzy, and a sample may present characteristics belonging to different categories. Any rigid classiÔ¨Åcation produced by hard class labels may represent an exploitable weakness of cyber detectors in adversarial settings. For this reason, we aim to introduce some degree of Ô¨Çexibility and uncer- tainty in the training process by using probability labels that allow the algorithm to capture additional information between classes such as similarity. The intuition is that a model that uses probability labels instead of hard class labels can be more resilient to adversarial samples, and can achieve comparable or superior results even in the absence of attacks. The main difÔ¨Åculty of a similar ap- proach is that probability labels are not readily available in the cyber domain; hence we devise an original solution built upon the two following phases: 1) generation of probability labels from hard class labels; 2) deployment of a supervised model trained with the generated probability labels to perform the cyber detection. Fig. 1 shows that this approach considers as its input a dataset and its class labels. Then, it computes the corre- sponding probability labels (represented in the leftmost box), and uses them to train a supervised model that will be integrated in the detector. We apply this method to the random forest machine learning algorithm by leveraging the foundations [46] of the defensive distillation for neu- ral networks [37]. By using the information encoded in the probability labels in the form of probability vectors, generated after training an initial model, it is possible to develop a second ‚Äúdistilled‚Äù model that is more robust against adversarial attacks. The entire workÔ¨Çow applied to the random forest algorithm is illustrated in Fig. 2 where each step is denoted by a circled number that is explained in the following subsections. Unlike the original defensive distillation technique, the generation of probability labels and their use for detection is performed through random forest-based models instead of neural networks. A. Generation of the probability labels The initial phase is performed through a random forest classiÔ¨Åer, the Condenser, denoted by C. We Ô¨Årst train this classiÔ¨Åer (step 1 in Fig. 2). Then, we leverage the intrinsic property of the random forest algorithm of being an ensemble method, that is, a composition of several decision trees (or estimators), where the Ô¨Ånal output is generated after evaluating the response of each individual tree. This characteristic allows us to produce the desired probability vectors by considering the percentage of estimators that predicted a speciÔ¨Åc result (step 2 in Fig. 2). Formally, let X be a dataset, |X| ‚ààN the number of samples that constitute X, and xi ‚ààX(0 ‚â§i ‚â§|X|) a sample within this dataset; let Y be the set of hard class labels (in the form of indicator vectors) associated 4 Figure 1: The two phases of the cyber detector. Figure 2: WorkÔ¨Çow of the algorithm where distillation is applied to the random forest algorithm. to dataset X, and yi ‚ààY the label associated to xi. If C is a random forest classiÔ¨Åer, then |C| ‚ààN is the number of estimators that compose C, and tj ‚ààC(0 ‚â§j ‚â§|C|) is a tree of classiÔ¨Åer C. After training C by means of X (as training dataset) and of Y (as labels), the set of probability labels Y ‚Ä≤ that can be obtained from X through C is: Y ‚Ä≤ =  y‚Ä≤ i | y‚Ä≤ i = P|C| j=1 ti j |C|  , (1) where y‚Ä≤ i is the probability vector corresponding to sample xi, and ti j denotes the output of tree tj for sample xi, which is an indicator vector. As an example, let us consider a random forest classiÔ¨Åer consisting of 100 estimators that are trained to solve a binary classiÔ¨Åcation problem (either 0 or 1). Now, let us assume that, for a given sample, 31 estimators predict 0 and produce the indicator vector (1, 0), while the remaining 69 predict 1 and produce the indicator vector (0, 1). In this case, although the Ô¨Ånal output of the classiÔ¨Åer is the indicator vector (0, 1), we generate the binary probability vector (0.31, 0.69) which encodes the output produced by each individual tree. On the other hand, if 69 estimators predict 0 and 31 estimators predict 1, we would obtain the probability vector (0.69, 0.31). It should be noted that the objective of the Condenser is to generate accurate probability labels but it does not perform detection. As the focus is on the prediction of every individual estimator, and not on the classiÔ¨Åcation results of the whole random forest classiÔ¨Åer, the concept of ‚ÄúmisclassiÔ¨Åcation‚Äù does not strictly apply to this phase. For example, let us consider a binary classiÔ¨Åcation scenario where we train the Condenser and then test it to generate the probability labels: it may be possible that, for a sample associated to the label 1, 69% of the estima- tors of the Condenser predict a 0. This event cannot be considered a misclassiÔ¨Åcation because the output of the Condenser is a probability (e.g., the probability vector (0.69, 0.31)). However, such occurrences may have a detrimental effect in the next phase. To minimize similar risks, we utilize the entire available dataset to both train and test C: this approach would yield the best results as it ensures that each sample is associated to a probability label with the highest degree of conÔ¨Ådence. B. Model deployment In the second phase, the probability vectors generated by the Condenser (step 3 in Fig. 2) are used as training labels for a random forest regressor that uses those probabilities as its training input (step 4 in Fig. 2). We deÔ¨Åne this model as the Receiver denoted by R. Since this model performs the actual detection tasks (step 5 in Fig. 2), we evaluate it against the adversarial inputs. Hence, it is important that this model is trained by following the best practices (as in [24]) to avoid the risk of overÔ¨Åtting. For example, the training and validation sets should be chosen through appropriate splits of the available dataset. We remark that the Receiver can be seen as a complex multi-output regressor with the challenging task of multi- target regression [47]. However, for the speciÔ¨Åc scenarios related to cyber detection, it is possible to devise a simpler regressor because the main goal is to analyze network trafÔ¨Åc and to identify illegitimate activities. Hence, we can model the case as a binary classiÔ¨Åcation instead of a multi-class problem, in which the algorithm 5 is required to determine only whether a given sample of trafÔ¨Åc is malicious or not. To this purpose, for each data sample, the Condenser needs to generate a single probability value (denoting the likelihood of being a malicious sample) instead of a multi-dimensional prob- ability vector. By considering the binary classiÔ¨Åcation example described in Section III-A, the 31 estimators of the Condenser that predicted a 0 would give the value 0, while the remaining 69 estimators would produce the value 1. Thus, the corresponding probability value for the analyzed sample is 0.69. These probability values are then used as the labels for the Receiver, whose output is another probability value that can be converted into a discrete number through a rounding operation: P(xi) = ‚åäRxi‚åâ, (2) where Rxi is the output of the Receiver R for the sample xi, and P(xi) ‚àà[0, 1] denotes the Ô¨Ånal prediction of the distilled model. IV. APPLICATION SCENARIO FOR THE DETECTOR A realistic scenario where the proposed detector can be applied successfully is represented in Fig. 3, which shows a large enterprise network with many internal hosts and a border router connected to a network Ô¨Çow exporter. The generated Ô¨Çows are inspected by a network intrusion detection system based on machine learning that aims to identify malicious activities (e.g., botnet) by leveraging the random forest algorithm. We assume that an attacker has already established a foothold in the internal network by compromising one or more machines and deploying botnet malware that communicate with a Command and Control (CnC) infrastructure. The attacker model can be described accordingly to the four characteristics described in [10]: goal, knowledge, capabilities, strategy. Figure 3: Example of network considered in our use-case. The main goal of the attacker is to evade detection so that he can maintain access to the internal net- work, compromise more machines and gather informa- tion about adopted defenses [48]. He knows that network communications are monitored by a NIDS based on machine learning. We assume that the attacker can issue commands to the bot through the CnC infrastructure, possibly modifying the underlying network behavior, but he cannot interact with the detector. Although the attacker does not know the speciÔ¨Åc machine learning algorithm (alongside its parameters and features) used by the NIDS, he can easily guess that the detector is trained over a dataset containing malicious Ô¨Çows generated by the same or a similar malware variant deployed on the infected machines. Hence, he has to devise some countermeasure to evade the botnet detector. The strategy to avoid detection is through a targeted exploratory integrity attack [10] that is performed by inserting tiny modiÔ¨Åcations in the communications be- tween the bot and its CnC server. These alterations may include slight increases of Ô¨Çow duration, exchanged bytes and exchanged packets. Similar changes can be applied without interfering with the application logic of the bot that can continue to operate as initially designed by the attacker. In such a way, the detector is induced to misclas- sify the network Ô¨Çows generated by bot communications despite being trained with malicious samples belonging to the botnet variant employed by the attacker. V. EVALUATION METHODOLOGY The evaluation and comparison of machine-based de- tectors subject to adversarial attacks is a complex pro- cedure. In this section, we describe the methodology of our evaluation by presenting the experimental testbed, the details of the considered random forest models, and the procedure to generate the adversarial samples. A. Experimental testbed The experimental evaluation considered in our paper is performed on a public collection of multiple datasets, known as ‚ÄúCTU-13‚Äù [49]. The CTU-13 includes net- work data captured at the Czech Technical University in Prague, and contains labelled network trafÔ¨Åc generated by various botnet variants and mixed with normal and background trafÔ¨Åc. These Ô¨Çows are captured in a network environment with hundreds of hosts, while the malicious trafÔ¨Åc is generated by infecting machines with mal- ware related to several botnet families [49]. Overall, the CTU-13 contains 13 distinct datasets of different botnet activity; each dataset refers to one botnet variant of the 6 considered families: Neris, Rbot, Virut, Menti, Murlo, NSIS.ay. We report the meaningful metrics of each dataset in the CTU-13 collection in Table I, which also includes the botnet-speciÔ¨Åc piece of malware and the number of infected machines. This Table highlights the massive amount of included data, which can easily represent the network behavior of a medium-to-large 6 real organization. Nevertheless, we remark that in our evaluation, we prefer not to consider the Sogou botnet because of the limited amount of its malicious samples. To generate each dataset, the authors Ô¨Årst capture the network data in speciÔ¨Åc packet-capture (PCAP) Ô¨Åles, and then convert them into network Ô¨Çows. A network Ô¨Çow (or netÔ¨Çow) is essentially a sequence of records, each one summarizing a connection between two endpoints (that is, IP addresses). The inspection of network Ô¨Çows allows administrators to easily summarize the information of two endpoints, such as the source and destination of trafÔ¨Åc, the class of service, and the size of transmitted data. Network Ô¨Çows are of particular interest for cyber se- curity applications because of the following beneÔ¨Åts with respect to full packet captures: lower amount of storage space required; faster analyses; reduced privacy concerns due to the absence of packet-speciÔ¨Åc payloads [50]. The authors of the CTU-13 convert the raw network packets into network Ô¨Çows by means of Argus, a network audit system. Argus presents a client-server architecture: the server component processes packets (either PCAP Ô¨Åles or live packet data) and generates detailed status reports of all the netÔ¨Çows in the packet stream, which are then provided to the dedicated clients. By inspecting the CTU-13, we can assume that the client used by the authors to extract the netÔ¨Çows from each individual PCAP Ô¨Åle is ra. The output of this conversion process is a CSV Ô¨Åle. The Ô¨Ånal step is the labeling of each individual network Ô¨Çow: indeed, the authors provide an additional ‚ÄúLabel‚Äù Ô¨Åeld, which separates legitimate from illegitimate Ô¨Çows. More speciÔ¨Åcally, benign Ô¨Çows cor- respond to the normal and background labels; whereas the botnet and CnC-channel labels denote malicious samples. B. Considered detectors For the evaluation we consider the following detectors based on random forest: ‚Ä¢ The Undistilled detector, which presents character- istics similar to the random forest classiÔ¨Åer model proposed in [51], is used as the baseline for the experiments; a graphical representation of its archi- tecture is provided in Fig. 4. ‚Ä¢ The Distilled detector represents the main proposal of this paper. It consists of the Condenser for gen- erating the probability labels, and of the Receiver to perform the detection tasks. This detector is evalu- ated against the Undistilled detector in adversarial and non-adversarial settings. Each detector has 6 instances, each one focusing on recognizing a speciÔ¨Åc malware family of the dataset. Figure 4: Architecture of the Undistilled detector. The motivation for this design choice comes from the observation that machine learning techniques yield supe- rior results when they pursue a speciÔ¨Åc goal rather than aiming to an impossible catch-all solution [6], [51]. For each botnet variant, we generate a dedicated train- ing set containing both benign and malicious samples belonging to that family; all instances share the same legitimate-to-illegitimate Ô¨Çow ratio in the training sets. Formally, let D be the set of all the traces of network Ô¨Çows considered in the testbed, and let Dl ‚äÇD and Dm ‚äÇD be the sets of all legitimate and malicious samples in D, respectively (so that Dl ‚à™Dm = D, and Dl ‚à©Dm = ‚àÖ). Now, let Db be the set of malicious Ô¨Çows corresponding to the b botnet family, so that S6 b=1 Db = Dm. We train each detector‚Äôs instance cor- responding to the b botnet family with samples randomly extracted from Dl and Db, in a 20 : 1 ratio. (The randomized extraction of samples is done to reduce the impact of selection bias.) The 20 : 1 ratio is similar to that in [15], and it is motivated by the fact that in realistic settings the legitimate Ô¨Çows largely outnumber the botnet-generated Ô¨Çows. Other studies use even greater ratios [52]. The instances of the Receiver are trained with 80% of the botnet Ô¨Çows generated by each malware variant, and validated on the remaining 20%. These splits are close to those adopted in [15], [20]. On the other hand, the instances of the Condenser, which generate the probability labels, are trained and tested on the same dataset containing all the malicious Ô¨Çows of the related botnet family. Other details are presented in Section III. These models adopt feature sets that are similar to those adopted in [20] and [51] because they achieve appreciable detection rates. We integrate these features with information about the IANA port type for the source and destination hosts, thus obtaining the list summarized in Table II. For completeness, we remark that the code for the experiments is implemented in Python3 and uses the scikit-learn toolkit. Moreover, we report in Table III the meaningful parameter settings of each model, which are chosen through extensive grid search operations. The F parameter denotes the number of features in input, and MSE is the Mean Squared Error. 7 Table I: Meaningful metrics of the CTU-13 collection. Source: [49]. Dataset Duration (hrs) Size (GB) Packets NetÔ¨Çows Malicious Flows Benign Flows Botnet # Bots 1 6.15 52 71 971 482 2 824 637 40 959 2 783 677 Neris 1 2 4.21 60 71 851 300 1 808 122 20 941 1 787 181 Neris 1 3 66.85 121 167 730 395 4 710 638 26 822 4 683 816 Rbot 1 4 4.21 53 62 089 135 1 121 076 1 808 1 119 268 Rbot 1 5 11.63 38 4 481 167 129 832 901 128 931 Virut 1 6 2.18 30 38 764 357 558 919 4 630 554 289 Menti 1 7 0.38 6 7 467 139 114 077 63 114 014 Sogou 1 8 19.5 123 155 207 799 2 954 230 6 126 2 948 104 Murlo 1 9 5.18 94 115 415 321 2 753 884 184 979 2 568 905 Neris 10 10 4.75 73 90 389 782 1 309 791 106 352 1 203 439 Rbot 10 11 0.26 5 6 337 202 107 251 8 164 99 087 Rbot 3 12 1.21 8 13 212 268 325 471 2 168 323 303 NSIS.ay 3 13 16.36 34 50 888 256 1 925 149 39 993 1 885 156 Virut 1 Table II: Features of the random forest models. Source: [20]. # Feature name Feature type 1,2 source/destination IP address type Boolean 3,4 source/destination port Numerical 5 Ô¨Çow direction Boolean 6 connection state Categorical 7 duration (seconds) Numerical 8 protocol Categorical 9,10 source/destination ToS Numerical 11,12 outgoing/incoming bytes Numerical 13 total transmitted packets Numerical 14 total transmitted bytes Numerical 15,16 source/destination port type Categorical 17 bytes per second Numerical 18 bytes per packet Numerical 19 packets per second Numerical 20 ratio of outgoing/incoming bytes Numerical Table III: Parameters of the random forest models. Parameter name Value Undistilled Number of estimators 763 Quality Function Gini Features for best split ‚àö F Bootstrap Yes Condenser Number of estimators 894 Quality Function Gini Features for best split ‚àö F Bootstrap Yes Receiver Number of estimators 1352 Quality Function MSE Features for best split F/2 Bootstrap Yes C. Generation of adversarial datasets We produce multiple adversarial datasets by manipu- lating the botnet netÔ¨Çows Db through feature modiÔ¨Åca- tions. Since the produced adversarial samples are used to evaluate the proposed approach, we consider the portion of botnet netÔ¨Çows from Db contained in the datasets used for the testing-phase, thus avoiding the submission of samples contained in the training set. An attacker can evade detection by increasing the Ô¨Çow duration through a small latency; and the number of bytes (or packets) by adding random junk data. All these modiÔ¨Åcations can be introduced in the network behavior of the bots without altering their underlying logic. To reproduce a similar adversarial attack pat- tern, we generate adversarial samples by manipulating combinations of up to 4 features, such as the duration of the Ô¨Çows, the total number of transmitted packets, the number of outgoing(Src) or incoming(Dst) bytes. Table IV reports the 15 groups of altered features denoted by G. As an example, adversarial samples belonging to group 1a alter only the Ô¨Çow duration, while those of group 3c include modiÔ¨Åcations to the duration, dst bytes and tot packets features. The feature manipulation is performed by augmenting each of these groups through 9 increment steps denoted by S; these steps are Ô¨Åxed for all the possible combinations. Hence, for each botnet family, we produce 135 adversarial collections, thus resulting in a total of 810 adversarial datasets (given by 15[groups of altered features] ‚àó9[increment steps] ‚àó 6[botnet families]). Table V reports the relationship between each step and the corresponding feature increments where Duration is measured in seconds. As an example, the adversarial datasets obtained through the VI step of the group 1b have the values of their Ô¨Çow outgoing bytes increased by 128. The adversarial datasets obtained through the II step of the group 3c have the values of their Ô¨Çow duration, incoming bytes and total packets increased by 2. There is a greater focus on small increments since they are easier to achieve and they are still able to generate samples that evade detection. The rationale behind the choice of the values shown in Table V is the following: our objective is to generate adversarial malicious samples that are only marginally different from 8 Table IV: Groups of altered features. Source: [20]. Group (g) Altered features 1a Duration (in seconds) 1b Src bytes 1c Dst bytes 1d Tot pkts 2a Duration, Src bytes 2b Duration, Dst bytes 2c Duration, Tot pkts 2d Src bytes, Tot pkts 2e Src bytes, Dst bytes 2f Dst bytes, Tot pkts 3a Duration, Src bytes, Dst bytes 3b Duration, Src bytes, Tot pkts 3c Duration, Dst bytes, Tot pkts 3d Src bytes, Dst bytes, Tot pkts 4a Duration, Src bytes, Dst bytes, Tot pkts their original counterparts, as shown in [12]. Although the exact numbers have been selected arbitrarily by adopting the powers of 2 for convenience, our goal is to represent the effects of small, but sensible variations of these features. Furthermore, introducing these small perturbations is a realistic task for the type of attacker considered in this paper. On the other hand, excessive increases higher than those shown in Table V may generate anomalous network Ô¨Çows that can be detected by different defensive mechanisms (e.g., [50]). Moreover, increasing the duration of each Ô¨Çow above 120 seconds may exceed the duration limits of the Ô¨Çow collector [50]. Table V: Increment steps of each feature for generating realistic adversarial samples. Source: [20]. Step (s) Duration Src bytes Dst bytes Tot pkts I +1 +1 +1 +1 II +2 +2 +2 +2 III +5 +8 +8 +5 IV +10 +16 +16 +10 V +15 +64 +64 +15 VI +30 +128 +128 +20 VII +45 +256 +256 +30 VIII +60 +512 +512 +50 IX +120 +1024 +1024 +100 The generation of the adversarial datasets is described in Algorithm 1, where A(¬∑) denotes the operator indi- cating an adversarially manipulated input. We remark the importance of the operation on line 19, because it shows that some features are mutually dependent. For example, for consistency reasons, increasing the Ô¨Çow duration requires to update also the bytes per second and the packets per second. For the performance evaluation we adopt the typical machine learning metrics: Precision (Prec), Detection Rate (DR, or Recall), F1-score, computed as follows: Prec = TP TP + FP , (3) DR = TP TP + FN , (4) F1-score = 2 ‚àóPrecision ‚àóDR Precision + DR, (5) where TP, FP, and FN denote true positives, false positives, and false negatives, respectively. In the remain- der of this paper, we consider a positive detection as a malicious sample. Algorithm 1: Algorithm for generating datasets of adversarial samples. Input: List of datasets of malicious Ô¨Çows Xm divided in botnet-speciÔ¨Åc sets Xb; list of altered features groups G; list of feature increment steps S. Output: List of adversarial datasets A(Xm). 1 A(Xm) ‚ÜêemptyList(); 2 foreach group g ‚ààG do 3 foreach step s ‚ààS do 4 foreach dataset Xb ‚ààXm do 5 Ag s(Xb) ‚ÜêCreateOneDataset(s, g, Xb); 6 Insert Ag s(Xb) in A(Xm); 7 return A(Xm) 8 // Function for creating a single adversarial dataset Ag s(Xb) corresponding to a botnet-specific dataset Xb, a specific altered feature group g, and a specific increment step s. 9 Function CreateOneDataset(s, g, Xb) 10 Ag s(Xb) ‚ÜêemptyList(); 11 foreach sample xb ‚ààXb do 12 Ag s(xb) ‚ÜêAlterSample(s, g, xb); 13 Insert Ag s(xb) in Ag s(Xb); 14 return Ag s(Xb) 15 // Function for creating a single adversarial sample Ag s(xb) corresponding to a botnet-specific sample xb, a specific altered feature group g, and a specific increment step s. 16 Function AlterSample(s, g, xb) 17 Ag s(xb) ‚Üêxb; 18 Increment features g of Ag s(xb) by s; 19 Update features of Ag s(xb) that depend on g; 20 return Ag s(xb) VI. PERFORMANCE EVALUATION We present the results of a large set of experiments with the aim of demonstrating that: (i) the proposed dis- tilled random forest detector achieves comparable or bet- ter detection performance than state-of-the-art algorithms in scenarios that are not subject to adversarial inputs; (ii) it signiÔ¨Åcantly improves the robustness of machine learning models against adversarial attacks. Achieving both results is an important outcome for cyber security contexts where we cannot anticipate whether a machine learning detector is subjected or not to adversarial at- tacks. 9 We evaluate and compare the performance of distilled and undistilled models in scenarios where samples are not adversarially modiÔ¨Åed. Then, we assess the effec- tiveness of the distilled random forest model against adversarial perturbations. Finally, we compare the result of the proposed method against two existing defensive strategies that can be applied to any supervised machine learning algorithm. A. Evaluation in normal scenarios We initially generate the probability labels for the Distilled detector by training and testing its Condenser model. Then, we train both the Distilled (through the Receiver) and Undistilled detectors on the same training set (but with appropriate labels), and proceed to evaluate them on the same test set. The results are shown in Table VI, where the columns report the chosen evaluation metrics, and the rows denote the botnet-speciÔ¨Åc instances of the Undistilled and Distilled detectors; the last row summarizes the results of each detector, which are av- eraged among all instances. From this table, we observe that the Distilled detector achieves the best results as it obtains higher Precision and F1-scores, and superior detection rates. We stress that the performance of the Distilled is similar to that obtained by state-of-the-art random forest-based botnet detectors [41], [51]. Further- more, we highlight that our proposal also outperforms the initial defensive distillation technique applied to neural networks in non-adversarial settings, because the distilled neural network model presents a reduced accuracy of ~1.5% when compared to a not-distilled neural network model [37]; this performance drop also affects distilled neural networks for malware classiÔ¨Åcation scenarios [39], which exhibit an increased rate of false alarms. It is important to note that the unusual perfect Prec scores achieved by both models for the Murlo botnet and by the Undistilled model for the Menti botnet can be motivated as follows: the large majority of the network Ô¨Çows generated by these botnet variants are signiÔ¨Åcantly different from benign trafÔ¨Åc, hence the models are able to recognize their malicious samples without generating false positives; however, some instances are still able to evade detection as indicated by the imperfect Recall value. These experiments show that, in the absence of ad- versarial attacks, our version of the distillation technique applied to random forests yields a detector with similar or superior performance than those that do not adopt a distillation technique. These results are crucial because they refer to a large set of scenarios and demonstrate that random forest-based detectors integrated with distillation are effective even in the absence of adversarial inputs. Table VI: Baseline vs. Distilled model performance. Botnet Detector F1-Score Precision Recall Neris Undistilled 0.9577 0.9615 0.9540 Distilled 0.9651 0.9671 0.9632 Virut Undistilled 0.9682 0.9876 0.9496 Distilled 0.9753 0.9876 0.9633 Murlo Undistilled 0.9932 1 0.9866 Distilled 0.9968 1 0.9937 Rbot Undistilled 0.9994 0.9999 0.9999 Distilled 0.9995 0.9999 0.9990 Menti Undistilled 0.9984 1 0.9969 Distilled 0.9979 0.9997 0.9969 NSIS.ay Undistilled 0.9213 0.9925 0.8596 Distilled 0.9273 0.9784 0.8812 Average Undistilled 0.9729 0.9774 0.9684 Distilled 0.9777 0.9804 0.9751 Since supervised machine learning methods for cyber defense need periodic re-trainings [6], it is important to evaluate the computational cost of the proposed solution. Thus, we measure and report the training times of the considered detectors in Table VII, which compares the time (in seconds) required for training the baseline Undistilled detector (composed of a single random forest classiÔ¨Åer) with those required by our method; as the proposed Distilled detector includes both the Condenser and the Receiver, we report the combined training time of these components. Computations are performed on a machine with the following hardware: CPU Intel Core i7- 7700HQ, RAM 32GB, and SSD 512GB. We observe that training the Distilled detector requires more effort, because it is composed of two models and, in addition, training a random forest regressor (that is, the Receiver) is more demanding than training a classiÔ¨Åer. However, we stress that these operations needs to be executed only periodically. Moreover, by performing the training computations on machines with dedicated hardware it is possible to decrease the absolute training time difference to negligible amounts. B. Evaluation in adversarial settings It must be determined whether and to which extent the proposed method is able to address issues related to adversarial attacks. To this purpose, we test the Distilled and the Undistilled detectors against the generated ad- versarial datasets, and compare their performance. The detection rate is the metric of interest for these analyses. We anticipate that this evaluation highlights a twofold improvement of our proposal: a signiÔ¨Åcant increase in the detection rate; a more stable behavior against different adversarial samples of the same botnet family. Among the considered 810 adversarial datasets, the Distilled detector clearly outperforms the baseline Undis- tilled in 759 cases; for the remaining 51 datasets, the 10 Table VII: Training time of each instance of the detectors. Botnet Detector Time (s) Neris Undistilled 75.8 Distilled 212.5 Virut Undistilled 16.7 Distilled 42.7 Murlo Undistilled 19.8 Distilled 53.9 Rbot Undistilled 77.1 Distilled 210.4 Menti Undistilled 2.8 Distilled 8.5 NSIS.ay Undistilled 1.6 Distilled 5.7 Average Undistilled 32.3 Distilled 87.0 Figure 5: Comparison of the average detection rates on each malware family. results of the two detectors are close. A comprehensive overview of the effectiveness of the two detectors is presented in Fig. 5, where the black and gray histograms report the detection rates of the Undistilled and Distilled detectors, respectively. Each histogram denotes the av- erage performance of the models applied to each botnet family. There is no doubt that the Distilled is signiÔ¨Åcantly superior to the Undistilled detector, with improvements ranging from 50% to 250%. We provide a more detailed comparison of the two detectors by considering the impact on detection rates of different altered features. The results are reported in Fig. 6, where the x-axis denotes the group of altered features, and every histogram is generated by averaging the detection rates achieved by each instance of the detectors for all increment steps. From this Ô¨Ågure, we can observe that the Distilled achieves superior detection rates for all the groups. The improvements for the groups 2a, 2b and 3a are the most signiÔ¨Åcant, as they allow the Distilled to retain a detection rate that is much higher than that of the Undistilled model. Moreover, the results for group 1a show that the Distilled detector is almost unaffected by alterations of the Ô¨Çow duration. On the other hand, adversarial alterations involving multiple features have a high impact on the performance of both detectors, as these modiÔ¨Åcations cause the malicious test samples to be considerably different than those used to train each model. Nevertheless, it is appreciable that, even in these tough circumstances, the Distilled is able to correctly identify more than twice the amount of malicious Ô¨Çows with respect to the Undistilled detector. Figure 6: Comparison of the average detection rates for each group of altered features. We also evaluate the detection rates of the two de- tectors for variable increment steps. The results are presented in Fig. 7, where the x-axis represents the increment steps and the histograms are generated by averaging the performance over all groups of altered features. We note that not only the Distilled outperforms the Undistilled model, but that it is much more resilient against samples that greatly differ from their original malicious version. Indeed, the detection rates for the VIII and IX steps are close to 50%; whereas the 15% detection rate of the Undistilled model is unacceptably low. This Ô¨Ågure also shows that the Distilled presents a more stable behavior against adversarial samples that are obtained through different increment steps: its de- tection rates are between 46% and 61%, against the much broader 11% to 45% range of the Undistilled model. From Fig. 7 and Fig. 6, we observe that greater perturbations correspond to the lowest detection rates; however, we remark that such modiÔ¨Åcations may gener- ate alerts from other defensive mechanisms (as explained in Section V). Furthermore, we highlight that adversarial attacks are more effective and more difÔ¨Åcult to detect when they are carried out through adversarial samples that are as close as possible to original samples. We investigate the increased stability of our proposal through the Ô¨Åne grained comparisons in Figs. 8, where the lines denote the detection rate (averaged for all botnet families) of the two models for four Ô¨Åxed groups of 11 Figure 7: Comparison of the average detection rates for each increment step. altered features (reported on top of each Ô¨Ågure) and vari- able increment steps. The x-axis denotes the increment steps, and the y-axis the detection rate. The black and the gray line refers to the Undistilled and the Distilled model, respectively. In order to appreciate the improved stability of the performance, we include in Figs. 9 the boxplots related to the results of Figs. 8. These boxplots highlight that the Distilled detector is not affected by sudden performance drops, thus indicating that it is able to maintain its performance even against adversarial inputs that are different from the scenarios considered in this paper. The increased resilience of the Distilled detector is motivated by the fact that its Receiver model adopts a more robust set of feature importances when compared to the Undistilled model. In other words, a random forest model makes a prediction by comparing the features of a sample with the feature importances learned during its training phase: the probability labels used to train the Receiver produce a random forest model with a set of feature importances having a higher degree of Ô¨Çexibility than that of the Undistilled classiÔ¨Åer, which adopts hard class labels. As a consequence, an adver- sary can signiÔ¨Åcantly alter the detection results of the Undistilled model through tiny alterations of the features, while the Distilled detector is capable of withstanding even perturbations of high magnitude. For example, let us consider two cases: in Fig. 8b the adversary modiÔ¨Åes only one feature (Dst Bytes); in Fig. 8d the adversary changes three features (Duration, Src Bytes and Dst Bytes). In the former case, the two detectors have comparable performance for the Ô¨Årst increment steps because the manipulated feature (Dst Bytes) has high and similar importance for both models. In the latter instance, when alterations concern even incoming bytes and Ô¨Çow duration, the detection rates of the Undistilled model are unacceptably low (below 15%). The improved resilience of our method is conÔ¨Årmed by comparing the detection rates of the two detectors for Ô¨Åxed botnet families. The results and corresponding box- plots are presented in Fig. 10 and Fig. 11, respectively. The name of the considered botnet family is reported on top of each Ô¨Ågure. Overall, these Ô¨Ågures conÔ¨Årm the superior detection capabilities and improved stability of the Distilled model. C. Comparison with existing defensive strategies We compare the effectiveness of our proposal against two known countermeasures against evasion adversarial attacks that have been proposed in the literature [18], [31], [34], [35], and that can be applied to any super- vised machine learning algorithm: adversarial retraining and feature removal. To this purpose, we perform our experiments by following the same procedures described in [18], due to the common characteristics shared by the considered adversarial scenarios and employed datasets. Hence, for the case of adversarial retraining we generate a ‚Äúhardened‚Äù Undistilled detector by re-training it after introducing a small (10%) portion of the generated adver- sarial samples into the corresponding training sets, and then measure its detection rate on the same adversarial datasets used in our previous experiments for both the normal and adversarial scenarios. The results of this evaluation are presented in Table VIII which shows the (averaged) Recall obtained by the re-trained Undistilled detector, the proposed Distilled detector, and the baseline Undistilled detector that we include for completeness. Table VIII: Comparison with adversarial retraining. Detector Type Recall (normal) Recall (adversarial) Undistilled (retrained) 0.9695 0.4987 Undistilled (baseline) 0.9684 0.2573 Distilled 0.9751 0.5152 With regards to feature removal, we develop a different Undistilled detector by training it on the same dataset used in our previous experiments but without considering the features that we modiÔ¨Åed to generate our adver- sarial samples (that is, Tot Pkts, Duration, Dst Bytes, Src Bytes), and then test it on the datasets used in Section VI-A; this is motivated by the fact that feature removal countermeasures, despite being resilient against adversarial attacks targeting the removed features, are known to generate excessive false alarms. The evaluation results are shown in Table IX, which compares the (av- erage) Precision, Recall and F1-score of the Undistilled detector (after excluding the features) with those obtained by the Distilled and the baseline Undistilled detector. By observing Table VIII, we note that our proposal exhibits a higher detection rate in both scenarios. At 12 (a) Group 1a: Duration. (b) Group 1c: Dst Bytes. (c) Group 2a: Duration & Src Bytes. (d) Group 3a: Duration & Src Bytes & Dst Bytes. Figure 8: Comparison of the detection rates on the adversarial datasets generated by all malware families. (a) Boxplot for Fig. 8a. (b) Boxplot for Fig. 8b. (c) Boxplot for Fig. 8c. (d) Boxplot for Fig. 8d. Figure 9: Boxplot visualization of the results in Figs. 8. Table IX: Comparison with feature removal. Detector Type F1-Score Precision Recall Undistilled (feature removal) 0.8728 0.8497 0.8974 Undistilled (baseline) 0.9729 0.9774 0.9684 Distilled 0.9777 0.9804 0.9751 the same time, concerning Table IX, we appreciate that the Distilled detector achieves signiÔ¨Åcantly better results. Indeed, we highlight that the proposed distillation method is not affected by the issues that characterize sim- ilar countermeasures: feature removal strategies generate unacceptable rates of false positives, whereas adversarial retraining requires to constantly update the training set with all the possible variations of samples that can be modiÔ¨Åed by the attacker (as explained in Section II). By taking into account all these analyses and evalua- tions, we can draw the following main conclusions. ‚Ä¢ Current state-of-the-art detection models based on machine learning have features that are too sensitive to the possibile manipulation of an attacker. ‚Ä¢ The proposed variation of the defensive distillation technique can be used to devise random forest detectors that: achieve same or better detection performance than existing algorithms in scenarios that are not subject to adversarial inputs; exhibit improved robustness and stability against adversar- 13 (a) Group 1a: Duration (b) Group 2b: Duration & Dst Bytes (c) Group 3a: Duration & Src Bytes & Dst Bytes (d) Group 4a: Duration & Src Bytes & Dst Bytes & Tot Pkts Figure 10: Comparison of the detection rates on the adversarial samples generated by speciÔ¨Åc malware families. (a) Boxplot for Fig. 10a. (b) Boxplot for Fig. 10b. (c) Boxplot for Fig. 10c. (d) Boxplot for Fig. 10d. Figure 11: Boxplot visualization of the results in Figs. 10. ial attacks; are not affected by the limitations of existing countermeasures. ‚Ä¢ Although our proposal is an important result to- wards the reduction of the impact of adversarial inputs against machine learning detectors, it rep- resents just a Ô¨Årst step. There is still space for researches that aim to further improve the detection rates. VII. CONCLUSIONS Adversarial attacks represent a prominent and dan- gerous menace to organizations that rely on machine learning cyber detectors. We observe that existing ap- proaches are based on classiÔ¨Åcation criteria that are too rigid for the highly variable cyber security domain. The intuition is that by developing more Ô¨Çexible models it is possible to counter the manipulation of malicious samples. For this reason, we present an original method that limits the impact of adversarial perturbations by leveraging the defensive distillation technique. We con- sider the random forest algorithm due to its superior per- formance in cybersecurity detection tasks. An extensive campaign of experimental evaluations demonstrates the effectiveness of the proposed method, which achieves a twofold advantage over the state-of-the-art: in scenarios subject to adversarially manipulated inputs, it improves 14 the detection rate up to 250%; in scenarios that are not subject to adversarial attacks, it achieves a similar or superior accuracy than existing techniques. This latter achievement is of particular importance because existing approaches that aim to counter adversarial attacks are often subject to a reduced performance in non-adversarial settings. Despite these promising results, our method presents room for further improvements. The proposed approach represents an original contribution to design robust detectors with high detection rates and strong enough against adversarial attacks. REFERENCES [1] A. L. Buczak and E. Guven, ‚ÄúA survey of data mining and machine learning methods for cyber security intrusion detection,‚Äù IEEE Commun. Surveys Tuts., vol. 18, no. 2, pp. 1153‚Äì1176, 2016. [2] E. Blanzieri and A. Bryl, ‚ÄúA survey of learning-based techniques of email spam Ô¨Åltering,‚Äù Artif. Intell. Review, vol. 29, no. 1, pp. 63‚Äì92, 2008. [3] H. Kettani and P. Wainwright, ‚ÄúOn the top threats to cyber systems,‚Äù in Proc. IEEE Int. Conf. Inf. Comp. Tech., Mar. 2019, pp. 175‚Äì179. [4] M. I. Jordan and T. M. Mitchell, ‚ÄúMachine learning: Trends, perspectives, and prospects,‚Äù Science, vol. 349, no. 6245, pp. 255‚Äì 260, 2015. [5] R. Sommer and V. Paxson, ‚ÄúOutside the closed world: On using machine learning for network intrusion detection,‚Äù in Proc. IEEE Symp. Secur. Privacy, 2010, pp. 305‚Äì316. [6] G. Apruzzese, M. Colajanni, L. Ferretti, A. Guido, and M. Marchetti, ‚ÄúOn the effectiveness of machine and deep learning for cybersecurity,‚Äù in Proc. IEEE Int. Conf. Cyber ConÔ¨Çicts, May 2018, pp. 371‚Äì390. [7] P. Torres, C. Catania, S. Garcia, and C. G. Garino, ‚ÄúAn analysis of recurrent neural networks for botnet detection behavior,‚Äù in Proc. IEEE Biennal Congress of Argentina, Jun. 2016, pp. 1‚Äì6. [8] J. Kim, J. Kim, H. L. T. Thu, and H. Kim, ‚ÄúLong short term memory recurrent neural network classiÔ¨Åer for intrusion detec- tion,‚Äù in Proc. IEEE Int. Conf. Platform Techn. Service, 2016, pp. 1‚Äì5. [9] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami, ‚ÄúThe limitations of deep learning in adversarial settings,‚Äù in Proc. IEEE Europ. Symp. Secur. Privacy, Mar. 2016, pp. 372‚Äì387. [10] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ÀáSrndi¬¥c, P. Laskov, G. Giacinto, and F. Roli, ‚ÄúEvasion attacks against machine learning at test time,‚Äù in Joint Europ. Conf. Mach. Learn. and Knowl. Discov. Databases. Springer, Sept. 2013, pp. 387‚Äì402. [11] N. Papernot, P. McDaniel, A. Sinha, and M. Wellman, ‚ÄúSok: Security and privacy in machine learning,‚Äù in Proc. IEEE Europ. Symp. Secur. Privacy, Apr. 2018, pp. 399‚Äì414. [12] J. Su, D. V. Vargas, and K. Sakurai, ‚ÄúOne pixel attack for fooling deep neural networks,‚Äù IEEE Trans. Evol. Comput., 2019. [13] S. Choudhury and A. Bhowal, ‚ÄúComparative analysis of machine learning algorithms along with classiÔ¨Åers for network intrusion detection,‚Äù in Proc. IEEE Int. Conf. Smart Tech. and Manag. Comp., Commun., Controls, Energy and Materials, May 2015, pp. 89‚Äì95. [14] O. Fajana, G. Owenson, and M. Cocea, ‚ÄúTorbot stalker: Detecting tor botnets through intelligent circuit data analysis,‚Äù in Proc. IEEE Int. Symp. Netw. Comput. Appl., Oct. 2018, pp. 1‚Äì8. [15] M. Stevanovic and J. M. Pedersen, ‚ÄúAn efÔ¨Åcient Ô¨Çow-based botnet detection using supervised machine learning,‚Äù in Proc. IEEE Int. Conf. Comput., Netw. and Commun., Feb. 2014, pp. 797‚Äì801. [16] P. A. A. Resende and A. C. Drummond, ‚ÄúA survey of random forest based methods for intrusion detection systems,‚Äù ACM Comput. Surv., vol. 51, no. 3, p. 48, 2018. [17] R. A. R. Ashfaq, X.-Z. Wang, J. Z. Huang, H. Abbas, and Y.- L. He, ‚ÄúFuzziness based semi-supervised learning approach for intrusion detection system,‚Äù Elsevier Inf. Sci., vol. 378, pp. 484‚Äì 497, 2017. [18] G. Apruzzese, M. Colajanni, L. Ferretti, and M. Marchetti, ‚ÄúAddressing adversarial attacks against security systems based on machine learning,‚Äù in Proc. IEEE Int. Conf. Cyber ConÔ¨Çicts, May 2019, pp. 1‚Äì18. [19] W. Hu and Y. Tan, ‚ÄúGenerating adversarial malware examples for black-box attacks based on gan,‚Äù arXiv:1702.05983, 2017. [20] G. Apruzzese and M. Colajanni, ‚ÄúEvading botnet detectors based on Ô¨Çows and random forest with adversarial samples,‚Äù in Proc. IEEE Int. Symp. Netw. Comput. Appl., Oct. 2018, pp. 1‚Äì8. [21] Z. Abaid, M. A. Kaafar, and S. Jha, ‚ÄúQuantifying the impact of adversarial evasion attacks on machine learning based android malware classiÔ¨Åers,‚Äù in Proc. IEEE Int. Symp. Netw. Comput. Appl., Oct. 2017, pp. 1‚Äì10. [22] M. Alazab, S. Venkatraman, P. Watters, and M. Alazab, ‚ÄúZero- day malware detection based on supervised learning algorithms of api call signatures,‚Äù in Proc. Austraalasian Conf. Data Mining, vol. 121, 2011, pp. 171‚Äì182. [23] M. Mannino, Y. Yang, and Y. Ryu, ‚ÄúClassiÔ¨Åcation algorithm sensitivity to training data with non representative attribute noise,‚Äù Elsevier Decis. Support Syst., vol. 46, no. 3, pp. 743‚Äì751, 2009. [24] I. H. Witten, E. Frank, M. A. Hall, and C. J. Pal, Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, 2016. [25] N. Dalvi, P. Domingos, S. Sanghai, D. Verma et al., ‚ÄúAdversarial classiÔ¨Åcation,‚Äù in Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining, Aug. 2004, pp. 99‚Äì108. [26] D. Lowd and C. Meek, ‚ÄúAdversarial learning,‚Äù in Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining, 2005, pp. 641‚Äì 647. [27] Y. Zhou, Z. Jorgensen, and M. Inge, ‚ÄúCombating good word attacks on statistical spam Ô¨Ålters with multiple instance learning,‚Äù in Proc. IEEE Int. Conf. Tools Artif. Intell., vol. 2, Oct. 2007, pp. 298‚Äì305. [28] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, ‚ÄúIntriguing properties of neural networks,‚Äù arXiv:1312.6199, 2013. [29] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar, ‚ÄúAdversarial machine learning,‚Äù in Proc. ACM Workshop Secur. and Artif. Intell., Oct. 2011, pp. 43‚Äì58. [30] I. Jeun, Y. Lee, and D. Won, ‚ÄúA practical study on advanced persistent threats,‚Äù in Comput. Appl. Secur., Control, Syst. Eng. Springer, 2012, pp. 144‚Äì152. [31] J. Gardiner and S. Nagaraja, ‚ÄúOn the security of machine learning in malware c&c detection: A survey,‚Äù ACM Comput. Surv., vol. 49, no. 3, p. 59, 2016. [32] W. Xu, Y. Qi, and D. Evans, ‚ÄúAutomatically evading classiÔ¨Åers,‚Äù in Proc. Symp. Netw. Distrib. Syst., Feb. 2016, pp. 21‚Äì24. [33] A. Demontis, P. Russu, B. Biggio, G. Fumera, and F. Roli, ‚ÄúOn security and sparsity of linear classiÔ¨Åers for adversarial settings,‚Äù in Proc. Joint. Int. Workshops Statist. Tech. Pattern Recognit. and Struct. Syntactic Pattern Recognit. Springer, Nov. 2016, pp. 322‚Äì 332. [34] F. Zhang, P. P. Chan, B. Biggio, D. S. Yeung, and F. Roli, ‚ÄúAdversarial feature selection against evasion attacks,‚Äù IEEE Trans. Cybern., vol. 46, no. 3, pp. 766‚Äì777, 2016. [35] H. S. Anderson, J. Woodbridge, and B. Filar, ‚ÄúDeepdga: Adversarially-tuned domain generation and detection,‚Äù in Proc. ACM Workshop Artif. Intell. Secur., Oct. 2016, pp. 13‚Äì21. [36] A. Demontis, M. Melis, B. Biggio, D. Maiorca, D. Arp, K. Rieck, I. Corona, G. Giacinto, and F. Roli, ‚ÄúYes, machine learning can be more secure! a case study on android malware detection,‚Äù IEEE Trans. Depend. Sec. Comput., 2017. 15 [37] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, ‚ÄúDistillation as a defense to adversarial perturbations against deep neural networks,‚Äù in Proc. IEEE Symp. Secur. Privacy, May 2016, pp. 582‚Äì597. [38] A. S. Ross and F. Doshi-Velez, ‚ÄúImproving the adversarial robust- ness and interpretability of deep neural networks by regularizing their input gradients,‚Äù in AAAI Conf. Artif. Intell., Apr. 2018. [39] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. Mc- Daniel, ‚ÄúAdversarial perturbations against deep neural networks for malware classiÔ¨Åcation,‚Äù arXiv:1606.04435, 2016. [40] W. Wang, M. Zhu, X. Zeng, X. Ye, and Y. Sheng, ‚ÄúMalware trafÔ¨Åc classiÔ¨Åcation using convolutional neural network for rep- resentation learning,‚Äù in Proc. IEEE Int. Conf. Inf. Netw. IEEE, 2017, pp. 712‚Äì717. [41] B. Abraham, A. Mandya, R. Bapat, F. Alali, D. E. Brown, and M. Veeraraghavan, ‚ÄúA comparison of machine learning approaches to detect botnet trafÔ¨Åc,‚Äù in Proc. IEEE Int. Joint Conf. Neur. Netw., Jul. 2018, pp. 1‚Äì8. [42] M. Stevanovic and J. M. Pedersen, ‚ÄúDetecting bots using multi- level trafÔ¨Åc analysis,‚Äù Int. J. Cyber Situational Awareness, vol. 1, no. 1, 2016. [43] N. Carlini and D. Wagner, ‚ÄúTowards evaluating the robustness of neural networks,‚Äù in Proc. IEEE Symp. Secur. Privacy, 2017, pp. 39‚Äì57. [44] B. Biggio, G. Fumera, and F. Roli, ‚ÄúPattern recognition systems under attack: Design issues and research challenges,‚Äù Int. J. Pattern Recogn. Artif. Intel., vol. 28, no. 07, p. 1460002, 2014. [45] P. Laskov et al., ‚ÄúPractical evasion of a learning-based classiÔ¨Åer: A case study,‚Äù in Proc. IEEE Symp. Secur. Privacy, 2014, pp. 197‚Äì211. [46] S. J. Pan, Q. Yang et al., ‚ÄúA survey on transfer learning,‚Äù IEEE Trans. Knowl. Data Eng., vol. 22, no. 10, pp. 1345‚Äì1359, 2010. [47] H. Borchani, G. Varando, C. Bielza, and P. LarraÀúnaga, ‚ÄúA sur- vey on multi-output regression,‚Äù Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 5, no. 5, pp. 216‚Äì 233, 2015. [48] M. Marchetti, F. Pierazzi, M. Colajanni, and A. Guido, ‚ÄúAnalysis of high volumes of network trafÔ¨Åc for advanced persistent threat detection,‚Äù Elsevier Comput. Netw., vol. 109, pp. 127‚Äì141, 2016. [49] S. Garcia, M. Grill, J. Stiborek, and A. Zunino, ‚ÄúAn empirical comparison of botnet detection methods,‚Äù Elsevier Comput. Se- cur., vol. 45, pp. 100‚Äì123, 2014. [50] F. Pierazzi, G. Apruzzese, M. Colajanni, A. Guido, and M. Marchetti, ‚ÄúScalable architecture for online prioritisation of cyber threats,‚Äù in Proc. IEEE Int. Conf. Cyber ConÔ¨Çicts, May 2017, pp. 1‚Äì18. [51] M. Stevanovic and J. M. Pedersen, ‚ÄúAn analysis of network trafÔ¨Åc classiÔ¨Åcation for botnet detection,‚Äù in Proc. IEEE Int. Conf. Cyber Situat. Awar., Data Analyt., Assessment, Jun. 2015, pp. 1‚Äì8. [52] G. Kirubavathi and R. Anitha, ‚ÄúBotnet detection via mining of trafÔ¨Åc Ô¨Çow characteristics,‚Äù Comput. Elect. Eng., vol. 50, pp. 91‚Äì 101, 2016.