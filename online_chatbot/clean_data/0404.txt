1 Municipal cyber risk modeling using cryptographic computing to inform cyber policymaking A case study in the policy contributions of cyber risk measurement for municipal cybersecurity1 Authors2: Avital Baral, Taylor Reynolds, Lawrence Susskind, Daniel J. Weitzner, Angelina Wu The authors would like to thank Rebecca Spiewak for her research building MIT’s Ransomware Readiness Index that was used as the control list for the study and to ScoI Shackelford and Etyan Tepper for their comments and suggesKons as discussants at the CLPSC. Working DraN for PresentaKon at the Cybersecurity Law and Policy Scholars Conference September 29, 2023 Abstract Municipali)es are vulnerable to cybera3acks with devasta)ng consequences, but they lack key informa)on to evaluate their own risk and compare their security posture to peers. Using data from 83 municipali)es collected via a cryptographically secure computa)on plaAorm about their security posture, incidents, security control failures, and losses, we build data-driven cyber risk models and cyber security benchmarks for municipali)es. We produce benchmarks of the security posture in a sector, the frequency of cyber incidents, forecasted annual losses for organiza)ons based on their defensive posture, and a weigh)ng of cyber controls based on their individual failure rates and associated losses. Combined, these four items can help guide cyber policymaking by quan)fying the cyber risk in a sector, iden)fying gaps that need to be addressed, priori)zing policy interven)ons, and tracking progress of those interven)ons over )me. In the case of the municipali)es, these newly derived risk measures highlight the need for con)nuous measured improvement of cybersecurity readiness, show clear areas of weakness and strength, and provide governments with some early targets for policy focus such as security educa)on, incident response, and focusing eﬀorts ﬁrst on municipali)es at the lowest security levels that have the highest risk reduc)on per security dollar invested. 1 The authors would like to thank Rebecca Spiewak for her research designing MIT’s Ransomware Readiness Index that was used as the control list for the study. We appreciate the collabora@on with Geoﬀrey Beckwith, Stan Corcoran, and Lin Chabra at the MassachuseGs Municipal Associa@on, Joe Callahan at Cabot Risk Strategies, as well as the municipal government cybersecurity professionals who assisted with data collec@on. 2 Authors listed in alphabe@cal order. Weitzner and Wu were supported, in part, by NSF grant Collabora@ve Research: DASS: Legally Accountable Cryptographic Compu@ng Systems (LAChS) Award Number: 21315415. Reynolds was supported by MIT’s Future of Data Ini@a@ve, MIT’s FinTech@CSAIL, and MIT’s Machine Learning Applica@ons @CSAIL. 2 Table of contents Abstract ...................................................................................................................................... 1 Table of contents ........................................................................................................................ 2 IntroducKon ................................................................................................................................ 4 Literature review ........................................................................................................................ 6 Data ............................................................................................................................................ 8 Model ......................................................................................................................................... 8 Results ........................................................................................................................................ 9 Benchmarking ......................................................................................................................... 9 Risk modeling results ............................................................................................................ 17 Policy implicaKons .................................................................................................................... 19 Securely built risk models provide valuable informaKon to policymakers ........................... 19 The need for a standardized language for talking about and measuring risk ....................... 20 OrganizaKons with low security levels beneﬁt most from policy intervenKons ................... 20 Need for regular benchmarking and dynamic improvement ............................................... 20 Working toward a deﬁniKon of reasonable cybersecurity for municipal governments ....... 21 Dynamic Improvement and insurance-based schemes ........................................................ 21 Policy related to speciﬁc security controls ............................................................................ 22 Awareness raising ................................................................................................................. 23 Materials / training at the state level ................................................................................... 24 EvaluaKng the results of exisKng programs .......................................................................... 24 Next steps and future research ................................................................................................ 25 Bibliography .............................................................................................................................. 26 Annex 1: MIT RRI QuesKonnaire ............................................................................................... 29 Annex 2: Technical details on creaKng the risk models ............................................................ 31 Data .......................................................................................................................................... 31 Data: Maturity levels ............................................................................................................ 31 Data: Incident counts ............................................................................................................ 32 Data: Control failures ............................................................................................................ 32 Financial costs: Total ............................................................................................................. 32 Financial costs: Controls ....................................................................................................... 32 3 Frequency ............................................................................................................................. 33 Defense Gap Index ................................................................................................................ 33 Losses ................................................................................................................................... 36 Results .................................................................................................................................. 36 4 Introduc2on MunicipaliKes are vulnerable to cyber aIacks with devastaKng consequences. In the United States, local governments are also oNen in charge of criKcal infrastructure and key services for consKtuents. AddiKonally, they are commonly strapped for resources with limits on Kme, money, and experKse, parKcularly in the IT domain (Norris et al., 2021). This combinaKon makes municipaliKes a frequent target of cyber aIacks, including notable recent examples in BalKmore (BalKmore Sun, 2019a), Atlanta (Reuters, 2018) and a 2019 aIack in Louisiana that forced the governor to issue a statewide emergency declaraKon (State of Louisiana, 2019). Cyber aIacks on public infrastructure have concrete negaKve impacts on communiKes, from slowing down or stopping important municipality-mediated endeavors such as home sales paperwork (BalKmore Sun, 2019b), as well as signiﬁcant impacts on individual lives (Spiewak, 2022). The US White House has convened a series of summits on issues such as strengthening the cybersecurity of US schools as a result (US White House, 2023). MunicipaliKes are reluctant to share informaKon regarding their cybersecurity challenges. They worry that sharing data regarding the incidents they face will result in public backlash as well as open the door to further aIacks (Preis and Susskind, 2022). In parKcular, while there exist surveys regarding the threats that municipaliKes (Norris et al., 2021), as well as organizaKons in other sectors face (US White House, 2018), there is a dearth of data regarding the frequency of aIacks and losses sustained (de Castro et al., 2020). Given this lack of data and the decentralized nature of US local government, municipaliKes lack a way of benchmarking their preparedness against peers and miss the opportunity to learn from each other’s experiences. State and federal governments want to support municipal cyber security preparedness but lack adequate guidance on what kind of support will be most useful. MunicipaliKes look to state and federal government for guidance and help with their cybersecurity challenges, but the lack of reliable data regarding the kinds of cyber threats, aIacks, and losses that enKKes are facing limits the eﬃcacy and adequacy of the help that those higher-level governments can provide. States have piloted diﬀerent cybersecurity iniKaKves over the last decade aimed at municipaliKes such as regional CISO as a service programs (StateTech, 2020), Cyber Volunteer Corps (NGA, 2017), and employee trainings (Commonwealth of MA, 2023). However, a criKcal problem with these iniKaKves is that they are not being evaluated to test the eﬀecKveness in reducing cyber aIacks against municipaliKes. This is because there is very liIle informaKon about aIacks and losses that is available. OrganizaKons are reluctant to share informaKon about current security pracKces, details about security control failures, and the scale of losses. States are well-intenKoned in their desire to help protect municipaliKes, but without robust data, it is challenging to evaluate the usefulness of diﬀerent intervenKons, which ones to keep and expand, and which ones are a poor allocaKon of limited resources. There are, however, new methods for securely and privately collecKng cyber risk data that can overcome these challenges. Cryptographic techniques such as secure mulKparty computaKon can help collect sensiKve data without requiring supplying parKes to idenKfy themselves or disclose their data. We demonstrate that if municipaliKes would leverage these tools we describe to build up metrics 5 and models in evaluaKng their cyber risks, then municipaliKes and policymakers at the state and naKonal level would be able evaluate and prioriKze cyber hygiene measures, and thereby minimize risks overall. This could take the form of a set of “reasonable” cybersecurity measures for municipaliKes. The Secure Cyber Risk AggregaKon and Measurement (SCRAM) system (de Castro et al., 2020) is a plakorm that uses secure mulKparty computaKon to aggregate parKcipant responses to customizable quesKonnaires regarding cybersecurity controls implementaKon, cybersecurity aIacks, and related monetary and insKtuKonal losses (Figure 1). ParKcipants in SCRAM computaKons receive aggregate scores for the sector for metrics laid out by the quesKonnaire without being required to disclose their own data to any third party. That means that neither the parKcipants nor the computaKon administrators can access any parKcipants’ responses. The outcome is more robust data regarding cybersecurity-related aIacks and losses in which parKcipants are incenKvized to honestly parKcipate in given the privacy of the process. The SCRAM plakorm has been used in pilot computaKons with large ﬁrms across diﬀerent sectors. Figure 1: SCRAM process for gathering data using secure mul9-party computa9on This paper provides a proof of concept towards providing vital insight into the municipal cybersecurity data problem using data from a collecKon across municipaliKes in a single US state. It shows the types of data that will be available to cyber policymakers in the future. We adopt an exisKng set of quesKons from the MIT Ransomware Readiness Index (Spiewak et al., 2021) and then collect the data in SCRAM to build the benchmarks, derive the indicators, and produce risk models and forecasts. These new data can then inform policy decisions at all levels of government. 6 There are several key outputs from our research: 1. Benchmarks: Security benchmarks that organizaKons can use to compare their own security posture against their peers 2. Risk forecasts: Risk forecasts can provide esKmates of the risk for an organizaKon, an indicaKon of how ﬁnancial risk changes with varying maturity levels of controls to prioriKze security investments, and informaKon on a fair price for insurance coverage. 3. Policy considera9ons: Benchmarks for the group provide important insights into how the municipaliKes perceive the maturity level of their own security controls. Policymakers can use these insights to target training, funding, and intervenKons to improve the overall security of criKcal infrastructure. Literature review There are a handful of naKonal surveys regarding cybersecurity in the public sector and together they paint a dire picture of state and municipaliKes’ cybersecurity preparedness. MunicipaliKes report themselves to be unprepared to eﬀecKvely face the threat of cyberaIack, chieﬂy ciKng lack of funding, lack of qualiﬁed staﬀ, and lack of prioriKzaKon of informaKon technology by their organizaKonal leadership as barriers to an adequate cybersecurity posture. Norris et al (2019) conducts a naKonwide survey of municipaliKes and ﬁnds that only a minority of responding municipal IT professionals considered their municipaliKes well-prepared to detect and recover from ransomware aIacks and cyberaIacks generally (Norris et al., 2019). Earlier work by Norris idenKﬁes top barriers to cybersecurity readiness in municipaliKes as the inability to pay compeKKve salaries to IT professionals, lack of funding for cybersecurity, and lack of dedicated security staﬀ (Norris et al., 2019). A consistent and concerning thread through the surveys is that a signiﬁcant porKon of respondents report that they do not know if or how oNen they are being aIacked and are unable to rate their own levels of preparedness, indicaKng a lack of preparaKon and prioriKzaKon of cybersecurity (Norris et al., 2019); (Preis and Susskind, 2022). Among those respondents that did report aIack aIempt frequencies, 27.7% of respondents reported being aIacked at least hourly, and another 19.4% reported being aIacked at least once a day (Norris et al., 2019). A biennial survey of state CIOs conducted jointly by DeloiIe and the NaKonal AssociaKon of State CIOs (NASCIO) repeatedly aﬃrms the ﬁndings above (DeloiIe and NASCIO, 2018; DeloiIe and NASICO, 2020). The DeloiIe-NASCIO reports highlight the need for greater cybersecurity funding at the state level and for a coherent, centralized policy strategy for local government. CISOs consulted in the 2018 and 2020 DeloiIe-NASCIO surveys reported that they viewed state- level regulaKons backed by funding commitments as most eﬀecKve in improving cybersecurity posture. Only a minority of states had a cybersecurity budget line item (DeloiIe and NASCIO, 2018; DeloiIe and NASICO, 2020) A 2020 survey of the CoaliKon of City CISOs in partnership with the InternaKonal City/County Managers AssociaKon (ICMA) of chief security oﬃcers serving 14 large American ciKes echoed the ﬁndings of prior surveys and found that aIacker demands for ransoms had increased in 7 2020 (Norris, 2021). The ICMA survey idenKﬁed leadership buy-in as another top barrier to improving cybersecurity posture in local governments. The state of Indiana produced a “State of Hoosier Cybersecurity” in 2020 that ran a large-scale survey of 178 organizaKons in Indiana (Boustead and Shackelford, 2020). The report found 19% of respondents had encountered a “cyber incident” in the previous three years. The report also found that half of the organizaKons indicated that they had cyber insurance (Boustead and Shackelford, 2020). MunicipaliKes are hesitant to share informaKon about their cybersecurity posture and incidents they have faced due to concerns about further opening themselves up to cyberaIack as well as poliKcal fallout (Preis and Susskind, 2022). The 2018 White House Council of Economic Advisors report, The Cost of Malicious Cyber AcKvity to the U.S. Economy (US White House, 2018), found an extremely broad range for the cost of cyberaIacks directed at state and local governments, chronicling successful breaches with costs from USD 665,000 to USD 40.53 million, with a median cost varying from USD 60,000 to as high as USD 1.87 million. There is addiKonal money that will be allocated at the federal level in the United States to help local governments with cybersecurity. In 2023, DHS announced USD 375 million in funding to boost state and local cybersecurity preparedness (US DHS, 2023). While the allocaKon is large in the aggregate, it would amount to an average of just over USD 4,100 for each local government once spread across the 90,875 local government units idenKﬁed by the US Census in 2022 (US Census, 2023). The US Cybersecurity and Infrastructure Security Agency (CISA) will also begin collecKng data on ransomware aIacks when an organizaKon reports their incident to the Federal Bureau of InvesKgaKon (FBI), CISA, or the US Secret Service (US CISA, 2023). It is unclear at this Kme whether the data compiled from each of the reporKng agencies will be comparable. While data on municipal cybersecurity is relaKvely new, there is a much longer history of cyber risk approaches. There are some well-known approaches for modeling cyber risk, but most of them are focused on individual risks and decisions related to speciﬁc control applicaKons rather than a broader, holisKc view of cyber risk for an organizaKon. Early approaches introduced a simple way of evaluaKng cyber risk. In the 1970s, Courtney posited that risk to electronic data processing systems could be summarized with two elements (Courtney, 1977): 1. Statement of impact - How badly a speciﬁc diﬃcultly hurts 2. Statement of probability of encountering that diﬃcult in a speciﬁed period of Kme Nearly ﬁve decades later, Courtney’s iniKal approach has largely held. The popular cyber risk modeling framework Factor Analysis of InformaKon Risk (FAIR) has the same top-level structure and provides a framework for coming up with top-level esKmates (Freund and Jones, 2014). 8 FAIR is commonly used in large sectors such as ﬁnancial services. Both Courtney and FAIR’s approach disseminate all risk measures from two components: frequency and impact. In FAIR, the implementaKon of security controls feeds into the frequency side, but at a low level in the structure. Security and risk teams speciﬁcally focus on the impact of controls on risk outcomes, so other modeling approaches pull up cyber defenses to a top line element in the risk model alongside frequency and impact. For example, the CRAM model from Mukhopadhyay introduces a variable called “SecT” for security technology to capture control decisions at a high level (Mukhopadhyay et al., 2019). In summary, the current literature shows low levels of security maturity in municipalities and a hesitation to share data about successful attacks. There are programs in place to support municipal cybersecurity across the various levels of government, but currently there is no way to evaluate the effectiveness of these programs. The basic cyber risk structures are well known however, and they can be applied to the municipal case using specific data that can be collected via security computations. Data The data were collected in June 2022 from the municipaliKes in MassachuseIs via a secure quesKonnaire prepared for the SCRAM plakorm. A descripKon of each of the data elements is provided below. It is important to note, however, that the results from a mulK-party computaKon are only aggregate ﬁgures and cannot be linked back to an individual organizaKon that submiIed data. The increase in privacy and security comes at the expense of no longer being able to see the individual inputs into a data aggregaKon. All data are self-reported by municipaliKes, and parKcipants are trained during a session on how to ﬁll out the forms and esKmate the data based on their own experiences. This training step is necessary to improve the comparability of the responses across ﬁrms. The collected data that are used in this analysis include: - Maturity level: 22 quesKons on the maturity level of controls (see Annex 1) - Incident count: 1 quesKon on the frequency of incidents - Control failures: Count of the Kmes individual controls failed leading to incidents - Financial costs (total): 1 quesKon on the total cost of incidents - Financial costs (control failures): Data on the aIributed costs of incident failures to speciﬁc controls Model In our model, the annual cyber risk for a ﬁrm is determined by the historical frequency of aIacks in the sector mulKplied by a defense gap index scalar and by the average impact of 9 incidents for the sector (EquaKon 1). Our key modeling assumpKons for the municipal risk analysis are detailed in Annex 2. (Eq 1) 𝐹𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦∗𝐷𝑒𝑓𝑒𝑛𝑠𝑒𝐺𝑎𝑝𝐼𝑛𝑑𝑒𝑥∗𝐼𝑚𝑝𝑎𝑐𝑡= 𝐴𝑛𝑛𝑢𝑎𝑙𝑅𝑖𝑠𝑘 Frequency is derived from data on incident counts in the data collecKon while impact data come from self-reported losses from those incidents. The Defense Gap Index is more complicated to build, but essenKally provides a mulKplier for risk measures that is based on how far the security of an organizaKon deviates from the average security posture of its peers. OrganizaKons with lower security levels have higher values with higher associated risks, while those with beIer security have reduced risk values that emerge. The Defense Gap Index also assigns more weight to control failures with a history of aIributed losses, so organizaKons with lower security in these areas are penalized more than for a similar gap in a controls that has no losses associated with it. Finally, the Defense Gap Index is mapped to a loss distribuKon that emerges from the secure computaKons where ﬁrms with lower security have higher losses than those with beIer security. The details of the data and models are provided in Annex 2. Results This secKon begins by providing the benchmarking results from the municipal computaKon related to control maturity, frequency of control failures, and corresponding ﬁnancial losses. Next, the secKon takes these benchmarks and uses them to develop risk models for the group. The data are all self-reported, which introduces biases and measurement challenges, but it sKll oﬀers the best insight we have into the current state of municipal cybersecurity defenses in a state. We have worked to reduce these biases by holding mulKple training sessions with the respondents to harmonize expectaKons, measurement parameters, and answer quesKons they have. Benchmarking There are 83 municipaliKes from the same state that parKcipated in the computaKon. Over a period of two years, the group reported 4 signiﬁcant incidents and 14 control failures related to those incidents that were responsible for the losses. The number of incidents across the group is low relaKve to the size of the group. From a security perspecKve, this is a desired outcome, but it does make the weights assigned to speciﬁc control failures and resulKng losses potenKally overweighted. As a result, researchers should keep these ﬁndings in context since small changes in the answers can have large eﬀects when extrapolated out to the enKre group. At the same Kme, these data can sKll provide us with preliminary informaKon about the overall rates of incidents and losses, as well as the percepKon of control maturity across municipaliKes. We run a total of six computaKons. The ﬁrst includes all municipaliKes (n=83) and then the same metrics are computed for ﬁve subgroups based on populaKon. All municipaliKes are 10 weighted equally within each of the computaKon groups, so a larger city will have the same weight on the average as a smaller town in the same group (Table 2). Table 2: Classiﬁca9on criteria of each of the six computa9ons ComputaKon 1 All municipaliKes combined n = 83 ComputaKon 2 MunicipaliKes with populaKons over 25,000 n = 8 ComputaKon 3 MunicipaliKes with populaKons between 15,000 and 25,000 n = 9 ComputaKon 4 MunicipaliKes with populaKons between 5,000 and 15,000 n = 29 ComputaKon 5 MunicipaliKes with populaKons less than 5,000 n = 16 ComputaKon 6 MunicipaliKes with no reported populaKon* n = 21 Note: Municipali/es with no popula/on can include school and ﬁre districts that have no standard popula/on but s/ll par/cipate in insurance pools. The data collecKon does not separate out much larger ciKes, and if a signiﬁcant number were included, they could aﬀect the averages in diﬀerent ways. But these results are applicable for somewhat smaller populaKon groups of municipaliKes. In terms of overall security levels, we do not ﬁnd large variaKons between the diﬀerent populaKon sizes except for the smallest populaKon groups (Figure 2). Unsurprisingly, we see that the largest municipaliKes report slightly higher maturity levels of their controls, while municipaliKes with smaller populaKons have less mature levels. One group of municipal enKKes does not include data on populaKon and can include ﬁre districts or school district in rural areas. These municipal enKKes with no populaKon recorded had the lowest maturity levels in the overall group. Figure 2: Average maturity of controls, overall and by popula9on size The average adopKon rate across all individual controls was 51%, and that corresponds to essenKally halfway between “parKally implemented” and “largely implemented” on our scale (Table 1). It’s important to note that the overall maturity rate of 51% is noKceably lower than the 65% maturity rate we have found among private ﬁrms in other sectors in forthcoming 11 research. Even the largest subgroup of municipaliKes (with populaKons over 25k) had a lower maturity rate of 56% that is below the averages we ﬁnd in private sector computaKons. Table 1: Implementa9on levels mapped to percentages for quan9ﬁca9on Not implemented ParKally implemented Largely implemented Fully implemented 0% 33% 67% 100% The 22 subcontrols in MIT’s Ransomware Readiness Index are broken into 10 broader groups.3 There are large diﬀerences in maturity levels across the 10 control groups for the 83 municipal enKKes (Figure 3). Figure 3: Security control maturity by category, average across municipali9es Areas of strength The areas of highest perceived strength are backups, patching of systems, and segmenKng the network. The backup category has the highest self-reported maturity of 85%. Within the backup group which scores highest across all categories, some elements have beIer maturity than others (Figure 4). The self-described maturity raKng for performing backups on a regular basis is 95% (essenKally fully implemented). However, the raKng falls to 81% for tesKng backup data and 79% for storing backup data in an oﬄine locaKon – both of which are crucial for remediaKon eﬀorts. This may imply that simply performing backups could be giving municipaliKes a false sense of security if those backups are not tested nor stored oﬄine. 3 Details of MIT’s Ransomware Readiness Index are provided in Annex 1. 12 Figure 4: Security control maturity, by control, average across all municipali9es Areas of weakness Somewhat surprisingly, mulK-factor authenKcaKon (MFA) has a relaKvely low maturity result of 33%, which is equivalent to parKally implemented in our scale (Table 1). The lack of MFA is oNen considered a “deal killer” by the technical groups supporKng underwriKng for cyber insurance coverage, so organizaKons without MFA would struggle to ﬁnd coverage from most insurers. The breakdown of the MFA data by populaKon size reveals some interesKng insights. The lowest reported maturity levels come from mid-size municipaliKes, not the larger or smaller ones (Figure 5). This result is seen across several of the variables and could be explained by the dynamics of operaKons for large and small municipaliKes. The largest municipaliKes oNen have budgets to support dedicated IT staﬀ that have some background in cybersecurity. The smallest set of municipaliKes oNen lacks a dedicated full-Kme IT staﬀ member, and oNen outsources its IT services to ﬁrms with a specializaKon in the area. It could be precisely the mid-size municipaliKes that are large enough to have an on-staﬀ IT member, but too small to have cyber specialists. We cannot know for sure without deeper invesKgaKon, but it is in an area for future exploraKon. 13 Figure 5: Mul9-factor authen9ca9on maturity, by municipality size Data at a more-granular level on MFA responses highlights a parKcular challenge for municipaliKes (Figure 6). Only 4 out of 83 municipaliKes report fully implemenKng MFA, and 23 municipaliKes say that MFA is not implemented at all, even for very sensiKve systems. Certainly, getng MFA implemented on these unprotected systems should be a key priority. Figure 6: MFA maturity counts, by maturity level, all municipali9es, counts EncrypKon related quesKons reveal relaKvely low maturity levels. The average maturity raKng for encrypKng data in transit is 52%, which is roughly equivalent to the average maturity level for all controls reported by all municipaliKes. We would expect that these numbers would be high due to the widespread adopKon of TLS and VPNs for connecKng systems over a wide area network. On the other hand, encrypKon of data at rest is lower at 34%, which corresponds to “parKally implemented” on our scale. In today’s current threat environment, ransomware aIacks oﬀer a one-two punch of ﬁrst exﬁltraKng data that can be released if ransoms are not paid and then encrypKng data on the organizaKon’s own systems to extract payments. EncrypKng data at rest, parKcularly sensiKve data such as personally idenKﬁable informaKon 14 (PII), is an important way to limit the impact of these aIacks with an exﬁltraKon component, parKcularly if the organizaKon is relying on backups to recover and avoid paying ransoms. The low maturity score of 34% highlights that there is signiﬁcant work to be done in this area. The security controls with the lowest reported maturity levels were related to incident response and checking the work. The maturity levels across these ﬁve controls run from 8% to 26% and are signiﬁcantly lower than the average across all controls of 51% (Figure 7). Figure 7: Security controls with low rated maturity levels MunicipaliKes largely lack outside tesKng of their security via red-team exercises or external penetraKon tesKng programs. It is not surprising that smaller communiKes may lack the resources to hire outside consultants to run these exercises. A detailed look at the count of the responses for establishing an external penetraKon tesKng program shows that most municipaliKes have not implemented external penetraKon tesKng, but there are 12 municipaliKes that have either largely or fully implemented an external pen-tesKng process (Figure 8). InteresKngly, informal conversaKons with parKcipants indicate that the municipaliKes that do have fully or largely implemented red-teaming exercises may include some smaller municipaliKes that outsource their IT and security to specialized ﬁrms that run pen-tesKng programs over their porkolio of clients. 15 Figure 8: Summary of responses for establishing an external pen-tes9ng program, counts One of the more important ﬁndings is that municipaliKes rate their incident response maturity as very low even though all have access to state resources to develop incident response plans, and the insurer provides extensive incident response support if the municipaliKes ask for it. In this case, there seems to be a combinaKon of a lack of awareness and missing out on resources at the state level that are already available to them. Benchmarking: Losses and control failures There are 4 signiﬁcant incidents across the group of 83 municipaliKes over a period of 3 years. The municipaliKes that have incidents are asked to report up to 5 control failures that led to each incident among the 22 controls in the quesKonnaire. The total losses from the incident are then spread across the implicated controls equally. We give them equal weights because the aIribuKon process is not precise enough currently to warrant an addiKonal set of weights on responsibility for the incident. The total losses across the four incidents are USD 628,000 which works out to an average of USD 157,052 for each incident. The SCRAM plakorm never has access to the raw inputs, so researchers do not have access to the magnitudes of any speciﬁc incident. However, by looking at the count of incidents in broad ranges, we can see that three of the losses were lower than US 100,000 and one loss was USD 500,000 or greater. We use the average and the general distribuKon of losses to model the impact of varying security controls on overall loss outcomes. Next, we examine the frequency of control failures and the losses aIributed to them. Figure 9 shows the number of failures aIributed for each of the controls across the 4 incidents. InteresKngly, the control for “deliver regular training” was implicated in 3 of the 4 incidents, and the “evaluate employee skills” was implicated in 2 of the 4. This highlights the diﬃcultly of training users to avoid behaviors such as clicking on malicious links or opening infected aIachments that can provide an entry point for aIackers. 16 Figure 9: Frequency of control failures, by control, total Other controls that failed more than once include “codify an incident response plan” and “test your incident response plan”. In many cases, the lack of an incident response plan can extend downKme and lead to higher losses. The controls for MFA, EDR, hunt for malicious acKvity, test backup data, and maintain and incident response plan all had one aIribuKon that led to a ﬁnancial loss. Again, when municipaliKes have an incident with a loss, they aIribute that loss to speciﬁc control failures represenKng that an implemented control failed or that a control was not implemented (up to 5). The ﬁnancial value of that loss is then allocated evenly across all the implicated controls. Then total losses from each control are summed together to produce a total loss amount per control. This data permits us to see which control failures are leading to the largest losses across the group. Figure 10 shows the ﬁnancial losses aIributed to each of the sub-controls. The controls related to user training have USD 247,000 of aIributed losses followed closely by incident response failures with USD 244,000 in losses. EDR failures are also high and account for USD 112,000 of the municipal losses. Finally, backup failures led to smaller losses of USD 15,000 while MFA was aIributed USD 12,000. User training and incident response failures alone accounted for 78% of the total of USD 628,000 in losses across the group. 17 Figure 10: AVributed losses from control failures, total, all municipali9es, USD These benchmarking results provide some insight into how municipaliKes see their own defensive posture, which controls fail the most frequently, and the losses aIributed to speciﬁc control failures. These results are a snapshot over Kme for a selecKon of municipaliKes in one state. Benchmarks such as these done across states, and parKcularly over Kme, can illuminate the evoluKon of criKcal infrastructure security, track conKnuous improvement over Kme, and highlight areas for speciﬁc targets for policymakers. Risk modeling results The benchmarking results can also be used to build models that forecast cyber risk based on an organizaKon’s own defense proﬁle. The results in this secKon bring together data on frequency, the Defense Gap Index scalar, and losses to create the cyber risk model. This secKon provides the results for each of the subsecKons and then the results of the overall risk model. Details of the risk modeling approach are available in Annex 2. Risk results The ﬁnal step is producing the risk forecasts based on the model in equaKon 1 with the ﬁndings from the data collecKon shown in equaKon 2. The results of these risk forecasts are shows below in Figures 11 and 12. Each of the ﬁgures show how forecasted risk increases or decreases with changes in the Defense Gap Index. (EQ 1) 𝐹𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦∗𝐷𝑒𝑓𝑒𝑛𝑠𝑒𝐺𝑎𝑝𝐼𝑛𝑑𝑒𝑥∗𝐼𝑚𝑝𝑎𝑐𝑡= 𝐴𝑛𝑛𝑢𝑎𝑙𝑅𝑖𝑠𝑘 (EQ 2) (0.016) ∗𝐷𝑒𝑓𝑒𝑛𝑠𝑒𝐺𝑎𝑝𝐼𝑛𝑑𝑒𝑥∗($157,000) = 𝐴𝑛𝑛𝑢𝑎𝑙𝑅𝑖𝑠𝑘 Figure 11 shows annual expected risk based on a municipality’s defense posture. The average risk of USD 2,523 for the average level of protecKon reﬂects the “fair price” for an insurance premium based on a pool with all 83 municipaliKes. In insurance parlance, this fair price is the equivalent of the expected loss for the pool, but does not include other internal costs, external costs, economic proﬁt needs, and capital costs that the insurance provider incurs to run its business. This means that the actual premium would need to be somewhat higher than the 18 calculated expected loss for the insurance company to operate. The “fair price” calculaKon also assumes that all costs would be covered in the case of an incident, but that is typically not the case as there are exclusions and deducKbles that lead to less than full coverage. The “fair price” calculaKons are imprecise, but they sKll provide a good guide for organizaKons to evaluate insurance oﬀers. Figure 12: Annual cyber risk forecasts by net weighted security control devia9on from group One of the key ﬁndings for policymakers is that focusing eﬀorts on organizaKons with the lowest security levels has a much higher return on investment than similar incremental improvements for organizaKons with above-average security levels. Our model shows that the risk for organizaKons with lower security is much higher than the average, with expected losses of over USD 15,000 per year if the weighted net security gap is 35% larger than the average organizaKon. Improving the security of organizaKons with the weakest defense postures would quickly improve the risk proﬁle of the overall pool given the distribuKon of losses. It is also worth noKng that improving the security of organizaKons that all already have net weighted security postures that are beIer than average does have posiKve returns, but at much lower levels than improving the security of ﬁrms that are much lower than average. Figure 12 shows the same trend but forecasts the ﬁnancial size of individual security incidents based on the security posture relaKve to the average. An organizaKon with the average security posture could expect an incident size of USD 157,000 when there is a successful aIack. However an organizaKon with a weighted net security gap that puts it 30% below average would expect an incident to cost USD 749,000 – nearly 5 Kmes the average. 19 Figure 12: Forecasted incident sizes by net weighted security control devia9on from group Leveraging the results The security and privacy guarantees of the mulK-party computaKon approach we use in SCRAM mean that we as researchers cannot see the input data or the individual security posture of the organizaKons that submit data into the plakorm. We only see the aggregated results which can be used to build models such as the ones above. We do, however, build tools that we distribute back to the parKcipants with these calculated models that allow them to privately evaluate their own risk forecasts based on the private data they submiIed. We, as the operators of SCRAM, do not have access to these individual forecasts that the parKcipants can put together using their own private data, but may in the future run a second computaKon to gather up the risk results and have a beIer picture oﬀ the overall risk landscape for the pool of parKcipants. Policy implica2ons The results of the computaKon with 83 municipaliKes provide some important insights at both the state and naKonal level for supporKng municipal cybersecurity and invesKng eﬀecKvely in security controls and training. This secKon provides policy consideraKons and recommendaKons at both levels emerging from the analysis. Securely built risk models provide valuable informa:on to policymakers One of the key ﬁndings of this research is that secure and private data collecKons can provide valuable informaKon to policymakers who otherwise may lack the informaKon they need to prioriKze cyber policy intervenKons and track their eﬀecKveness. The relaKvely small amount of data that was collected from each municipality was enough to benchmark defenses, idenKfy gaps, prioriKze controls, and build risk models. This data has been too sensiKve to disclose in the past, but new private computaKon techniques have made it possible to produce aggregated results across a peer group without any data disclosure. This makes new data sets available to policymakers that were never available before. With this pilot, we show what is possible with 20 data collected from these new secure computaKon technologies and hope to convert the results into usable risk metrics. The need for a standardized language for talking about and measuring risk In the past, the reluctance to pool informaKon meant there were no standardized deﬁniKons for cyber risk because the data was always considered too sensiKve to share. With the development of secure computaKonal techniques, there is now a need to harmonize the language that organizaKons and policymakers use to talk about cyber risk. For example, there is even confusion about the diﬀerence between an “event” and an “incident”, with the wording someKmes used interchangeably, and other Kmes used to diﬀerenKate between common anomalies versus infrequent larger issues. Now that secure computaKon is available, we need common set of language and tools for deﬁning and measuring cyber risk. Organiza:ons with low security levels beneﬁt most from policy interven:ons Another key ﬁnding of the paper is that policy intervenKons targeKng organizaKons with the lowest security posture provide highest return on investment in terms of risk reducKon. Policy intervenKons that focus on bringing up all organizaKons to a base level of security will have more overall risk reducKon than marginally improving the security of municipaliKes that have above-average security levels. Need for regular benchmarking and dynamic improvement Our focus of this pilot research is on the operators of criKcal infrastructure that support the quality of life we enjoy in communiKes. MunicipaliKes lack the same level of resources as large private ﬁrms to defend their networks, but these municipal operaKons are vital to the populaKon and can have life or death consequences if disrupted. As is noted earlier in the paper, typically data on cyber security defense posture, incident frequencies, and loss data are too sensiKve to share. As a result, we have very liIle insight to just how vulnerable municipaliKes and the criKcal infrastructure they operate are to cyber- aIacks. This benchmarking exercise oﬀers us a detailed view into the maturity level of security controls in one US state throughout its municipaliKes, and the need to have regular benchmarking exercises to idenKfy security gaps, evaluate policy intervenKons, and allocate resources eﬀecKvely. The data that we collect as part of this research is only one snapshot in Kme, but cyber security is an ever-evolving challenge for municipaliKes. The threat actors modify their approaches, and defenses need to evolve to counter them. As a result, one staKc snapshot in Kme is not enough for the dynamic improvements that municipaliKes must make to defend their assets. As a result, we believe that this research highlights the need to have benchmarking exercises on a regular basis to evaluate the progression of defenses and understand the risk that municipaliKes are facing. Having a cross-secKon and Kme series of data allows a much richer analysis and beIer policy recommendaKons. 21 Working toward a deﬁni:on of reasonable cybersecurity for municipal governments Security benchmarks for municipaliKes paired with informaKon on actual losses helps policymakers understand and work toward a deﬁniKon of reasonable cybersecurity for municipal governments. MunicipaliKes would beneﬁt from advice on which controls should be implemented and in which prioriKzed order. Dynamic Improvement and insurance-based schemes The decentralized nature of municipal government and decision-making in the United States makes it diﬃcult to coordinate eﬀecKve response to risks such as cybersecurity threats. There is a precedent for aligning the incenKves of local governments to reducing systemic risk through an insurance-based scheme: the NaKonal Flood Insurance Program (FEMA, 2020), in parKcular the Community RaKng System. The NaKonal Flood Insurance Program, administered by the Federal Emergency Management Agency (FEMA), oﬀers federally backed ﬂood insurance for homeowners and businesses located in high and moderate risk ﬂooding areas. The NFIP was created in 1968 by the passage of the NaKonal Flood Insurance Act [3]. The enactment of the NaKonal Flood Insurance Act was itself a response to a lack of market-based ﬂood insurance for high-risk areas and mounKng federal ﬂooding disaster assistance costs (NAIC, 2022). As part of remaining eligible to parKcipate in the NaKonal Flood Insurance Program, parKcipaKng communiKes, as they are known, must fulﬁll minimum NFIP requirements regarding proper ﬂoodplain management pracKces. “The Community RaKng System was created to encourage communiKes to establish sound programs that recognize and encourage ﬂoodplain management acKviKes that exceed the minimum NFIP requirements. By conducKng miKgaKon and outreach acKviKes that increase safety and resilience, including CRS credits for regulaKng to higher standards, communiKes can earn credits and discounts (up to 45 percent within the Special Flood Hazard Area) on ﬂood insurance premiums for property owners.” (FEMA, 2023a) ParKcipaKng communiKes that meet the higher CRS standards are eligible for reduced ﬂood insurance premiums. The CRS Coordinator Manual (FEMA, 2023b) details a series of ﬂood preparedness goals, and relates fulﬁllment of those goals with a reducKon of ﬂood insurance premium prices, up to a 45% discount. Each community parKcipaKng in CRS must aIest yearly that it is implemenKng its stated ﬂood miKgaKon policies in a process known as recerKﬁcaKon. CommuniKes are subject to a lengthier veriﬁcaKon process at the conclusion of three-year or ﬁve-year cycles, depending on community classiﬁcaKon and risk proﬁle. The MassachuseIs Municipal AssociaKon, in partnership with the MassachuseIs Interlocal Insurance AssociaKon, runs a cybersecurity liability insurance program (MIIA, 2023). A potenKal incenKve mechanism for encouraging municipaliKes to parKcipate in regular benchmarking and improvement in their cybersecurity posture would be to Ke favorable insurance rates to conKnuous improvement on the self-reported SCRAM assessment metrics, combined with regular audiKng by the insurance provider to ensure compliance. 22 Policy related to speciﬁc security controls This analysis has idenKﬁed three core areas of weakness among municipal cyber defenses that are responsible for signiﬁcant losses. The largest impact area is employee training and evaluaKng employee skills. Failures of these two controls accounted for 39% of all losses seen across the group, and three out of four incidents blamed employee training and evaluaKng skills as core reasons for the ulKmate ﬁnancial loss. These results are common across other computaKons we have run in other sectors. There is a nearly uniform distribuKon of responses across the quesKons related to evaluaKng employee skills and delivering regular training (Figures 13 and 14). The good news is that 36% of municipaliKes say they have fully implemented regular cyber training, and 24% reported a fully implemented program for evaluaKng employee skills. More concerning is the boIom end of the responses where 16 municipaliKes do not deliver regular training and 21 municipaliKes do not evaluate employee skills. Figure 13: Distribu9on of responses for evalua9ng employee skills, counts 23 Figure 14: Distribu9on of responses for delivering regular training, counts Employee skills is a criKcal area of weakness that all sectors need to address because human errors remain a core entry method for threat agents. In talking with municipaliKes, we realized that many organizaKons did not have the funding to run extensive training and felt they did not have the adequate skills to develop materials. This points to a potenKal role for state and naKonal governments to help bring together resources that can be leveraged for employee training to defend these municipaliKes. Incident response also had large losses associated with it that represented 39% of all reported losses across the municipaliKes. The largest losses were associated with codifying and incident response plan and tesKng that plan once it is ready. What was surprising is that the state has detailed resources for incident response planning that are available to the municipaliKes. AddiKonally, the insurance provider also has extensive incident response resources that are available at any Kme to the municipaliKes. This may imply that resources are available, but there is a lack of awareness about them. Finally, there were signiﬁcant losses associated with detecKng malicious acKvity, either through hunKng for malicious acKvity, or by deploying an endpoint detecKon and response system. These detecKon failures accounted for 18% of all reported losses across the group. One of the reasons that EDR and hunKng for malicious acKvity or criKcal is that the costs associated with an incident increase the longer it takes to detect. Systems that can quickly detect anomalous acKvity and allow organizaKons to remedy the problem will reduce the impact of events. Awareness raising As menKoned above, municipaliKes oNen were unaware of the resources that were available to them at the state level, and they did not know about support they could receive from the insurance provider when faced with an incident. One recommendaKon coming out of this research could be for awareness raising among municipaliKes about the resources currently available at their disposal. 16 21 16 30 Not implemented Partially implemented Largely implemented Fully implemented 5b. Deliver regular training 24 Materials / training at the state level Employee training and evaluaKng employee skills, as menKoned before, is a common failure point across the municipaliKes but also in other sectors of the economy. There may be a role for states to develop resources and training for cyber hygiene that can be provided to employees. It may make more sense to have high quality materials available for everyone than for each organizaKon to try and develop their own. MunicipaliKes could take elements developed at the state level and tailor them speciﬁcally to their own needs. Among the municipaliKes, there are many with similar roles and similar criKcal infrastructure porkolios, so even subgroups of municipaliKes could work together on uniﬁed training materials with state support. Evalua:ng the results of exis:ng programs Over the last decade, various programs have been implemented by states to support municipaliKes in building resiliencies. Of note among them include the Michigan Cyber Civilian Corps (MiC3) (NGA, 2017) , “a group of trained, civilian technical experts who individually volunteer to provide rapid response assistance to the State of Michigan in the event of a criKcal cyber incident”. The Corps is composed of IT professionals who already hold certain IT and security cerKﬁcaKons and parKcipate in addiKonal training regarding the nuances of working in the public sector. Other programs include regional CISO programs, where an individual paid by the state splits CISO responsibiliKes for mulKple municipaliKes at once (StateTech, 2020). A common resource is the provision of state-provided employee cybersecurity training such as those run in MassachuseIs (Commonwealth of MA, 2023). Crucially, data about the eﬃcacy about these well-intenKoned state programs has not been rouKnely collected or made available making It challenging to establish which programs are the best allocaKon of scarce state resources in service of municipal cybersecurity. Without the ability to benchmark progress, there is no way to gauge the impact of these programs and evaluate their eﬀecKveness. By building an ongoing benchmarking program, states could evaluate the eﬃciency and eﬀecKveness of these programs in diﬀerent groups of municipaliKes. UlKmately, this would help states prioriKze their security investments toward intervenKons with the most impact. While our focus in this paper has been on municipal cyber security, there are also lessons that we can learn from this data at the naKonal or broader level. For example, there are no standardized risk measures or benchmarks of control, maturity across municipaliKes, states, or even countries. One policy consideraKon could be encouraging standardized risk measures and security benchmarking across states, and potenKally across naKons that could be used for apples-to-apples comparisons. The larger the group of comparable data, the quicker that lessons learned can be disseminated and implemented across the organizaKons. There is also a need that goes beyond the state level for governments to encourage vendors and government agencies to work together on addressing the largest causes of cyber losses. For example, how can the broader public and private community work together to improve cyber 25 educaKon for employees and reduce the number of signiﬁcant incidents? Governments are large buyers of cyber security tools and they may have some leverage with suppliers to help create tools, standards, and use cases that work across diﬀerent sectors. Finally, benchmarking and standardized measures could be promoted at not just the state and naKonal level, but also with relevant internaKonal organizaKons to provide more opportuniKes for broader benchmarking, risk aggregaKon assessment, and research into the overall impact of cyber security on the economy. OrganizaKons such as the OECD could play an important role. Next steps and future research Future research on municipal cyber risk could track changes over Kme and how those changes aﬀect loss outcomes and overall risk measures. The research could also be expanded to include and compare municipaliKes from other states. AddiKonal research is needed to understand the relaKonship between defensive posture and losses to do accurate risk modeling. Future research could develop new ways to test for these correlaKons in a private and secure manner. The frequency of aIacks and successful aIempts are likely dependent on elements such as the municipality’s size or its aIracKveness as a target. Our analysis in this paper takes the frequency of aIacks as staKc, but future research could examine how to model the frequencies more precisely. 26 Bibliography BalKmore Sun, 2019a. BalKmore esKmates cost of ransomware aIack at $18.2 million as government begins to restore email accounts [WWW Document]. BalKmore Sun. URL hIps://www.balKmoresun.com/maryland/balKmore-city/bs-md-ci-ransomware-email- 20190529-story.html (accessed 9.10.23). BalKmore Sun, 2019b. Home sales are held up; BalKmore ransomware aIack cripples systems vital to real estate deals [WWW Document]. BalKmore Sun. URL hIps://www.balKmoresun.com/maryland/balKmore-city/bs-md-ci-ransomware-home- sales-20190514-story.html (accessed 9.10.23). Boustead, A., Shackelford, S., 2020. State of Hoosier Cybersecurity 2020. Commonwealth of MA, 2023. Municipal Cybersecurity Awareness Grant Program | Mass.gov [WWW Document]. Municipal Cybersecurity Awareness Grant Program. URL hIps://www.mass.gov/municipal-cybersecurity-awareness-grant-program (accessed 9.10.23). Courtney, R.H., 1977. Security risk assessment in electronic data processing systems, in: Proceedings of the June 13-16, 1977, NaKonal Computer Conference on - AFIPS ’77. Presented at the the June 13-16, 1977, naKonal computer conference, ACM Press, Dallas, Texas, p. 97. hIps://doi.org/10.1145/1499402.1499424 de Castro, L., Lo, A.W., Reynolds, T., Susan, F., Vaikuntanathan, V., Weitzner, D., Zhang, N., 2020. SCRAM: A Plakorm for Securely Measuring Cyber Risk. Harvard Data Science Review 2. hIps://doi.org/10.1162/99608f92.b4bb506a DeloiIe, NASCIO, 2018. 2018 DeloiIe-NASCIO Cybersecurity Study - States at Risk: Bold Plays for Change. NASCIO. URL hIps://www.nascio.org/resource-center/resources/2018- deloiIe-nascio-cybersecurity-study-states-at-risk-bold-plays-for-change/ (accessed 9.10.23). DeloiIe, NASICO, 2020. 2020 DeloiIe-NASCIO Cybersecurity Study - States at Risk: The Cybersecurity ImperaKve in Uncertain Times. NASCIO. URL hIps://www.nascio.org/resource-center/resources/2020-deloiIe-nascio-cybersecurity- study-states-at-risk-the-cybersecurity-imperaKve-in-uncertain-Kmes-2/ (accessed 9.10.23). Eling, M., Wirfs, J., 2019. What are the actual costs of cyber risk events? European Journal of OperaKonal Research 272, 1109–1119. hIps://doi.org/10.1016/j.ejor.2018.07.021 FEMA, 2023a. Local Government Oﬃcials - Floodplain Management Resources | FEMA.gov [WWW Document]. URL hIps://www.fema.gov/ﬂoodplain-management/manage- risk/local (accessed 9.10.23). FEMA, 2023b. Community RaKng System Coordinator’s Manual [WWW Document]. URL hIps://www.fema.gov/sites/default/ﬁles/documents/fema_community-raKng- system_coordinators-manual_2017.pdf (accessed 9.10.23). FEMA, 2020. Laws and RegulaKons | FEMA.gov [WWW Document]. URL hIps://www.fema.gov/ﬂood-insurance/rules-legislaKon/laws (accessed 9.10.23). Freund, J., Jones, J., 2014. Measuring and Managing InformaKon Risk: A FAIR Approach. BuIerworth-Heinemann. MIIA, 2023. MIIA - About MIIA [WWW Document]. URL hIps://www.emiia.org/about (accessed 9.10.23). 27 Mukhopadhyay, A., ChaIerjee, S., Bagchi, K.K., Kirs, P.J., Shukla, G.K., 2019. Cyber Risk Assessment and MiKgaKon (CRAM) Framework Using Logit and Probit Models for Cyber Insurance. Inf Syst Front 21, 997–1018. hIps://doi.org/10.1007/s10796-017-9808-5 NAIC, 2022. Flood Insurance/NaKonal Flood Insurance Program (NFIP) [WWW Document]. URL hIps://content.naic.org/cipr-topics/ﬂood-insurancenaKonal-ﬂood-insurance-program- nﬁp (accessed 9.10.23). NGA, 2017. NaKonal Governors AssociaKon Memo: Building a Civilian Cyber Corps (Memo). Norris, D.F., 2021. A Look at Local Government Cybersecurity in 2020 [WWW Document]. A Look at Local Government Cybersecurity in 2020. URL hIps://icma.org/arKcles/pm- magazine/look-local-government-cybersecurity-2020 (accessed 9.10.23). Norris, D.F., Mateczun, L., Joshi, A., Finin, T., 2021. Managing cybersecurity at the grassroots: Evidence from the ﬁrst naKonwide survey of local government cybersecurity. Journal of Urban Aﬀairs 43, 1173–1195. hIps://doi.org/10.1080/07352166.2020.1727295 Norris, D.F., Mateczun, L., Joshi, A., Finin, T., 2019. CyberaIacks at the Grass Roots: American Local Governments and the Need for High Levels of Cybersecurity. Public AdministraKon Review 79, 895–904. hIps://doi.org/10.1111/puar.13028 Preis, B., Susskind, L., 2022. Municipal Cybersecurity: More Work Needs to be Done. Urban Aﬀairs Review 58, 614–629. hIps://doi.org/10.1177/1078087420973760 Reuters, 2018. Atlanta oﬃcials reveal worsening eﬀects of cyber aIack. Reuters. Spiewak, R., 2022. Overlooking the LiIle Guy: An Analysis of Cyber Incidents and Individual Harms (Thesis). MassachuseIs InsKtute of Technology. Spiewak, R.L., Reynolds, T.W., Weitzner, D.J., 2021. Ransomware Readiness Index: A Proposal to Measure Current Preparedness and Progress Over Time (Working Paper). State of Louisiana, 2019. Cybersecurity Incident Resources | Oﬃce of Governor Jeﬀ Landry [WWW Document]. URL hIps://gov.louisiana.gov/page/cybersecurity-incident- resources- (accessed 1.31.24). StateTech, 2020. Michigan’s CISO as a Service Boosts Local Cybersecurity [WWW Document]. Technology SoluKons That Drive Government. URL hIps://statetechmagazine.com/arKcle/2020/04/michigans-ciso-service-boosts-local- cybersecurity (accessed 9.10.23). US Census, 2023. 2022 Census of Governments [WWW Document]. Census.gov. URL hIps://www.census.gov/data/tables/2022/econ/gus/2022-governments.html (accessed 1.31.24). US CISA, 2023. Report Ransomware | CISA [WWW Document]. URL hIps://www.cisa.gov/stopransomware/report-ransomware (accessed 1.31.24). US DHS, 2023. DHS Announces AddiKonal $374.9 Million in Funding to Boost State, Local Cybersecurity | Homeland Security [WWW Document]. URL hIps://www.dhs.gov/news/2023/08/07/dhs-announces-addiKonal-3749-million- funding-boost-state-local-cybersecurity (accessed 1.31.24). US White House, 2023. Biden-Harris AdministraKon Launches New Eﬀorts to Strengthen America’s K-12 Schools’ Cybersecurity [WWW Document]. The White House. URL hIps://www.whitehouse.gov/brieﬁng-room/statements-releases/2023/08/07/biden- harris-administraKon-launches-new-eﬀorts-to-strengthen-americas-k-12-schools- cybersecurity/ (accessed 1.31.24). 28 US White House, 2021a. ExecuKve Order on Improving the NaKon’s Cybersecurity | The White House 24. US White House, 2021b. White House Memo to Corporate ExecuKve and Business Leaders on Ransomware. US White House, 2018. The Cost of Malicious Cyber AcKvity to the U.S. Economy. 29 Annex 1: MIT RRI Ques?onnaire QuesKons answered as one of four levels of implantaKon: “Not”, “ParKally”, “Largely”, “Fully” 1a. Deploy mulK-factor authenKcaKon across the enterprise 2a. Deploy an endpoint detecKon and response (EDR) system / host-based IPS agent 2b. Hunt for malicious acKvity 3a. Encrypt data in transit 3b. Encrypt data at rest 4a. Remove barriers to sharing threat intelligence 4b. Receive external threat intelligence 5a. Evaluate employee skills 5b. Deliver regular training 6a. Perform regular backups of systems 6b. Test backup data 6c. Protect backups 6d. Store backups in oﬄine locaKon 7a. Deploy updates and patches in a Kmely manner 7b. Implement a centralized patch management system 7c. Apply patches using a risk-based approach 8a. Codify an incident response plan 8b. Test your incident response plan 8c. Maintain your incident response plan 9a. Establish an external penetraKon tesKng program 9b. Perform red team exercises 10a. Adopt network segmentaKon to ensure isolaKon of criKcal systems in an aIack QuesKons answered as a number: 11a. Number of signiﬁcant incidents over three years (sum of 2019, 2020, 2021) (see note*) 11b. Total cyber losses for all incidents combined over 3 years, US$, (sum of 2019, 2020, 2021) QuesKons answered via a selecKon: 12: If incidents are listed, the respondent must select up to 5 control failures that led to the loss. A screenshot of the quesKonnaire in an Excel spread sheet is shown below in Figure 15. 30 Figure 15: Screenshot of the MIT RRI Ques9onnaire for municipali9es 31 Annex 2: Technical details on crea?ng the risk models Data This section describes how use our secure data collection platform SCRAM to gather data from 83 municipalities in Massachusetts and use them to produce security benchmarks, risk models, and forecasts to understand the state of defenses, evaluate the risk, and provide a new feedback mechanism to evaluate state and national programs. The data were collected in June 2022 from the municipaliKes in MassachuseIs via a secure quesKonnaire prepared for the SCRAM plakorm. A descripKon of each of the data elements is provided below. It is important to note, however, that the results from a mulK-party computaKon are only aggregate ﬁgures and cannot be linked back to an individual organizaKon that submiIed data. The increase in privacy and security comes at the expense of no longer being able to see the individual inputs into a data aggregaKon. All data are self-reported by municipaliKes, and parKcipants are trained during a session on how to ﬁll out the forms and esKmate the data based on their own experiences. This training step is necessary to improve the comparability of the responses across ﬁrms. The collected data that are used in this analysis include: - Maturity level: 22 quesKons on the maturity level of controls (see Annex 1) - Incident count: 1 quesKon on the frequency of incidents - Control failures: Count of the Kmes individual controls failed leading to incidents - Financial costs (total): 1 quesKon on the total cost of incidents - Financial costs (control failures): Data on the aIributed costs of incident failures to speciﬁc controls Data: Maturity levels The data for this analysis include a set of 22 quesKons across 10 categories on the maturity of cybersecurity controls related to ransomware. We use the MIT Ransomware Readiness Index (RRI) quesKonnaire (Spiewak et al., 2021) as the set of controls municipaliKes use to provide a self evaluaKon for the computaKon. The RRI is a disKllaKon of the controls idenKﬁed by the 2021 White House ExecuKve Order on Cybersecurity (US White House, 2021a) and the US White House memorandum on ransomware (US White House, 2021b) as essenKal in assessing organizaKons’ cybersecurity readiness. In the RRI, parKcipants are asked to report their status (Not Implemented, ParKally Implemented, Mostly Implemented, and Fully Implemented) on 22 controls in 10 categories: MulKfactor AuthenKcaKon, Endpoint DetecKon and Response, EncrypKon, Empowerment, Training, Backup, Patching, Incident response, Checking the work, and SegmenKng. The answers to these quesKons are mapped to a 100% scale shown in Table 1. 32 Table 1: Implementa9on levels mapped to percentages for quan9ﬁca9on Not implemented ParKally implemented Largely implemented Fully implemented 0% 33% 67% 100% Data: Incident counts The data on incident counts asks organizaKons for the total number of incidents with a ﬁnancial loss greater than USD 1,000. The incident count data is the sum across all organizaKons in each of the ﬁve groups, in our case grouped by populaKon size. There were 4 incidents reported across 83 municipaliKes over the period of 3 years from 2019 to 2021). Data: Control failures OrganizaKons must idenKfy between 1 and 5 control failures that are responsible for the losses they report as part of an incident. The respondents are instructed to idenKty the top 5 controls responsible for all their incidents in cases where they have more than one incident to report. If an organizaKon reports an incident, then they must idenKfy at least one control failure. There are 14 control failures idenKﬁed across the 4 reported incidents, of which 9 are unique. The list of controls with failures is provider later in Table 3. Financial costs: Total The data include informaKon on the total ﬁnancial losses of all the incidents combined. The total ﬁnancial losses from the 4 cyber incidents over the 3-year period were USD 628,000. Financial costs: Controls The data collecKon also allocates reported losses across controls that were reported to fail. The total loss for each control is the porKon of losses for all incidents that were allocated to failures of that control. Each control has a total loss amount across all the municipaliKes. There are 9 controls with aIributed losses contributed by the 83 municipaliKes over the 3 years. Model For this paper, we propose a risk model that includes a frequency of incidents measure, a Defense Gap Index, and impacts (losses) as the three top-level components determining cyber risk. Assembling these three components together in a model lets us make changes to any of the three and then see how they impact an organizaKon’s risk. From a policy perspecKve, we care about how certain “levers” that aﬀect the each of the three aﬀect the overall quanKﬁed risk of the organizaKon. Our Defense Gap Index is comparable to SecT in the CRAM model (Mukhopadhyay et al., 2019), but we have named it diﬀerently to capture the dynamic that higher scores of the variable relate to higher risk. As a result, the name Defense Gap Index more accurately captures the dynamics of control variaKons on risk measures. Bringing the Defense Gap Index (a measure of security posture) up to the top level alongside frequency and impact allows us to work more directly with changes in control strength and their impact on risk outcomes. 33 In our model, the annual cyber risk for a ﬁrm is determined by the historical frequency of aIacks in the sector mulKplied by a defense gap index scalar and by the average impact of incidents for the sector (EquaKon 1). Our key modeling assumpKons for the municipal risk analysis are detailed in the secKon below. (Eq 1) 𝐹𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦∗𝐷𝑒𝑓𝑒𝑛𝑠𝑒𝐺𝑎𝑝𝐼𝑛𝑑𝑒𝑥∗𝐼𝑚𝑝𝑎𝑐𝑡= 𝐴𝑛𝑛𝑢𝑎𝑙𝑅𝑖𝑠𝑘 Frequency In our general model, outside factors can inﬂuence the frequency of aIacks including the sophisKcaKon of threat actors, the industry proﬁle, ﬁrm size, and the appeal of the protected assets to those wishing to exploit systems. In this municipal implementaKon however, we only have access to historical frequency rates based on our limited data collecKon. We assume that any outside factors (sophisKcaKon of actors, etc.) that can inﬂuence frequency are constant across the populaKon in the data collecKon. The frequency data is determined by a SCRAM computaKon that collects the total number of incidents and their magnitudes across the municipaliKes over a three-year period and then converts it into an annual frequency rate per municipality. The data are self-reported and private but are checked against informaKon we received from the insurance provider. Frequency calculaKon method: - Count of signiﬁcant incidents across group over 3-year period / number of ﬁrms x 3 Frequency AssumpKons: - All outside factors that can aﬀect frequency such as actor sophisKcaKon and assets being protected are assumed to be constant (similar) across all municipaliKes in the sample. - The impact of security controls is not captured in frequency. They are picked up in the defense gap index. Defense Gap Index The defense gap index is a way to adjust risk levels based on how an organizaKon’s control maturity compares to its peer group. The index is a scaler that captures deviaKons of weighted security control maturity levels from group averages and applies these deviaKons to a nonlinear model of losses to produce a scalar that impacts the risk forecasts. The innovaKon of the Defense Gap Index is that it uses actual loss data and aIribuKons to speciﬁc control failures from the group to weight each of the controls in the index. Then, variaKons away from the average are scaled according to these derived weights. Producing the Defense Gap Index is a mulK-step process as follows: Step 1: Allocate overall category weights between controls groups with and without losses In this ﬁrst step we decide how much importance to place on control failures that led to losses, the “loss group”, compared to controls that were not implicated in loss events, the “no-loss group”. 34 - In the municipality context, we allocate 85% of the total weight to the loss group which has 9 controls. We assign the remaining 15% to the no-loss group that has 13 controls. This breakdown of 85/15 was done to ensure that all controls with actual losses have higher weights assigned than controls without losses. Step 2: Allocate individual control weights within loss and no-loss groups In the second step, we decide how the weights should be subdivided across controls in the loss and no-loss groups. We prorate the 85% total weight in the loss group by the relaKve losses of individual controls. We assign equal weights across the 15% total weight in the no-loss group (See Figure 16 and Table 3). - For the municipaliKes, controls in the loss group receive a prorated share of the overall 85% weight for the indicator based on their total losses across the group. The controls in the no-loss group are all weighted equally across the remaining 15% of total weight. Figure 16: How weights are applied to controls with and without aVributed losses Note: The speciﬁc control weights and names are provided in Table 3. Step 3: Calculate the net weighted devia9on from the group average The third step looks for maturity diﬀerences from the group average and then weights them based on the control weights in Step 2. The diﬀerences are expressed as percentage diﬀerences from the group average, then then these are mulKplied by the individual control weights and then summed to create a net deviaKon score. (Eq 3) 𝑁𝑒𝑡𝑊𝑒𝑖𝑔ℎ𝑡𝑒𝑑𝐷𝑒𝑣𝑖𝑎𝑡𝑖𝑜𝑛= ∑ ((𝑂𝑤𝑛𝑆𝑐𝑜𝑟𝑒! −𝐺𝑟𝑜𝑢𝑝𝐴𝑣𝑒𝑟𝑎𝑔𝑒!) " !#$ ∗𝐶𝑜𝑛𝑡𝑟𝑜𝑙𝑊𝑒𝑖𝑔ℎ𝑡!) 35 Step 4: Create a loss model ﬁ[ng observed loss data to the net weighted devia9on for controls The fourth step evaluates and models the distribuKon of observed losses over the net weighted deviaKon for controls from Step 3. We assume that higher security (posiKve net weighted variaKons) corresponds to lower losses. We map the largest losses to the lowest security levels (e.g. 30% below average) and the smallest losses to the higher security levels (e.g. 20-30% above average). We also have the average loss and the average security level. With these 3 or more points in place, next we ﬁt a funcKon to the observed data that will be used to create the Defense Gap Index scalar. The loss data typically follow an exponenKal funcKon. - With this municipal data, we derived the following funcKon to map actual loss data do security variances. A detailed derivaKon of this value is provided in results secKon. (Eq 4) 𝑦= 𝑒%&.()*+ Step 5: Calculate the Defense Gap Index scaler: When we collect data on incident frequencies and impact, the resulKng numbers correspond to the average security posture across the sampled group. The Defense Gap Index converts deviaKons from this average into a scalar that changes the overall risk outcome. The ﬁnal step produces the Defense Gap Index scaler that is greater than 1 when maturity levels are lower than average, and less than 1 when an organizaKon’s defenses are beIer than their peers. The size of the scaler is dependent on the weights assigned in earlier stages, so gaps or areas of strength in one do not necessarily compensate 1:1 for gaps or areas of strength in other controls of a similar magnitude. - The equaKon for the defense gap index scaler for municipaliKes (Eq 5) 𝐷𝑒𝑓𝑒𝑛𝑠𝑒𝐺𝑎𝑝𝐼𝑛𝑑𝑒𝑥= 𝑒%&.()*∗-./0."12/.34.5"6/"7! The average maturity level of controls across the group is normalized at 1 as the base case against which an organizaKon’s own maturity level is evaluated. OrganizaKons with beIer security than the average receive a lower gap score, while those with worse security receive a higher gap score. Defense Gap Index CalculaKon: - Step 1: Allocate overall category weights between controls groups with and without losses - Step 2: Allocate individual control weights within loss and no-loss groups - Step 3: Calculate the net weighted deviaKon from the group average - Step 4: Create a loss model ﬁtng observed loss data to the net weighted deviaKon for controls - Step 5: Calculate the Defense Gap Index scaler Defense Gap Index AssumpKons 36 - We assume that any deviaKons away from the maturity average for controls with reported losses should be given more weight than controls that had no reported losses across the data collecKon. - The assigned 85% and 15% weighKng reﬂect our view that controls with observed losses should have the highest weighKng, but that controls without losses also play a role and need to be included. This is because successful controls may not have losses precisely because they are stopping the progression of aIacks. - When creaKng the loss-ﬁtng model, we assume that the largest losses are associated with the lowest security levels. This may not actually be the case, but we believe it is a safe assumpKon for these modeling purposes. Future data collecKons will allow us to test for correlaKons between security levels and losses. Losses Loss data are pulled from the aggregated data in the SCRAM computaKon. SCRAM outputs the sum of all losses and the average loss per incident. The research team cannot see individual entries, but we do create ﬂags for diﬀerent ranges of answers to understand the distribuKons of answers. For example, we could count the number of losses between USD 5,000 and USD 50,000 in one category, and between USD 50,000 and USD 500,000 in another. This provides us with visibility into the distribuKon of losses that is used to model the Defense Gap Index. Loss calculaKons - Sum of the losses from all reported incidents - The average loss per incident from the total losses divided by the count of incidents - A distribuKon of losses based on broad loss ranges Loss assumpKons - We assume that the data submiIed by ﬁrms on their losses is in the general range of actual amounts, but they do not need to be exact. We provide a spreadsheet to all parKcipants that helps them esKmate losses for diﬀerent kinds of incidents. Results Frequency results The municipal data put into the SCRAM data collecKon cover three years (2019, 2020, 2021) for 83 municipaliKes. Over the course of the three-year period, there are four incidents, which produces an expected frequency a single municipality having an expected successful cyberaIack with losses greater than USD 1,000 once every 62.5 years, or a 0.016 chance in any given year. The frequency value should be interpreted with cauKon over long-term forecasts because tacKcs and techniques constantly evolve, but it does oﬀer the best frequency esKmate we have since it is based on the most recent loss events. Defense gap index results The Defense Gap Index is simply a scaler based on the security state of the municipality that will increase or decrease the risk forecast based on deviaKons from the group average. We model a 37 band of +/- 30% from the average defense posture. As menKoned earlier, we assume that lower security levels result in higher losses when there is an incident. We use the losses from the benchmarking exercise to esKmate the top and boIom ends of our loss range, with some addiKonal headroom on the top end given our low number of incidents in the sample. The benchmarking data from the SCRAM computaKon gives us several points we can use to formulate a trendline. The data has one loss of USD 500,000 more, and three losses under USD 100,000. One of the smaller losses is greater than USD 50,000, and the remaining two are below that amount. We also know the average loss (USD 157,000) corresponds to the average security maturity across the group. We assume that organizaKons with security maturity levels that are 30% higher than the average have only a small risk of losses, while organizaKons with security levels that are 30% below average are more likely to have the largest losses observed, in our case greater than USD 500,000 per incident. We use a weighted measure for the Defense Gap Index that assigns higher weights to controls exhibiKng large losses and lower weights to controls with small or no losses across the group. We assign 85% of the total weight to security gaps for controls that have losses. We prorate the weights on each of the variable in the “loss group” according to the magnitude of their observed losses. The raKo of 85/15 aligns best to the data so that the lowest weights on controls with losses are higher than the weights assigned to each of the controls in the no-loss group. For example, the control “2a. Deploy EDR” has USD 11,529 in reported losses across the group and receives a weight of 1.6%, while “3a. Encrypt in transit” has no losses but sKll receives 1.2% of the total weight. Table 3 shows the observed losses and the prorated weights assigned that are used to calculate the Defense Gap Index scalar. One column shows what the weighKng would be if all controls were weighted equally, while the next column shows the prorated control rates weighted by reported losses. It is worth noKng that the prorated control weights are very high for the controls that had large reported losses. Training and evaluaKng employee skills together account for over 34% of the total weight applied to defensive posture. OrganizaKons with poor training or skill evaluaKons will have much higher Defense Gap Index numbers and corresponding high-risk forecasts. 38 Table 3: Observed losses and prorated control weights for the defense gap index, USD Control Observed losses Equal control weights Prorated control weights by losses: 85% prorated across losses 15% equally across non-losses 5b. Deliver regular training $130,780 4.5% 17.7% 5a. Eval employee skills $116,250 4.5% 15.7% 8a. Codify incident response plan $114,530 4.5% 15.5% 8b. Test incident response plan $114,530 4.5% 15.5% 2b. Hunt malicious activity $100,000 4.5% 13.5% 6b. Test backups $14,530 4.5% 2.0% 8c. Maintain incident response plan $14,530 4.5% 2.0% 1a. Deploy MFA $11,529 4.5% 1.6% 2a. Deploy EDR $11,529 4.5% 1.6% 3a. Encrypt in transit $0 4.5% 1.2% 3b. Encrypt at rest $0 4.5% 1.2% 4a. Remove sharing barriers $0 4.5% 1.2% 4b. Threat intelligence $0 4.5% 1.2% 6a. Regular backups $0 4.5% 1.2% 6c. Protect backups $0 4.5% 1.2% 6d. Store backups offline $0 4.5% 1.2% 7a. Timely updates & patching $0 4.5% 1.2% 7b. Centralized patch system $0 4.5% 1.2% 7c. Risk-based patching $0 4.5% 1.2% 9a. External pen testing $0 4.5% 1.2% 9b. Red team exercises $0 4.5% 1.2% 10a. Network segmentation $0 4.5% 1.2% Sums $628,208 100.0% 100.0% Once we have the prorated rates applied to each of the control diﬀerences, we sum them together to produce the net weighted security control deviaKon that is used to compute the Defense Gap Index scalar. This scalar increases or decreases the forecasted impact of incidents based on the organizaKon’s security posture. The next step is determining the shape of the loss funcKon relaKve to security posture. Loss/Impact results Cybersecurity losses are characterized by non-linear funcKons with infrequent large losses and a larger number of smaller losses (Eling and Wirfs, 2019). Our results from the benchmarking follow a similar trend with one large loss and three much smaller ones. We take these losses and map them across the maturity levels that are between +/-30% of the average maturity level for the group. We assume that the largest loss would be associated with a security raKng near 39 the -30% level, and the small losses at the other end. We also include revealed average loss for the average security posture. One ploIed, we esKmate a funcKon to capture the relaKonship (Figure 17). This funcKon is ﬂaIer among organizaKons with beIer security, but steeper (with losses getng larger faster) as the security level of the organizaKon declines relaKve to the average. Our ﬁIed exponenKal equaKon is shown below allows for losses greater than the USD 500,000 or larger loss that was observed in our data collecKon. This makes general sense as larger municipal losses have been reported in other states. FiIed equaKon using observed loss data: (Eq 4) 𝑦= 𝑒%&.()&+ Figure 17 shows the Defense Gap Index scalar in relaKonship to the net weighted security control deviaKon. For organizaKons with the average level of security maturity, the Defense Gap Index scalar is 1 and results forecasts of average losses. However, if the organizaKon’s net weighted security is 10% lower than average, the equaKon above produces a Defense Gap Index scalar of 1.6 which implies that losses will be 1.6 Kmes higher than average when there is an incident.4 By contrast, when an organizaKon’s net weighted security is 10% beIer than average, we forecast losses to be 41% lower than the average. Figure 17: Comparing weighted security control devia9on with the Defense Gap Index scalar The average reported loss for an incident was USD 157,000, and this is used as the benchmark for an organizaKon with the average level of security maturity. 4 The Defense Gap Index uses a weighted calcula@on that assigns higher weights to controls exhibi@ng large losses and lower weights to controls with small or no losses across the group.