1 Towards Automated Cyber Range Design: Characterizing and Matching Demands to Supplies Ekzhin Ear Department of Computer Science Uni. of Colorado Colorado Springs Colorado Springs, United States eear@uccs.edu Jose L. C. Remy Department of Computer Science Uni. of Colorado Colorado Springs Colorado Springs, United States jcastano@uccs.edu Shouhuai Xu Department of Computer Science Uni. of Colorado Colorado Springs Colorado Springs, United States sxu@uccs.edu Abstract—Cyber ranges mimic real-world cyber environments and are in high demand. Before building their own cyber ranges, organizations need to deeply understand what construction supplies are available to them. A fundamental supply is the cyber range architecture, which prompts an important research question: Which cyber range architecture is most appropriate for an organization’s requirements? To answer this question, we propose an innovative framework to specify cyber range requirements, characterize cyber range architectures (based on our analysis of 45 cyber range architectures), and match cyber range architectures to cyber range requirements. Index Terms—cyber range, architecture, requirements, metrics I. INTRODUCTION Cyber range is an emerging technology that can aptly leverage the dynamic and informative interactions between attackers and defenders in cyberspace to mimic a real-world cyber environment. As organizations across many industry sectors have increased their technological and cyberspace foot- print, their demand for cyber ranges have likewise increased. While there are commercial cyber ranges, there is a lack of treatment with scientific rigor, which is important because the current design of cyber ranges are heuristic or experience- based. As a consequence, customers have no choice but to passively adopt what is offered by cyber range vendors. In this paper, we advocate treating cyber range as a new kind of cybersecurity instrument and studying it with scientific rigor. This is important, for example, to help organizations choose the most appropriate cyber ranges according to their requirements, help organizations customize their own cyber ranges, and provide significant insights into future cyber range design and development. Our Contributions. We make two contributions. First, we formulate three research questions to guide the design of cyber ranges: (i) How should we specify cyber range requirements? (ii) How should we characterize cyber range architectures? (iii) How should we map available cyber range architectures to given cyber range requirements? Second, we propose an innovative conceptual framework to systematically address these research questions, by defining 22 attributes to specify cyber range requirements, proposing six cyber range archi- tectures, which we abstract from 45 real-world cyber range architectures; and proposing an algorithm that matches cyber range architectures to satisfy an organization’s needs. Paper Outline. The rest of the paper is organized as follows. Section II details the proposed framework. Section III reviews related prior studies. Section IV concludes the paper. II. THE FRAMEWORK As highlighted in Figure 1, our framework has three components: (i) characterizing cyber range requirements; (ii) characterizing cyber range architectures; and (iii) designing an algorithm to match cyber range architectures to cyber range requirements. These components are elaborated below. Architecture Functional Architecture Structure Capabilities Logical Topology Requirements Scope Purpose Constraints Matching Dataset Matching Process Fig. 1. Illustration of our framework. A. Characterizing Cyber Range Requirements Cyber range demand varies greatly in terms of organiza- tions’ requirements. For example, we see a preponderance of cyber ranges created for cybersecurity training purposes [1]– [5] and for testing cyber-physical systems [6]–[8]. There are many additional use cases (such as leveraging cyber ranges as an instrument for conducting cybersecurity research) that highlight the need to accurately characterize cyber range re- quirements. Fig. 2 outlines the three requirement sets, namely purpose, scope, and constraints, with their associated attributes highlighted in Table I and elaborated below. Requirement 1: Purpose. This requirement describes the context for utilization of the cyber range and expresses its objectives in the following attributes. • Employment: This attribute captures the overall opera- tional purpose of the cyber range. For example, several commercial vendors, such as Offensive Security, offer certifications through their cyber range scenarios. In this case, the purpose is to evaluate and certify candidates. • Sector: This attribute identifies the sector of coverage for the cyber range, especially what types of objectives the Copyright © 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. arXiv:2307.04416v1 [cs.CR] 10 Jul 2023 Requirements Purpose Employment Teaming Scope Updateability Availability Integration Tutoring Scoring Sector Constraints Build Speed Staff Budget Security Physical Domain Concurrency Connectivity Federation Duration Retention Fidelity Scalability Latency Fig. 2. Taxonomy of cyber range requirements. cyber range will be used for (e.g., academic requirements can differ greatly from military ones). • Teaming: This attribute identifies the participants and describes the attack-defense interactions between them. Many publicly available cyber range scenarios, such as PicoCTF and Root-Me, focus almost exclusively on the red (i.e., attacker) team, while a few others focus on blue (i.e., defender) team training. However, there is a growing demand to mimic the complex interactions of both teams. • Scoring: This attribute identifies how user activity in the cyber range will be scored. This is particularly impor- tant for assessing the capabilities of red and blue team members for real-world cyber environments. • Tutoring: This attribute specifies the sophistication of the instructional function of the range. This is particularly relevant to education- and training-oriented cyber ranges. Requirement 2: Scope. This requirement describes the extent of functionality and usage of the cyber range. Deployment size and complexity can vary significantly. For example, the Kypo Cyber Range Platform can facilitate hundreds of users, while the Kypo Cyber Sandbox Creator is for a single user via low-resourced virtual machines (VM) [1]. This requirement is characterized by the following attributes. • Domain: This attribute describes the application domain to mimic, e.g., IT versus OT networks. • Federation: This attribute describes the required integra- tion with other cyber ranges, such as integrating cyber ranges into a European cyber range ecosystem [9]. • Concurrency: This attribute describes the average concur- rent user usage (e.g., a cyber range may be designed for use by 10 users at the same time [10]). • Connectivity: This attribute describes the methods of user connectivity (e.g., users may require local on-site access to a cyber range vs. remote access via a gateway). • Fidelity: This attribute is the degree of exactness of the cyber range to real-world systems and networks, measured by the average ratio of simulation to emula- tion employed. Simulation is a higher abstraction of the entity mimicked which only captures specific properties desired, while emulation mimics a more substantial set of properties, representing the entity more fully. • Duration: This attribute is the continuous deployment duration required. For example, NATO’s cyber range for Locked Shields requires more than 30 days in duration, even though the exercise completes in two days [11]. • Availability: This attribute describes the user time usage regime, such as whether cyber range scenarios will be deployed on-demand or will be continuously available. • Retention: This attribute is the required duration of cyber range scenario data retention (e.g., a research-oriented cyber range scenario may require 6 months of retention to facilitate research investigations). • Integration: This attribute identifies the internal integra- tion between scenarios (e.g., two red team scenarios may integrate into a new complex blue team defense scenario). • Updateability: This attribute is the degree of planned up- dates to the infrastructure and scenario sets. For example, one cyber range design may require incremental updates to ensure its relevance to smart campus [12]. • Scalability: This attribute is the degree of planned in- crease in purpose and scope, where the degree is mea- sured as the average increase across the range of values of purpose and scope attributes. Requirement 3: Constraints. This requirement describes con- straints imposed on the cyber range designer and are associated with the attributes of purpose and scope requirements. • Budget: This attribute is the annual monetary funds available for construction, lifecycle, and maintenance. • Build Speed: This attribute is the specified average maxi- mum time allowed to provision and deploy a cyber range, and/or its scenario set. • Latency: This attribute is the average permissible network and system delay time in the cyber range. • Staff: This attribute is the number of full-time per- sonnel available to support administration and sched- uled/unscheduled maintenance. • Physical: This attribute is the non-contiguous physical space available to house and facilitate scenarios. • Security: This attribute describes the level of data and process protection required to secure the cyber range itself and to mitigate the potential damages or attacks that may originate from the cyber range against others. Given the preceding set of 22 attributes, where each can take multiple values, there are many combinations of cyber range characteristics. This means that it is not feasible to manually characterize these numerous combinations. B. Characterizing Cyber Range Architectures Defining Cyber Range Architectures. From our study of 45 architectures discussed in the literature, ranging from federated enterprise business [9], to space satellite [13], to maritime systems [5], we define architectures (as depicted in Fig. 1) to systematically describe the design of cyber ranges. We note that cyber ranges are typically defined by their functionality. Hence, functional architecture sits at the top of the framework. Users interact with the cyber range at this layer. TABLE I REQUIREMENTS ATTRIBUTES AND THEIR RANGE OF VALUES. C* Attribute Range of Values Employment education, testing/experimentation, research and development, training/certification, multiple Sector academic, military/government, commercial, multiple Teaming yes(red, blue, both), none Scoring yes(task based, jeopardy, flag, multiple), none Tutoring yes(text, images, multimedia), none Domain IT enterprise, networking, OT/cyber-physical/IoT, critical infrastructure, cloud based systems, multiple Federation stand alone, integrated Concurrency <100 users, 100-999 users, >1,000 users Connectivity local, remote, both Fidelity L (sim), M (sim/emulation), H (emulation) Duration L (<1 day), M (1-30 days), H (>30 days) Availability L (on-demand), M (on-demand), H (continuous) Retention L (<1 month), M (1-12 months), H (>1 year) Integration stand alone, integrated Updateability L (seldom), M (incremental), H (full) Scalability L (static), M (one degree), H (multiple degree) Budget L (<$50k), M ($50k-$250k), H (>$500k) Build Speed L (<60 seconds), M (1-60 minutes), H (>1 hour) Latency L (<100 ms), M (100-800 ms), H (>800 ms) Staff L (<10 people), M (10-50 people), H (>50 people) Physical L (<5K ft2), M (5K-50K ft2), H (>50K ft2) Security L (no sensitive), M (PII/proprietary), H (classified) Purpose Constraints Scope *C = Category; L = low; M = medium; H = high; IT = Information Technology; IoT = Internet of Things; ms = milliseconds; OT = Operational Technology; sim = simulation The functional architecture describes the runtime environment, where assets are deployed to perform their functions (e.g., se- curity, network, storage) and integrate with each other across a cyber range. It also describes management functions (typically found in the cantonment area of a cyber range) that allows for the deployment, maintenance, and customization of portals (i.e., user interfaces), user roles (i.e., access and permissions), environment resources (e.g., memory, storage), monitoring (e.g., network traffic), and the scenario lifecycle (i.e., creation, modification, deployment, generation, and execution). At the next layer, capabilities describes the tools that enable the use and functionality of a cyber range. One difference of the cyber domain from the kinetic domain is that capabilities often come in the form of software. We cataloged 350 tools, including cybersecurity tools, and gained the following insight: Insight 1: Most free/publicly available cybersecurity tools (205 of 241) are for offensive or forensics purposes. Insight 1 makes sense as vendors have financial incentives to produce commercial defensive risk and incident management tools. By contrast, most commercial companies are not paying for offensive cyber capabilities nor technical forensics tools. The logical topology describes the various networks that support the administration and scenario deployment of the cyber range including the degree of cloud employment. In ex- ploring potential topologies, we gained the following insight: Insight 2: Within a highly managed cyber range, administra- tive data and processes will likely interfere with the objectives of high fidelity scenarios (e.g., experiments and certification). Insight 2 emphasizes that highly managed cyber ranges have sensors deployed across the cyber range for management, monitoring, and security, which cyber range scenario develop- ers must account for. These hardware and software sensors, as well as the data and traffic they collect, can introduce foreign artifacts that, for example, could corrupt the control group of an experiment, or pollute the exam network of a certification. Lastly, the structure defines the physical components of the cyber range. Modern hardware systems are complicated to configure and ensure compatibility, as well as expensive to procure, maintain, and replace. Hence, organizations must exercise significant foresight in structure design. Designing Reference Architectures. We propose six refer- ence architectures. What are common to these architectures include: (i) each architecture contains a cantonment cluster responsible for management and administration of the cyber range; (ii) each cantonment cluster can leverage similar tools to enable their management capabilities (e.g., syslog for monitor- ing); and (iii) a DMZ is used as an example security implemen- tation, which is replaceable with other security approaches. Reference Architecture 1: Pure Physical. The defining charac- teristic of this architecture is that all systems and appliances are physically the same as their real-world counterparts that they intend to mimic. Its capabilities are also the same in this regard (e.g., routing and segmentation are accomplished via physical network appliances). Fig. 3 depicts a logical topology and structure with typical use cases directly relatable to physical devices. The enterprise network clusters illustrate the potentially high resource demands of this architecture. Enterprise Network Clusters Network Appliance Clusters Internet DMZ Cantonment Cluster Cyber-Physical Clusters Digital Forensics Lab WAN Fig. 3. Pure physical reference architecture depicting cyber-physical sensors, network appliances for hardware testing, and hardware for digital forensics. Reference Architecture 2: Centrally Virtualized. This architec- ture emphasizes virtualization technology over physical de- vices, significantly decreasing the complexity of the structure. Hence, tools like Xen Server, KVM, and VirtualBox are key capabilities to provide very high fidelity emulation. Fig. 4 depicts the logical/physical topology where entire enterprise network clusters are virtualized within a server stack, as well as all cantonment cluster functionalities. Internet DMZ PetaSAN Servers Cantonment Data Scenario Data VM Data/Backups XCP-NG Servers Xen Orchestra UI Portal Monitoring Management Scenario Clusters Example Virtualized Scenarios Fig. 4. Centrally virtualized reference architecture with example capabilities (e.g., PetaSAN and XCP-ng) to enable storage pooling and virtualization. Reference Architecture 3: On-Premise Cloud. Cloud properties (e.g., self service capable, resource pooling, rapidly elastic) make this architecture distinct, while its logical/physical topol- ogy is quite similar to Fig. 4 because private clouds are enabled by virtualization (e.g., via XCP-ng). Openstack is the main capability to create a private cloud. However, this comes with immense complexity, comprising of over 30 services, countless configurations files, and at least a dozen nodes. Reference Architecture 4: Public Cloud. The public cloud approach benefits from cloud properties while offloading cloud maintenance tasks to third parties that have seemingly limitless resources (e.g., compute), though at a price. Further, the management plane provides robust and mature capabilities. It also natively supports automation tools like Terraform, though core cloud capabilities are proprietary. Its topology is also similar to Fig. 4, except it resides outside the organization. Reference Architecture 5: Distributed Virtualization. This ap- proach emphasizes the use of remote physical nodes (e.g., end-user desktops) to house cyber range scenarios. While a cantonment cluster centrally manages the overall cyber range, more control resides with the end-user (i.e., root user of the desktop). Fig. 5 illustrates the logical/physical topology. A use case is a classroom of students who only interact with a few VMs in simple scenarios. Virtualization, micro-cloud, containerization, and automation tools such as VirtualBox, MicroStack, Docker, and Vagrant are key capabilities. Internet DMZ Cantonment Cluster Desktop Software Stack Management Agent (i.e., environment, monitoring, traffic) Environment Deployment (e.g., MicroStack, Vagrant, VirtualBox, Kubernetes, Docker) Fig. 5. The distributed virtualization reference architecture where low resource remote nodes can host VMs, containers, and/or micro-clouds. Reference Architecture 6: Hybrid. This architecture leverages the strengths and capabilities of the other approaches, bringing them together into a single architecture of varying densities. Fig. 6 depicts the architecture where: a public cloud hosts low-resourced, non-persistent VMs (decreasing subscription cost); a private cloud hosts persistent, data-rich, and heavy workload scenarios (leveraging cloud properties, e.g., resource pooling); an on-premise centrally virtualized environment han- dles medium persistent workloads (decreasing staffing cost); distributed virtualization facilitates light persistent scenar- ios (decreasing cost and administration); and an on-premise physical network supports cyber-physical testing (supporting required objectives). However, this approach has the drawback of increasing the overall complexity of the architecture. On-Premise Virtualized Medium Workload Persist. Scenarios OpenStack Horizon Example Virtualized Scenarios On-Premise Cyber-Physical Clusters Internet DMZ Distributed Systems - Software Stack Management Agent Environment Deployment (Light Persistent Scenarios) Public Cloud Light Workload Non- Persistent Scenarios Example Light Scenarios On-Premise Cloud OpenStack Compute Heavy Workload Scenarios On-Premise Cloud OpenStack Storage Cantonment Data Scenario Data VM Data/Backups Cantonment Cluster Fig. 6. Hybrid reference architecture incorporating other architectures. Through our design of the six reference architectures, we gained the following insight: Insight 3: It is best to employ well-maintained infrastructure capabilities and not inject custom code patching. Insight 3 speaks to the complexity of cyber range infras- tructures. Well-supported hardware and software capabilities (i.e., by regularly issued vendor patches) allow the cyber range to grow and mature with new innovations. Conversely, customizing cyber range deployments by directly patching code (e.g., in the XCP-ng codebase) has a greater risk of breaking functionality and snowballing overhead requirements. Defining Metrics to Characterize Cyber Range Architec- tures. Appropriate characterization of cyber range architec- tures prior to technical design and procurement saves money and time. This prompts us to define four categories of metrics in light of the requirement attributes we have discussed. Metric Category 1: Scope. In consideration of the scope of the cyber range demand, we define two metrics. Extensibility is the degree of ability to shift across the possible values of the various scope requirement attributes (e.g., from a pure IT enterprise domain to cyber-physical). Capacity is the degree of ability to achieve the highest value levels across the various scope requirement attributes (e.g., > 1, 000 concurrent users). Metric Category 2: Performance. Concerning the performance of the cyber range, we define two metrics. Build Speed is the amount of time (in seconds, minutes, or hours) it takes to provision and deploy a scenario set. Latency is the amount of average network and system delay time (in milliseconds). Metric Category 3: Cost. Likewise, the cost of the cyber range is defined in two metrics. Budget is the amount of annual monetary funds required to construct and maintain the cyber range. Staff is the number of personnel required to support administration and maintenance of the cyber range. Metric Category 4: Security. We propose using standard security metrics: confidentiality, integrity, availability, non- repudiation, authenticity, and privacy. Analyzing Reference Architectures. We apply these metrics at the ordinal scale and qualitatively via a Likert Scale approach [14]. We compare the reference architectures (of a sizeable deployment) to an average cyber range deployment scenario (i.e., to a medium-sized organization’s enterprise business environment). Table II summarizes our analysis, enumerating our qualitative results and the significant strengths and weaknesses of each architecture per metric category. C. Matching Cyber Range Architectures to Requirements Building a Matching Dataset. This dataset contains the matching of each requirement attribute to each of the six archi- tectures. We make the following definitions: (i) Supportability: the level of ability a reference architecture is able to facilitate a requirement attribute. (ii) Significance: the level of importance of a requirement attribute given its value (e.g., significance of a budget value of low is different than medium). We apply our domain expertise to assess supportability by qualitatively scoring each architecture per attribute, and significance by providing weights for each attribute per value (where the range of values are enumerated in Table I). We then store the resulting dataset as a CSV with the following columns: (i) attribute name; (ii) attribute value; (iii) attribute weight: the weight assigned to the attribute value; (iv) architecture scores: for the remaining six columns, the assigned supportability score for each reference architecture. Automating the Matching Process. We automate the match- ing process (as depicted in Fig 1) via a python script that ingests the matching dataset and requirements of interest. It outputs a score for each reference architecture, along with a heat map to visualize and explain the score. At a high level, the score reflects to what extent an architecture satisfies the given cyber range requirements; the architecture with the highest score can be used to build a cyber range. Core functions of the script are given below: 1 def score_lookup(user_input, matching_dataset): 2 for key, value in user_input: 3 select line from matching_dataset where (attribute == key and value == value) ,→ 4 append line to score_lookup_df 5 return score_lookup_df 6 def score_calculation(score_lookup_df): 7 for architecture column in score_lookup_df: 8 append sum(score_looup_df.weight * architecture.score) to architecture_scores_df ,→ 9 return architecture_scores_df The score lookup function retrieves the applicable CSV line from the matching dataset for every requirement at- tribute value selected by the user and provides it to the score calculation function, which computes the score for each reference architecture by summing the products of each requirement attribute value’s weight and architecture score. It outputs the score totals and provides this computed data to a heat map generation function. Fig. 7. Heat map depicting a sample result, where the hybrid architecture is recommended especially because of the strong match in terms of domain, availability, retention, scalability, and latency requirements. Fig. 7 shows a sample output with reference architectures indicated in the y-axis and requirement attributes indicated in the x-axis. At a high level, the heat map provides explainability to the result by visualizing how requirements have impacted the selection of reference architectures. III. RELATED WORK We divide related prior studies into three categories. First, to catalog cyber ranges, there are studies on characterizing cyber range functions [15], [16], on conducting questionnaire- based survey of cyber range components and tools [17], on recommending cyber ranges according to objectives [18], and on summarizing cyber range infrastructures and capture-the- flag environments [19]. The present study goes much beyond these studies in terms of comprehensiveness, especially con- cerning architectures. Second, there are studies on creating and managing cyber ranges [20], on modeling and detecting flaws in cyber range-based training [21], on qualitatively analyzing a particular cyber range [22], and on assessment methodologies for cybersecurity training platforms [23]. The present study goes beyond them by proposing a systematic set of attributes and metrics (e.g., extensibility and capacity) for assessing and characterizing cyber range architectures. Third, there are studies on extending on-premise cyber ranges to public [24] and private cloud infrastructures [25], and on combining virtualized and physical devices into a hybrid cyber range [26]. The present study goes beyond these studies by proposing a systematic set of cyber range reference architectures. IV. CONCLUSION FOR FUTURE WORK We have presented a framework for specifying cyber range requirements, characterizing cyber range architectures, and matching cyber range architectures to requirements. This study represents a significant first step towards automating the design of cyber ranges according to requirements. The present study has several limitations which need to be addressed in the future. First, the matching algorithm heavily relies on our domain expertise; a more intelligent algorithm TABLE II QUALITATIVE ANALYSIS OF OUR REFERENCE ARCHITECTURES, ANNOTATED WITH APPLICABLE STRENGTHS (+) AND WEAKNESSES (-). Metric Pure Physical Centrally Virtualized On-Premise Cloud Public Cloud Distributed Virtualization Hybrid Scope - Extensibility & Capacity Weaker + high fidelity scenarios - bounded by hardware - require re-engineering - new deployment challenges Stronger + abstracted from physical architecture + leverage clustering of resources to a degree Stronger + abstracted from physical architecture + leverage clustering + leverage cloud properties Much Stronger + abstracted from physical + leverage cloud properties + immense extensibility and capacity Weaker + inherent ability to add additional nodes - lack of resources - non-uniform end-user nodes Much Stronger + leverage cloud capacity and extensibility + mitigate physical system deployment limitations Performance - Build Speed & Latency Stronger + direct access to hardware + network connectivity intentionally designed - less automatable provisioning/deployment Much Stronger + hardware resources more directly employed than cloud + automatable provisioning and deployment Stronger + Cloud properties can improve build speed + computing prioritization - farther abstraction from physical hardware resources Comparable + Cloud properties can improve build speed + provisioning can be prioritized via computing - varying network distance Weaker - less available resources - varying network distance - less automatable - unoptimized host operating systems Much Stronger + leverage direct hardware efficiencies + leverage cloud properties and virtualization automation Cost - Budget & Staff Much Weaker - requires replica staff and budget of enterprise/real-world networks Comparable + less complexity for staff than cloud implementation + rigidly defined budget - virtualization experts Weaker + rigidly defined budget - substantial cloud technical and management experts Much Weaker + less technical staff - cloud management experts - unpredictable and fluctuating subscription rates Stronger + less hardware and staff - more difficult if staff provides BYOD support Weaker - heterogeneous network and systems require more budget and expert staff Security - CIANA-P Much Weaker - heterogenous systems - large attack surface - direct access to hardware - dispersed geographically Stronger + virtual segmentation - can become overly complex and abstracted Stronger + built in Identity mgmt + built in segregation - can become overly complex and abstracted Much Stronger + professionally managed - mgmt plane complexity - collateral damage from attack on cloud vendor Much Weaker - heterogenous systems - cumbersome policies - dispersed geographically (difficult for troubleshooting) Weaker - Increased complexity and heterogenity - larger attack surface CIANA-P = Confidentiality, Integrity, Availability, Non-Repudiation, Authenticity, Privacy. Blue/(gray) cells indicates most/(least) advantageous architecture for given metric set. Qualitative scoring: Much Weaker → 1; Weaker → 2; Comparable → 3; Stronger → 4; Much Stronger → 5 is needed. Second, we used qualitative metrics to assess the reference architectures; future study needs to incorporate quantitative metrics (e.g., [27]–[32]). Third, we focused on assessing general reference architectures; we want to explore more industry- and sector-specific architectures in the future. Acknowledgement. This work was supported by NSF Grants #2122631 and #2115134, and Colorado State Bill 18-086. REFERENCES [1] J. Vykopal, P. ˇCeleda, P. Seda, V. ˇSv´abensk`y, and D. Tovarˇn´ak, “Scalable learning environments for teaching cybersecurity hands-on,” in 2021 IEEE Frontiers in Education Conference (FIE), pp. 1–9, IEEE, 2021. [2] R. Beuran, D. Tang, C. Pham, K.-i. Chinen, Y. Tan, and Y. Shinoda, “Integrated framework for hands-on cybersecurity training: Cytrone,” Computers & Security, vol. 78, pp. 43–59, 2018. [3] S. Karagiannis, Systematic Design, Deployment and Evaluation of Gamified Cybersecurity Learning Environments. PhD thesis, 2022. [4] T. Debatty and W. Mees, “Building a cyber range for training cyberde- fense situation awareness,” in 2019 ICMCIS, pp. 1–6, IEEE, 2019. [5] G. Potamos, A. Peratikou, and S. Stavrou, “Towards a maritime cyber range training environment,” in 2021 IEEE CSR, pp. 180–185, 2021. [6] S. Ahmad et al., “Design and implementation of a network of specialized and hybrid cyber-ranges,” 2021. [7] S. J. Coshatt, Q. Li, B. Yang, S. Wu, D. Shrivastava, J. Ye, W. Song, and F. Zahiri, “Design of cyber-physical security testbed for multi-stage manufacturing system,” in IEEE GLOBECOM, pp. 1978–1983, 2022. [8] A. Ronkainen, F. Teye, M. Koistinen, J. Kaivosoja, L. Pesonen, and P. Suomi, “Mtt cropinfra,” in Testbeds and Research Infrastructure. Development of Networks and Communities: ICST, Springer, 2012. [9] C. Vir´ag, J. ˇCegan, T. Lieskovan, and M. Merialdo, “The current state of the art and future of european cyber range ecosystem,” in IEEE CSR, pp. 390–395, IEEE, 2021. [10] T. Lieskovan and J. Hajn`y, “Building open source cyber range to teach cyber security,” in ARES, pp. 1–11, 2021. [11] M. Smeets, “The role of military cyber exercises: A case study of locked shields,” in CyCon, vol. 700, pp. 9–25, IEEE, 2022. [12] Z. Tian, Y. Cui, L. An, S. Su, X. Yin, L. Yin, and X. Cui, “A real-time correlation of host-level events in cyber range service for smart campus,” IEEE Access, vol. 6, pp. 35355–35364, 2018. [13] M. Luglio, C. Roseti, and F. Zampgnaro, “A satellite network emulation platform for implementation and testing of tcp/ip applications,” in Testbeds and Research Infrastructure. Development of Networks and Communities: ICST, pp. 1–2, Springer, 2012. [14] K. A. Batterton and K. N. Hale, “The likert scale what it is and how to use it,” Phalanx, vol. 50, no. 2, pp. 32–39, 2017. [15] M. M. Yamin, B. Katt, and V. Gkioulos, “Cyber ranges and security testbeds: Scenarios, functions, tools and architecture,” Computers & Security, vol. 88, 2020. [16] E. Ukwandu, M. A. B. Farah, H. Hindy, D. Brosset, D. Kavallieros, R. Atkinson, C. Tachtatzis, M. Bures, I. Andonovic, and X. Bellekens, “A review of cyber-ranges and test-beds: Current and future trends,” Sensors, vol. 20, no. 24, p. 7148, 2020. [17] N. Chouliaras, G. Kittes, I. Kantzavelou, L. Maglaras, G. Pantziou, and M. A. Ferrag, “Cyber ranges and testbeds for education, training, and research,” Applied Sciences, vol. 11, no. 4, p. 1809, 2021. [18] J. Davis and S. Magrath, “A survey of cyber ranges and testbeds,” 2013. [19] S. Kucek and M. Leitner, “An empirical survey of functions and con- figurations of open-source capture the flag (ctf) environments,” Journal of Network and Computer Applications, vol. 151, p. 102470, 2020. [20] V. Orbinato, “A next-generation platform for cyber range-as-a-service,” in IEEE ISSREW, pp. 314–318, IEEE, 2021. [21] M. Macak, R. Oslejsek, and B. Buhnova, “Applying process discovery to cybersecurity training: An experience report,” in 2022 IEEE EuroS&PW, pp. 394–402, IEEE, 2022. [22] I. Priyadarshini, Features and architecture of the modern cyber range: a qualitative analysis and survey. University of Delaware, 2018. [23] R. Beuran, J. Vykopal, D. Belajov´a, P. ˇCeleda, Y. Tan, and Y. Shinoda, “Capability assessment methodology and comparative analysis of cyber- security training platforms,” Computers & Security, p. 103120, 2023. [24] R. Beuran, Z. Zhang, and Y. Tan, “Aws ec2 public cloud cyber range deployment,” in IEEE EuroS&PW, pp. 433–441, IEEE, 2022. [25] A. P. Luise, G. Perrone, C. Perrotta, and S. P. Romano, “On-demand deployment and orchestration of cyber ranges in the cloud.,” in ITASEC, pp. 80–91, 2021. [26] H. Farhat, “Design and development of the back-end software architec- ture for a hybrid cyber range,” 2021. [27] M. Pendleton, R. Garcia-Lebron, J.-H. Cho, and S. Xu, “A survey on systems security metrics,” ACM Comput. Surv., vol. 49, pp. 62:1–62:35, Dec. 2016. [28] J.-H. Cho, S. Xu, P. M. Hurley, M. Mackay, T. Benjamin, and M. Beaumont, “Stram: Measuring the trustworthiness of computer-based systems,” ACM Comput. Surv., vol. 51, no. 6, pp. 128:1–128:47, 2019. [29] S. Xu, “Sarr: A cybersecurity metrics and quantification framework (keynote),” in Science of Cyber Security - Third International Conference (SciSec’2021), vol. 13005 of LNCS, pp. 3–17, 2021. [30] D. Li, Q. Li, Y. F. Ye, and S. Xu, “Arms race in adversarial malware detection: A survey,” ACM Comput. Surv., vol. 55, no. 1, 2023. [31] S. Xu, “The cybersecurity dynamics way of thinking and landscape (invited paper),” in ACM Workshop on Moving Target Defense, 2020. [32] H. Chen, H. Cam, and S. Xu, “Quantifying cybersecurity effectiveness of dynamic network diversity,” IEEE Transactions on Dependable and Secure Computing, 2021.