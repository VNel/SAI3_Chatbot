arXiv:2206.11586v1 [cs.CR] 23 Jun 2022 1 MAGIC: A Method for Assessing Cyber Incidents Occurrence Massimo Battaglioni, Giulia Rafaiani, Franco Chiaraluce, Marco Baldi Polytechnic University of Marche, Department of Information Engineering, Ancona, Italy e-mail: {m.battaglioni, g.rafaiani, f.chiaraluce, m.baldi}@univpm.it Abstract—The assessment of cyber risk plays a crucial role for cybersecurity management, and has become a compulsory task for certain types of companies and organizations. This makes the demand for reliable cyber risk assessment tools continu- ously increasing, especially concerning quantitative tools based on statistical approaches. Probabilistic cyber risk assessment methods, however, follow the general paradigm of probabilistic risk assessment, which requires the magnitude and the likelihood of incidents as inputs. Unfortunately, for cyber incidents, the likelihood of occurrence is hard to estimate based on historical and publicly available data; so, expert evaluations are commonly used, which however leave space to subjectivity. In this paper, we propose a novel probabilistic model, called MAGIC (Method for AssessinG cyber Incidents oCcurrence), to compute the likelihood of occurrence of a cyber incident, based on the evaluation of the cyber posture of the target organization. This allows deriving tailor-made inputs for probabilistic risk assessment methods, like HTMA (How To Measure Anything in cybersecurity risk), FAIR (Factor Analysis of Information Risk) and others, thus consid- erably reducing the margin of subjectivity in the assessment of cyber risk. We corroborate our approach through a qualitative and a quantitative comparison with several classical methods. I. INTRODUCTION The massive exploitation of data and information systems in companies and organizations is motivating an increasing attention to cybersecurity and its management. One of the main pillars upon which cybersecurity management relies is cyber risk assessment, for which a plethora of standards and models exist. Risk does not have a unique deﬁnition, but according to the NIST (National Institute of Standards and Technology) [2], it is a measure of the extent to which an entity is threatened by a potential circumstance or event, and is typically a function of: (i) the adverse impacts that would arise if the circumstance or event occurs; and (ii) the likelihood of occurrence. A unique method for cyber risk assessment does not exist, but inter- national standards provide general guidelines which should be followed when designing a cyber risk assessment method. For example, the ISO (International Standard Organization) unpacks risk assessment into risk identiﬁcation, analysis and evaluation [3], [4]. In the risk identiﬁcation phase, the critical services are identiﬁed and the threats and vulnerabilities they The material in this paper was presented in part at the AEIT 2021 International Annual Conference [1]. This research was supported in part by the “Cyber Risk Assessment Models and Algorithms (CybeRAMA)” project (Ref. 2019.0421), funded by the Cariverona Foundation within the “Research and Development 2018” call (https://cyberama.dii.univpm.it/). could cause are determined. Risk analysis, instead, is needed to determine the likelihood of occurrence and the impact of these threats [4], [5]. Finally, in the risk evaluation phase the obtained results are compared with some pre-established risk acceptance criteria [3], [4]. In this paper we focus on the second one of the aforementioned phases, that is, risk analysis and likelihood estimation. Existing cyber risk assessment approaches can be grouped into quantitative and qualitative methods. In the former ones, the risk analysis phase is carried out through numerical eval- uations, mostly based on probability theory, as in classical PRA (probabilistic risk assessment) [6]. Widely speaking, these approaches have some potential advantages, like their robustness, reproducibility, and comparability of results. While it seems generally reasonable to rely on past events for risk analysis, in the speciﬁc case of cyber incidents these data may be unavailable or incomplete and, thus, probabilistic methods must resort to the help of experts, which makes them barely practical in real-case scenarios [2]. Qualitative methods, in turn, exploit a nonnumerical ap- proach and, therefore, they are often simpler to implement and interpret. However, their results are scarcely reproducible and comparable and, above all, they have an intrinsic nature of subjectivity. A. Contribution To the best of our knowledge, a probabilistic method enabling the computation of the likelihood of occurrence of cyber incidents, based on the posture of the organization rather than on subjective expert estimates, is currently missing. To ﬁll this gap, we propose a method called MAGIC (Method for AssessinG cyber Incidents oCcurrence), which is a probabilis- tic model to quantitatively estimate the likelihood of a cyber incident for a speciﬁc organization, starting from qualitative assessment approaches based on questionnaires. Owing to its own nature, MAGIC should be seen as a tool to be used in conjunction with existing probabilistic cyber risk assessment methods, to make their results more reproducible and less subjective. For this purpose, we consider four indicators representing the awareness of the employees, the maturity, the complexity, and the attractiveness of the target organization. These aspects are commonly qualitatively assessed through questionnaires, 2 so we estimate the values of the corresponding indicators start- ing from questionnaires concerning the target organization, provided by one or more assessors. Then, the four indexes are combined, using a probabilistic quantitative approach, in order to ﬁnd the likelihood of occurrence of a given type of cyber incident. In particular, we consider two scenarios: in the ﬁrst simpliﬁed scenario, we obtain as output the estimated frequency of occurrence of cyber incidents in a given time period (and the associated likelihood); in the second scenario, we obtain the probability that the organization will face exactly one successful cyber incident in a given time period. Indeed, in the latter case we assume that, after one cyber incident, the organization will change its posture and thus a new assessment needs to be performed. MAGIC provides inputs that can be used in conjunction with classical PRA methods such as HTMA (How To Measure Anything in cybersecurity risk) [7], FAIR (Factor Analysis of Information Risk) [8], as well as many others. In the former method, it is assumed that the frequency of adverse events follows a log-normal distribution, whose mean and variance is not directly related to the organization posture. Similarly, in the FAIR method, for a given adverse event, an expert is needed to estimate the minimum, maximum and most likely values for its frequency of occurrence. In the case of cyber events, these processes often become subjective, due to the lack of reliable historical data concerning each speciﬁc type of cyber incident. Our target is to reduce such a subjectivity as much as possible, by following and extending the approach in [1], [9]. In short, according to the proposed method, the main effort required from the target organization is to provide information regarding the state of their technological systems and pro- tection measures: expert evaluations are no longer required and the relations among all the components of the considered infrastructure do not need to be ﬁgured out. We consider ques- tionnaires based on international cybersecurity standards and frameworks, which are comprehensive and widely recognized, but MAGIC is general and can be applied with other types of questionnaires. A basic version of our model has been introduced in [1]. Even though the approach proposed in this paper has some similarities with that in [1], since the ﬁnal goal of both of them is to estimate the likelihood of occurrence of cyber incidents based on the organization posture, there are some profound differences between them. The most important ones are described next: • the approach in [1] does not take the awareness of the employees into account as a determinant factor for risk assessment, which is instead considered in the model we propose; • we provide the set of controls to be used for complexity assessment; • the approach in [1] stands as an independent method for probabilistic assessment, whereas MAGIC is a transversal tool to provide tailor-made inputs for existing probabilis- tic cyber risk assessment methods; owing to this, we are also able to report a larger number of numerical results; • in this paper, the ﬁnal likelihood is computed differ- ently from that in [1], where the probability that the organization faces a certain number of cyber attacks is not considered in the computation. Through the novel approach, instead, the likelihood of a cyber incident can be easily converted into a frequency, comparable to that obtained through expert evaluations in existing methods; • differently from [1], in this paper we also consider the practical scenario in which the target organization does not immediately realize that a cyber incident has occurred. B. Paper organization The paper is organized as follows. In Section II we provide a high-level description of many related works. In Section III we recall the basic functioning of some probabilistic cyber risk assessment methods. In Section IV the components of the cyber incident occurrence model we propose are discussed. In Section V we show how our approach can be combined with known probabilistic methods in order to overcome their main limitations when dealing with cyber risks. In Section VI we present some numerical results, aimed at validating the effectiveness of the proposed approach. In Section VII we provide both a quantitative and a qualitative comparison of our method with other approaches. Finally, Section VIII concludes the paper. II. RELATED WORKS A plethora of approaches for cyber risk assessment can be found in existing literature. Besides those proposed by national and international organizations (like the mentioned ISO/IEC (International Standard Organization/International Electrotech- nical Commission) 27005:2018 [4] and NIST SP (Special Pub- lication) 800-30 [2]), others have been introduced by public and private organizations, like EBIOS (Expression des Besoins et Identiﬁcation des Objectifs de S´ecurit´e) [10], CRAMM (Central Computer and Telecommunications Agency Risk Analysis and Management Method) [11], OCTAVE (Opera- tionally Critical Threat, Asset, and Vulnerability Evaluation) [12], MEHARI (MEthod for Harmonized Analysis of RIsk) [13], MAGERIT (Risk Analysis and Management Methodol- ogy for Information Systems) [14], IRAM2 (Information Risk Assessment version 2) [15], IT-Grundschutz [16], and CORAS [17]. An extensive and critical literature review can be found in [18], [19]. A lot of attention has been devoted to solving the problem of estimating the likelihood of occurrence of a threat and the corresponding impact. For example, several methods have been proposed using different techniques like Bayesian networks [20], attack path graphs [21], fuzzy logic [22], probabilistic model checking [23], vulnerability assessments [24], Monte Carlo simulations [7], [8], and others. Next we provide a brief description of some of the afore- mentioned methods, highlighting the differences with MAGIC as well as the possible common aspects. A deeper comparison is then carried out in Section VII, after the description of our method. 3 • ISO/IEC 27005:2018 [4]: is an international framework for managing information risks. It describes all processes for risk management, including risk assessment. In this case, the estimation of the likelihood can be performed in a qualitative, quantitative, or hybrid way. However, the ISO/IEC 27005 standard only provides guidelines for doing it, without describing any speciﬁc practical method. • NIST SP 800-30 [2]: is a guide for conducting risk assessment. In order to determine the likelihood of oc- currence of a security incident, this method identiﬁes all the potential vulnerabilities and the probability of their exploitation. The likelihood is then described using a qualitative or semi-quantitative scale. Such an approach is opposed to the numerical one we propose, since it leaves space to subjectivity. • EBIOS [10]: is a scenario-based approach for risk man- agement that relies on the establishment of a strong link among different stakeholders. It uses a modular approach for identifying risk causes. However, all the phases con- sidered in this method, including risk assessment, are the result of security debates among the team and, therefore, they are subjective. • CRAMM [11]: is a qualitative method for risk as- sessment. It measures risks as the product of asset, threat, and vulnerability values. It uses trained experts. Threats and vulnerabilities are not exhaustively assessed, but the assessor can choose among different predeﬁned threat/asset and threat/impact combinations. Relying on structured questionnaires, and/or on the expertise of the assessor, the method determines the likelihood of threats and vulnerabilities; however, differently from MAGIC, it does not directly link those likelihoods with the likelihood of occurrence of a cyber incident. • OCTAVE [12]: is an asset-driven method for assessing information security risks. While the original approach is designed for large organizations, the OCTAVE-S [25] and the more recent OCTAVE-Allegro [26] versions can be also applied to small and medium enterprises. This method ﬁrstly identiﬁes all the assets, and then focuses on the critical ones. For each of them, it determines the related threats, and qualitatively labels their likelihood of occurrence as “Low”, “Medium” or “High”. • MEHARI [13]: is a risk management model with the aim of helping the implementation of ISO/IEC 27005. It performs the risk assessment phase through an audit. This phase includes the identiﬁcations of assets, threats and vulnerabilities. The likelihood of occurrence of a threat is qualitatively described using a four levels scale. • MAGERIT [14]: is an asset-oriented risk management model. Using this model, security professionals evaluate the assets and all the possible threats. Then, they describe the likelihoods of occurrence of those threats using a numerical scale with a limited number of levels. The likelihood is usually evaluated relying on the annual rate of occurrence of any speciﬁc threat for each speciﬁc asset, which is substantially estimated through historical data. • IRAM2 [15]: is a qualitative threat-driven method for information risk assessment and treatment. The likeli- hood of success of a threat is estimated using lookup tables having as input two numerical values describ- ing the strength of the threat and that of the security controls implemented by the organization. Then, using this method, the residual likelihood is evaluated (also, through a lookup table) as a combination of likelihood of success and likelihood that an attacker will try to cause an incident based on the considered threat. • IT-Grundschutz [16]: is a qualitative method for identi- fying and assessing security incidents. Using this method, some qualiﬁed staff has to identify all possible threats. Then, for each of them this method evaluates the fre- quency of occurrence using a qualitative scale. This frequency is ﬁnally combined with the impact through a risk matrix. • CORAS [17]: is a method for security risk analysis. All the risk assessment phases are performed through structured brainstorming, where people with different backgrounds and competences collaborate to identify assets, vulnerabilities, threats, and the related likelihoods of occurrence. The likelihood assessment can be done both in a qualitative or a quantitative way; however, the evaluation is strictly related to the subjectivity of the assessors. • LiSRA [21]: is a risk assessment method taking a bidi- mensional input; on one dimension domain-speciﬁc infor- mation is required from an expert, while the other dimen- sion is ﬁlled by the user, according to the practices of the considered organization. Then, the risk is conventionally obtained as the combination of the probability to have successful attacks and their impact. The probability of success is computed based on attack trees but, differently from our method, the latter need to be entirely computed by external experts. Therefore, MAGIC might be seen, with some adaptations, as an alternative method allowing to bypass the need of experts, rather than a completely different approach. • Risk analysis based on fuzzy decision theory [22]: the ﬁrst step of this approach is to identify an expert; then, a taxonomy of events and scenarios has to be deﬁned (second step). Finally, the expert builds a matrix with potential accidents on the rows and possible scenarios on the columns: each entry of the matrix has to be ﬁlled with a probability that the accident takes place in a certain scenario. Also in this case, differently from our method, external expert estimates are used. Starting from this matrix, the fuzzy decision theory is then applied, which returns the expected value of the considered option. Also in this case, MAGIC is not in contrast with this method, but can be seen as a variation of it. In fact, in MAGIC the a-priori probabilities can be numerically derived from the posture of the organization, rather than estimated by an expert. • CVSS-based risk assesment: the CVSS (Common Vul- nerability Scoring System) [27] gives scores to threats exploiting vulnerabilities on the basis of three categories. One can combine this approach with attack graphs [24] in order to derive the vulnerabilities starting from known 4 threat sources, or to bayesian decision networks [20], in which the attacks are modeled starting from corre- lated alerts. The probabilities that the considered threats exploit certain vulnerabilities are computed from the scores associated to each of them, according to CVSS. In particular, CVSS 2.0 provides for the use of three types of metrics: base, temporal and environmental. Base metrics represent the intrinsic characteristics of threats exploit- ing vulnerabilities that are constant over time and user environments; temporal metrics represent the features of threats exploiting vulnerabilities that change over time but not over user environments; environmental metrics are based on the characteristics of threats exploiting vulnera- bilities that are unique to a particular user’s environment. These methods may be directly comparable to MAGIC, in that the probabilities derive from numerical assessments, even though a subjective component is still required. In particular, in the basic version of CVSS-based risk assessment methods, the probability of occurrence of the i-th item in the Common Vulnerabilities and Exposures (CVE) list can be computed as a product of some CVSS metrics, i.e., Li = AV × AC × Au × E × RC, where AV is the access vector metric, AC is the access complexity metric, Au is the authentication metric, E is the exploitability metric and RC is the report conﬁdence metric. The exact numerical values of all these metrics need to be chosen in a range of preﬁxed values, which makes this numerical approach also subjective. There are several reasons that make many of the aforemen- tioned approaches difﬁcult to apply in real case scenarios. In fact, as explained above, they make risk assessment result in a long process requiring the availability of a signiﬁcant amount of data. Moreover, the application of these methods often requires the help of an expert assessor external to the organization and able to provide quantitative measures of the cyber risk, which makes them cumbersome and exposed to subjectivity. MAGIC, instead, allows computing quantitative parameters starting from simple questionnaires, which makes the risk assessment process straightforward and less exposed to subjectivity. III. PROBABILISTIC CYBER RISK ASSESSMENT: PRELIMINARIES In this section we describe two state-of-the-art probabilistic cyber risk assessment methods based on Monte Carlo simula- tions, i.e., HTMA [7] and FAIR [8]. A. Probabilistic risk assessment using HTMA In a nutshell, the HTMA method for cyber risk assessment [7] is based on the following steps: • deﬁnition of the potential cyber threats; • estimation of the likelihood of occurrence and impact of each event; • Monte Carlo simulation for generating the scenarios; • results interpretation. While the likelihood of occurrence of each threat is given by a single value, the impact is associated to a 90% conﬁdence interval, identiﬁed by a lower and an upper bound. All these three numerical values are to be determined by external experts. The Monte Carlo simulation, in each scenario, works as follows: 1. for any threat i, a real number r is generated by sampling uniformly at random the range between 0 and 1, boundaries included, denoted as [0, 1]. If r < Li, where Li is the likelihood of the i-th threat, it is assumed that the event has not happened, and vice versa; 2. the impact of events which did not occur is 0, whereas the impact of occurred events is obtained by randomly sampling a log-normal distribution of the impacts, obtained according to the values of the given 90% conﬁdence interval; 3. all the impacts of occurred events are summed, in order to obtain an estimate of the total annual risk. The results obtained with the Monte Carlo simulation are used to construct the LEC (Loss Exceedance Curve), which corresponds to the graphical representation of the complemen- tary cumulative distribution function of the annualized loss expectancy. B. Probabilistic risk assessment using FAIR The FAIR methodology [8] can be described through the following four steps: • deﬁnition of the scenario under exam and its decomposi- tion into sub-scenarios; • estimation of the parameters for any sub-scenario; • generation of the frameworks through Monte Carlo sim- ulations; • results interpretation. More generally, FAIR deﬁnes an ontology for the risk, which is summarized in Fig. 1. The ontology describes how the assessment of risk can be obtained; in particular, it considers all the risk factors that contribute to the evaluation of the risk and all the relationships between them. Risk factors can be measured and estimated; then, it is possible to calculate risk using mathematical expressions of the relationships among factors. In essence, the risk is computed as a combination of LEF (Loss Event Frequency) and Loss Magnitude. In order to facilitate the process of risk evaluation, these two factors can be individually decomposed in other factors that, in their turn, could be further decomposed, as well. This way, the user can assess the risk considering a speciﬁc layer of the ontology, according to which factors he is able to estimate. The LEF depends on several factors; the most relevant ones for our model are redeﬁned next: the TEF (Threat Event Frequency) is deﬁned as the frequency with which, in a given time period, the attacker tries to breach the organization; the Vulnerability is the probability of success of any of these breaches. The Loss Magnitude, instead, is the sum of the losses caused by the a certain primary threat event and of the losses caused by its side-effects (such as, for example, the reaction of secondary 5 Contact Frequency Probability of Action Threat Capability Difﬁculty Secondary Loss Event Frequency Secondary Loss Magnitude Threat Event Frequency Vulnerability Primary Loss Secondary Risk Loss Event Frequency Loss Magnitude Risk Fig. 1. Ontology of the FAIR risk. stakeholders), which are included in the concept of secondary risk. Conventionally, the outputs of the FAIR approach are scat- terplots with LEF and Loss Magnitude on the x-axis and y- axis, respectively, and/or tables summarizing various results of the Monte Carlo simulation. IV. CYBER INCIDENT MODEL In this section we introduce the quantitative approach we propose for estimating the likelihood of occurrence of a cyber incident, called MAGIC. First of all, we provide some basic deﬁnitions: • Cyber threat [28]: any circumstance or event with the potential to adversely impact organizational operations (including mission, functions, image, or reputation), or- ganizational assets, or individuals through an information system via unauthorized access, destruction, disclosure, modiﬁcation of information, and/or denial of service. We will simply refer to cyber threats as “threats” in the following. • Cyber attack: any realized attempt to partially or totally disclose, expose and/or compromise data by a malicious entity. We will simply refer to cyber attacks as “attacks” in the following. • Cyber incident: any cyber attack which had success. The proposed model, with all the key parameters, which we will discuss next, is schematized in Fig. 2. We need to distinguish between two types of threats: those coming from external threat agents and those happening as a consequence of human misbehavior, i.e., non-malicious threats. In Fig. 2, the dashed arrows are referred to the former scenario, the dotted arrows describe the latter, whereas the solid arrows are valid for both scenarios. The model components in black are described in detail in the following subsections, while those in blue are not among the objects of our theoretical analysis, but they are taken into account in Section VI, where numerical results are provided. A. Awareness of the employees The awareness of the employees can be deﬁned as their level of consciousness about the cybersecurity risks. It can be directly related to the training programs the organization supplies to its workers. In order to practically evaluate the awareness, it is possible to consider a list of best practices devoted to this issue, as that in the NIST SP 800-53 [28], or to rely on speciﬁc subsets of controls proposed in cybersecurity frameworks; for example, Control 14 in the CIS (Center for Internet Security) Controls [29] deals with security awareness and training programs. The evaluation of the awareness parameter can be carried out in different ways. For example, the assessor can determine if ev- ery identiﬁed control is fully implemented by the organization or not, simply associating to it a Yes or No answer. Another approach can be using a scale in order to determine at which degree each considered control is implemented and, therefore, assigning better ratings as the level of implementation com- pleteness of the control increases. With both these approaches, an N/A option needs to be included; this should be used when one or more controls are considered to be not applicable in the context under examination. Then, according to the results of the assessment, a numerical score should be assigned to every control. For example, using the ﬁrst of the approaches stated above, a score equal to 1 may be assigned to all the controls with Yes as an answer, while 0 may be assigned to the controls answered with No. Instead, when using the second approach, one may consider as many scores as the number of the possible implementation levels. In other words, if the scale used for the implementation evaluation has 5 possible levels, the score that can be assigned to each control can be, for example, a number between 0 and 4. But, obviously, other choices are possible. In the following, we denote by smax the maximum score value (so, smax = 1 or smax = 4 in the mentioned examples). Moreover, a weight may be assigned to every control. The weights are given mainly according to the aspects that the organization wants to stress out, so they are not mandatory. In fact, an organization may want to focus its analysis on some speciﬁc training strategies, while it may not be interested in other ones; in this case, it can assign higher (or lower) weights to the controls that it considers more crucial (or less relevant). Finally, a weighted average is calculated in order to obtain a ﬁnal value for the awareness. Note that the N/A controls should not be counted in the average. In this paper, we consider the awareness index as a number between 0 and 10. Let E be the total number of considered 6 Maturity of the organization Complexity of the organization Maturity of attackers Probability of success of an attack Awareness of the employees Attractiveness of the organization Number of attacks Likelihood of occurrence of a cyber incident Impact derived from the occurrence of a cyber incident Risk Fig. 2. Relations among the components of MAGIC. controls, si ∈[0, smax] and ai ≥0 the score and the weight associated to the i-th control, respectively; the awareness index is computed as Awareness Index = AI = PE i=1 si × ai PE i=1 ai × 10 smax . (1) Notice that, as shown in Fig. 2, the awareness directly inﬂuences the maturity of the organization, formally deﬁned in Section IV-B. This holds for both malicious and non- malicious threats. Moreover, when speciﬁcally dealing with non-malicious threats, the awareness is inversely related to the number of threats. In other words, if employees are well- trained about cyber risks, we can assume that the organization will suffer fewer potential threats, since the employees are less prone to causing cyber incidents. B. Maturity of the organization The maturity of an organization can be deﬁned as the level of implementation of all practices and procedures that the organization executes in order to reduce the risk of receiving cyber attacks which may cause security breaches, data leakage, denial of service, and so on. As mentioned in Section IV-A, awareness concurs to determine maturity (being one of its most relevant components) but other issues need to be taken into account, too. The evaluation of this further part of the maturity can be done by assessing the organization compliance to the security controls proposed by one or more cybersecurity frameworks. The frameworks chosen as the reference ones will inﬂuence the area of application and, therefore, the kind of risk the organization is going to evaluate. An organization, in fact, may be interested in assessing the risk of suffering breaches, or the risk of losing, as a consequence of an attack, data conﬁdentiality and/or integrity and/or availability, or may be interested in assessing the risk related to a combination of them both. The security frameworks that can be used as reference are, for example, the one proposed by CIS in [29] for assessing cybersecurity risk, or the set of controls introduced by the ENISA (European Union Agency for Cybersecurity) in [30] for assessing data protection risk, or the NIST Cybersecurity Framework [31] for assessing both cybersecurity and data protection risks. Therefore, ﬁrst of all, the organization should choose the kind of risk it intends to assess and, according to the choice, select one or multiple appropriate security frameworks as reference. The next step is evaluating if and at which level all the security controls listed in the reference framework are implemented within the organization. Using the controls of cybersecurity frameworks to assess the maturity will result in an accurate picture of the actual cyber posture of the organization. In fact, since the security frameworks are usually considered as a set of best practices, their use will lead to a robust and complete mapping of the area of interest. Moreover, since the security frameworks are continuously updated, the maturity can be dynamically assessed, simply repeating the evaluation as soon as new versions of the frameworks are released. Because of the need to combine the mentioned different aspects, the maturity index evaluation is performed through a two-step procedure. The ﬁrst step is equivalent to that described in the previous section for the evaluation of the awareness index. Clearly, the controls of the chosen framework about cybersecurity awareness and training programs must be excluded from the list used for this part of the evaluation. Thus, after answering to all the remaining controls, a score is associated to any answer. The N/A option should be included as well. Also in this case, it is possible to assign a weight to each control, according to its relevance for the kind of assessment the organization wants to perform. Let T be the total number of considered controls, s′ i ∈[0, s′ max] and a′ i ≥0 respectively the score and the weight associated to the i-th control; a weighted average is computed as follows M = PT i=1 s′ i × a′ i PT i=1 a′ i × 10 s′max . (2) Then, in the second step of the procedure, the maturity index 7 is eventually obtained as a weighted average of the awareness index and M. In this case, AI and M are weighted by E/(E + T ) and T/(E + T ), respectively. We ﬁnally obtain Maturity Index = AI × E + M × T E + T . (3) We observe that, according to this deﬁnition, the maturity index is a real number ranging between 0 and 10. Note that the maturity of the organization will directly inﬂuence the probability of success of an attack. In fact, a higher maturity means that the organization has addressed more attention to cybersecurity practices and, therefore, the probability for an attack attempt to be successful will decrease, and vice versa. C. Complexity of the organization The complexity of an organization can be deﬁned as the measurement of the intricacy of its technological infrastructure and of how the processes, the activities, and the services are managed. The concept that the risk does not only depend on the maturity of the organization, but also on its com- plexity is introduced in [29], where three IGs (Implemen- tation Groups) are deﬁned. Following that approach, every organization should identify itself, mainly according to its dimension, in one of the proposed IGs and, therefore, should implement a speciﬁc subset of security controls. This is due to the fact that smaller organizations are usually expected to be less exposed to threats, when compared to larger organiza- tions. However, the dimension should not be considered as the only discriminating parameter. For this reason, starting from the controls proposed in [32] for the evaluation of the inherent risk, we have identiﬁed a set of punctual and speciﬁc controls useful to assess the inherent complexity of an organization1. Coherent with the above considerations, the controls we address do not consider only the dimension of the organization, but they try to identify all possible critical points of hardware, software, networks, and facilities. More precisely, we have grouped all the considered controls into ﬁve categories: Networks and Infrastructure, IP (Internet Pro- tocol) network technologies, Applications, Services and IT (Information Technology) department. Therefore, beside the number of employees, the assessment includes the number and the characteristics of the components (physical and software systems) and their interconnections, the number of services and their characteristics, and the entropy of the IT system management. In order to facilitate the assessment, for each control we have identiﬁed ﬁve possible guided answers. The answers are associated to Very Low, Low, Medium, High, and Very High complexity. In order to practically assess the complexity, the process is equivalent to the maturity assessment. After evaluating all the controls, a score is associated to every evaluation. The N/A option should be included as well. Also in this case, it is possible to assign a weight to each control, according to its relevance for the kind of assessment the organization 1The set of controls considered in this paper for the complexity assessment can be found at https://github.com/secomms/cyber-risk-assessment. wants to perform. Then, a weighted average is calculated in order to obtain a complexity index for each one of the ﬁve categories considered above. Similarly to the previous indexes, we consider the complexity index as a number between 0 and 10. Let Cj be the number of controls included in the category j, s′′ i,j ∈[0, s′′ j,max] and a′′ i,j ≥0 respectively the score and the weight associated to the i-th control of the j-th category; the complexity index of the category j is computed as CI(j) = PCj i=1 s′′ i,j × a′′ i,j PCj i=1 a′′ i,j × 10 s′′ j,max . (4) Finally, a global complexity index for the organization is computed as the weighted average of the ﬁve complexity indexes computed before; in this case, the weight associated to each category is simply computed as the number of controls included in that category divided by the total number of controls, i.e., bj = Cj/(P5 j=1 Cj). Then, we have Complexity Index = 5 X j=1 (CI(j) × bj). (5) We observe that the same approach with the two weighted averages could be used for the maturity assessment when the security controls chosen for the evaluation are divided into categories. Note that the complexity of the organization will inversely inﬂuence the probability of success of an attack. In fact, a higher complexity means that the technological infrastructure of the organization is more intricate and, therefore, it is more prone to being successfully targeted. In other words, the probability for an attack attempt to be successful will increase for increasing complexities, and vice versa, according to the law that will be described in Section IV-F. D. Attractiveness of the organization The attractiveness of an organization can be deﬁned as the level of interest the organization causes in potential attackers. The attractiveness depends on several factors, as the type of business, the kind and the amount of data the organization manages, etc. The insertion of this parameter in our model is due to the assumption that cyber criminals will likely attack organizations from which they can obtain larger proﬁts. The dimension of the organization does not affect its attractiveness. In fact, for example, small organizations may operate in critical environments and/or may process a large amount of sensi- tive data, making them more attractive to cyber criminals if compared to larger organizations with a limited technological value. In order to practically estimate the attractiveness, we have examined the data proposed in cybersecurity reports like [33]. In particular, we have considered the number of attacks any type of organization has suffered in one year, with respect to the total number of attacks analyzed in the report. According to the given data, we have classiﬁed the type of organization as reported in Table I. Therefore, during the assessment, the organization should simply identify itself in one of the proposed types of business, and the attractiveness will be assigned consequently. 8 TABLE I CLASSIFICATION OF THE ORGANIZATIONS AS A FUNCTION OF THE PERCENTAGE π OF ATTACKS RECEIVED WITH RESPECT TO THE TOTAL NUMBER. Percentage of attacks Type of organization π < 1.25% Very lowly attractive 1.25% ≤π < 2.5% Lowly attractive 2.5% ≤π < 5% Averagely attractive 5% ≤π < 10% Highly attractive π ≥10% Very highly attractive Note that the attractiveness will inﬂuence both the number of attacks and the maturity of attackers. In fact, we assume that a highly attractive organization will likely suffer more and more structured attacks if compared to a lowly attractive organization. E. Maturity of attackers Given the deﬁnition of cyber attack proposed in Section IV, let us devise the attacker model. As mentioned above, we should distinguish between attacks coming from malicious attackers, and threats deriving from the lack of awareness of the employees. For the probabilistic model we devise, however, there is no need to mathematically distinguish be- tween these cases. Thus, for the sake of simplicity, in the rest of the paper we will also refer to non-malicious threats as “attacks”, keeping in mind that they come from non-malicious employees, who assume the role of unaware attackers in the model. In general, the organization might be targeted by multiple attackers, but we assume that they cannot conduct more than one attack in the same time slot ∆t. The value ∆t can be chosen sufﬁciently small, so that attacks performed in close periods of time can be distinguished. Moreover, under this assumption, the identity of the attacker does not play a relevant role and, therefore, we will generically refer to “the attacker”. Another assumption that we make, in order to keep the analysis feasible, is that different attack attempts are not correlated. In other words, each attack attempt does not depend on previous attack attempts, and its outcome does not inﬂuence future attack attempts. Clearly, this hypothesis may not always be veriﬁed and, in the scenario where the assumptions are too optimistic, our model provides a lower bound to the likelihood of the adverse events, rather than an estimate. As shown in Fig. 2, the attractiveness is related to the ma- turity of the attackers: attractive organizations will more likely face more structured attacks, and vice versa. The maturity of the attackers, in its turn, inﬂuences the probability of success of an attack, described in the next section. F. Probability of success of an attack Obviously (and luckily) attacks are not always successful and, therefore, more than one attempt may be needed before breaking through the organization defenses. The single attack attempt is thus associated to a probability of success. In particular, the computation of the probability of success of a single attack takes into account the four key indexes described above. Considering the maturity of the organization as a variable x, we need a function that decreases when x increases, since we expect more mature organizations to be more resistant to cyber attacks, and vice versa. Furthermore, we do not expect this trend to be linear, since slight improvements of the maturity of a very immature organization may not be sufﬁcient to signiﬁcantly decrease the probability of success of an attack and, similarly, very mature organizations should not need to further improve signiﬁcantly their posture, since the probability of an attacker breaching them may already be low enough. A function that may ﬁt well this scenario is the logistic function [34] f(x) = K 1 + e−B(x−t) , (6) where K is the saturation level, i.e., the upper horizontal asymptote that limits the curve’s maximum value; t is the mid- point, i.e., the value of x corresponding to half the saturation level and B is the growth rate, i.e., the steepness of the curve. In order to obtain the aforementioned trend, B must be chosen negative. According to (6), f(x) has a lower asymptote equal to 0. It is possible to extend (6), by considering a lower asymptote different from 0 and a non-symmetric shape, thus obtaining the so-called generalized logistic function [35] f(x) = A + K −A (1 + Q × e−B(x−x0))1/ν , (7) where A is now the lower asymptote, Q is a variable, related to f(0), that inﬂuences the inﬂection point, f(x0) = A + K−A (1+Q)1/ν and ν > 0 determines the asymmetry of the curve. Notice that, by choosing Q = ν = 1, x0 corresponds to the point at which the curve is at its midpoint and has the maximum slope. By using the generalized logistic function, with Q = ν = 1, to express the probability of success of a single attack Ps(x), (sometimes Ps, for notation simplicity, in the following) we have Ps(x) = A + K −A 1 + e−B(x−x0) . (8) By increasing x0, Ps(x) shifts toward right. So, we associate the value of the complexity of the organization to x0, to take into account that, for the same value of x, more complex orga- nizations are expected to face attacks with a larger probability of success. According to the discussion in Section IV-B, the maturity x takes values in the range from 0 to 10. Moreover, we assume that the probability of success cannot reach the extreme and thus unrealistic values of 1 and 0. So, we set the maximum value U at x = 0 and the minimum value L at x = 10. Consequently, K and A depend on x0 and can be easily obtained by solving the system with f(0) = U and f(10) = L. An example of curves for different values of x0 is shown in Fig. 3. Indeed, as mentioned above, a more signiﬁcant value of the probability of success can be obtained by taking into account the maturity of the attackers, the latter playing the role of 9 0 1 2 3 4 5 6 7 8 9 10 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 x Ps x0 = 1 x0 = 5 x0 = 9 Fig. 3. Probability of success of a single attack (Ps) for different values of x0. B = −1, U = 0.97, L = 0.03. weight coefﬁcient w. Explicitly this means that the ﬁnal value of the probability of success of a single attack results in p∗= ps(x) = wPs(x). (9) A possible choice for the values of w is given in Table II, showing how more attractive organizations suffer attacks which are more likely to be successful because of the higher maturity of the attackers. In this case, we assume that a linear law models well the expected behaviour. We observe that, in case of non-malicious threats, w is always equal to 1, since attractiveness does not play any role. TABLE II POSSIBLE VALUES FOR w AS A FUNCTION OF THE ATTRACTIVENESS OF THE ORGANIZATION. Attractiveness Very Low Low Medium High Very High w 0.6 0.7 0.8 0.9 1 G. Number of attacks Let us consider a time period containing t time slots, each of duration ∆t. An attacker (malicious or non-malicious, accord- ing to the discussion in Section IV-F) may perform an attack attempt or not in any time slot, with a certain probability. Therefore, at most t attack attempts will be suffered by the organization in the considered time period. Let us deﬁne two discrete random variables: • A: represents the outcome of the attacker choice in a time slot. Either he performs an attack attempt (A = 1), or he does not (A = 0). Therefore, A is a binary random variable and follows a Bernoulli distribution. • N: represents the number of attack attempts suffered by the organization in the considered time period. Its realization n respects the condition 0 ≤n ≤t. We assume that N follows a binomial distribution2. The 2An alternative, which might be considered when navg is signiﬁcantly smaller than t, is the Poisson distribution, whose probability mass function is Pr(N = n) = λn n! e−λ, where λ = navg. PMF (probability mass function) associated to the binomial distribution is therefore Pr(N = n) = t n  Pr(A = 1)nPr(A = 0)(t−n) = t n  navg t !n 1 −navg t !(t−n) . (10) In (10) we have denoted by navg the average number of attack attempts suffered by the organization in the considered time period, so that, reminding the properties of the binomial distribution, Pr(A = 1) = navg t = 1 −Pr(A = 0). A possible choice is to pick navg as the number of attack attempts suffered by the organization in a previous time period having the same length of the considered one. Then, wishing to calculate the probability that the organi- zation suffers at most (or at least) n attack attempts, we have Pr(N ≤n) = n X k=0 Pr(N = n) Pr(N ≥n) = t X k=n Pr(N = n), respectively. H. Likelihood of occurrence of cyber incidents In the previous section, we have analyzed the probability that the organization suffers a certain number of attack at- tempts. Even though this represents an interesting information for the stakeholders, it is more relevant to estimate the likeli- hood of these attacks being successful. In fact, in the deﬁnition of risk [2] the likelihood of a successful attack, i.e., a cyber incident, is multiplied by its impact, and the impact of non- successful attacks is obviously 0. We therefore introduce a further random variable S, which models the number of cyber incidents faced by the organization in the considered period of time. We consider two scenarios: • the likelihood of facing s cyber incidents, after at most t attempts by the attacker. In this simplistic approach, it is assumed that the organization does not promptly realize when it is successfully attacked, and, therefore, does not take the appropriate countermeasure to change posture; • the likelihood of facing exactly one cyber incident, after at most t attack attempts by the attacker. In this more realistic approach, it is assumed that the organization immediately notices the breach, and tries to improve its posture. 1) Organizations which do not change posture: In this scenario, having assumed that the organization does not react immediately after experiencing one or more cyber incidents, we can model the attacks as Bernoulli experiments, with success probability given by (9). We can actually assume that also the outcome of the single attack attempt is a continuous random variable P with realization p. In particular, taking into account the uncertainty inherent in the problem (where the maturity index, for example, results from the opinion of some experts or from the assessment of questionnaires) we 10 assume that P follows a PERT (Project Evaluation and Review Techniques) distribution, with pm = ps(min{x + q, 10}), pM = ps(max{x −q, 0}), for some arbitrary value of q, and p∗= ps(x) respectively the minimum, maximum, and most likely values. In the following, we have considered q = 1. Therefore, the probability that s out of n ≤t trials are successful is given by Pr(S = s|N = n) = Z pM pm n s  ps(1 −p)(n−s)fP(p)dp, where fP(p) = (p −pm)α−1(pM −p)β−1 B(α, β)(pM −pm)α+β−1 , being α = 1 + 4 p∗−pm pM−pm and β = 1 + 4 pM−p∗ pM−pm , while B(α, β) = R 1 0 xα−1(1 −x)β−1dx is the Beta function. For the likelihood, we ﬁnally obtain L(NC)(s) = Pr(S = s) = t X n=1 Pr(S = s|N = n)Pr(N = n). (11) 2) Organizations which change posture: In this scenario, we assume that the organization runs into a single cyber incident, detects it immediately and decides to promptly take countermeasures, changing its posture. Therefore, a new cyber risk assessment should be performed after one successful at- tack, since the initial conditions, and the value of the maturity index above all, should have been changed. In this case, we consider a cumulative geometric distribution, i.e., Pr(S = 1|N = n) = Z pM pm n X k=1 p(1 −p)(k−1)fP(p)dp. Then, the likelihood of occurrence of a single cyber incident can be obtained by considering all possible values of n, i.e., L(C) = Pr(S = 1) = t X n=1 Pr(S = 1|N = n)Pr(N = n). (12) V. CYBER INCIDENT MODEL IN HTMA AND FAIR In this section we show how the values of the likelihood (or frequency) computed in Sections IV-H1 and IV-H2 can be plugged into HTMA and FAIR, minimizing the subjectivity of the assessor and, rather, allowing the computation of risk according to the posture of the considered organization. A. Use of MAGIC with HTMA According to the description in Section III-A, HTMA does not take into account the possibility that a threat happens more than once in a year and, therefore, we must consider the likelihood that the organization faces the threat just once. This means that the scenario described in Section IV-H2 must be applied. As for the monetary impact of these threats, they are not object of investigation in this paper and, therefore, we rely on available results. In particular, MAGIC can be used to estimate the likelihood of occurrence of the threats considered in the Monte Carlo simulation (point 1. in Section III-A). In fact, instead of relying on the expertise of the assessor, it is possible to employ the likelihood obtained through our cyber incident model (12) as input to the HTMA method. However, we need to associate a likelihood to every single cyber threat in the considered list (and not a single, general likelihood value) and, therefore, the model must be adapted. Explicitly the procedure works as follows. We start by deﬁning a list of threats. This list is then combined with the list of considered controls through a table containing weight coefﬁcients ωi,j as shown in Table III, where τj and γi denote the j-th threat and the i-th control, respectively. TABLE III WEIGHT COEFFICIENTS FOR LIKELIHOOD ASSESSMENT IN HTMA. τ1 τ2 · · · τn γ1 ω1,1 ω1,2 · · · ω1,n γ2 ω2,1 ω2,2 · · · ω2,n ... ... ... ... ... γm ωm,1 ωm,1 · · · ωm,n First of all, one should answer the question “if the control γj is not implemented, does the risk relative to the threat τi increase?”. If not, then ωi,j is 0, otherwise a non-zero value can be assigned to ωi,j, according to some predetermined rule. This way, for each threat we can select a subset of controls that includes only those controls that have a non-zero weight ω for that speciﬁc threat. Notice that the subsets are not necessarily disjoint. We can compute a maturity index for each of these subsets and obtain the likelihood of occurrence of a cyber incident caused by the threat τj, noted by Lj, following the procedure presented in the previous sections. B. Use of MAGIC with FAIR Referring to the FAIR ontology in Fig. 1, the blocks that are directly inﬂuenced by our model are the TEF, the Vulnerability and the LEF (both for the primary and the secondary threat event). Clearly, these blocks will also indirectly inﬂuence the upper layers of the ontology which, however, also depend on blocks which are not object of our analysis. We remind that the TEF is deﬁned as the frequency with which, in a given time period, the attacker tries to breach the organization. In Section IV-G, we have devised a model that returns the probability that the organization faces a given number of attacks. As discussed below, the link between number of attacks and frequency of the attacks is immediate, once the reference time period has been deﬁned. The Vulnerability, deﬁned as the probability of success of any of these breaches, assumes the same meaning of the probability of success of the attacks, deﬁned in Section IV-F and denoted as p∗. Finally, the LEF is obtained by combining the other two parameters; it is deﬁned as the frequency with which the attacker succeeds, causing a monetary loss to the organization. Also this parameter has been covered by our analysis, especially that in Section IV-H1, where the likelihood that the organization is breached a given number of times is computed through (11). Also in this case, the connection between likelihood and frequency is straightforward. 11 In the original FAIR approach, it is assumed that the LEF follows a PERT distribution, whose parameters must be com- puted based on experts’ estimates. Using MAGIC, we maintain the Monte Carlo approach, as already done for the HTMA approach, but we sample the LEF from the distribution of S (i.e., Pr(S = s)) reported in (11), which is computed taking into account both the TEF and the Vulnerability. The random sampling can be performed very easily, since it is referred to a discrete distribution: we can associate a probability to each allowed value of s and sample according to these probabilities. Notice that, once s is obtained by random sampling, the LEF in the considered framework can be simply computed as LEF = s t . VI. NUMERICAL RESULTS In this section we provide some numerical results, which aim to validate the effectiveness of MAGIC. A. Case study with HTMA In order to take into account a realistic scenario, we have considered the list of threats given in [9, Table 1] (originally taken from [36]), and here reported, for the sake of clarity, in Table IV. Furthermore, as a case study, we have analyzed a dummy organization in the “Healthcare” sector. We have simulated a posture assessment, ﬁnding, for each threat, a maturity index as in Table IV. As for the attractiveness, according to the discussion in Section IV-D, we have found that organizations in the healthcare sector are very highly attractive. Indeed, this is not surprising, and well-known in the literature. Finally, we have considered a range of values for the complexity index, from 4.5 to 7.5 with step 0.5. We have also considered different values of navg, ranging from 2 to 5; we have assumed that, in each simulation, navg is the same for all threats (for the sake of comparison between different threats). As in [1], we choose B = −2, U = 0.97 and L = 0.03 as parameters of the generalized logistic function. Looking at Table IV we notice that we are talking about an attractive organization, which however has some serious maturity shortcomings, which may lead to weaknesses against cyber threats. The parameters of the PERT distribution, along with the likelihood of occurrence of each threat in a year (t = 365 and ∆t = 1, assuming that the attacker performs at most one attack a day), computed by (12), are also shown in Table IV for navg = 4 and complexity index 5. As expected, due to the relatively low values of the maturity indexes associated to each threat, the organization has a relatively high probability of suffering a cyber incident through them. Still, threats corresponding to lower maturity indexes are less likely to be suffered than threats corresponding to higher maturity indexes. Regarding the monetary impacts, we have considered those in [9, Table 2] and also reported in Table V, for the sake of completeness. In order to keep a uniform notation with the rest of the paper, we have applied a currency exchange from dollars (used in [9]) to Euro. Notice that, assuming that all threats happen and the associated cyber incidents cause the largest possible loss (given by the upper value of the corresponding 0 2 4 6 8 10 12 Loss [million Euro] 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 LEC Complexity index = 4.5 Complexity index = 5 Complexity index = 5.5 Complexity index = 6 Complexity index = 6.5 Complexity index = 7 Complexity index = 7.5 Fig. 4. LEC for the considered (very highly attractive) organization, for different values of the complexity index and navg = 4, using the HTMA method. 0 2 4 6 8 10 12 Loss [million Euro] 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 LEC Fig. 5. LEC for the considered (very highly attractive) organization, for different values of navg and complexity index equal to 5, using the HTMA method. interval), we obtain an upper bound for the total loss which, in the proposed example, is a value between 11.5 and 12 million Euro (precisely, 11.8361 million Euro). As usual, we express the ﬁnal output of the model as a LEC, summarizing the probability that the loss faced by the organization will be greater than or equal to a certain value, obtained with a Monte Carlo simulation with 10, 000 trials. We show the LECs, obtained using the aforementioned values of the complexity and considering navg = 4, in Fig. 4. Notice that, coherent with our analysis, more complex organizations are more likely to suffer cyber attacks and, therefore, they will be subject to larger monetary losses. Finally, we have shown in Fig. 5 the LECs for complexity index equal to 5 and different values of navg. Also in this case, as expected, the losses increase for increasing values of navg, which we have chosen as the number of attack attempts suffered in the previous year, relatively to each threat. 12 TABLE IV PARAMETERS OF THE PROPOSED MODEL, AS INPUTS OF HTMA FOR THE CONSIDERED SCENARIO, WHEN THE COMPLEXITY INDEX IS 5 AND navg = 4. ID Threat Maturity index p∗ pm pM LC 1 Malware 4.3 0.50 0.28 0.72 0.86 2 Web-based attacks 5.6 0.23 0.11 0.43 0.61 3 Denial of services 3.6 0.66 0.43 0.83 0.92 4 Malicious insiders 1.9 0.90 0.79 0.95 0.97 5 Phishing and social engineering 3.6 0.66 0.43 0.83 0.92 6 Malicious code 6.0 0.17 0.08 0.34 0.52 7 Stolen devices 4.8 0.38 0.19 0.62 0.78 8 Ransomware 5.1 0.32 0.16 0.55 0.72 9 Botnets 4.3 0.50 0.28 0.72 0.86 TABLE V IMPACT RANGE OF THE CONSIDERED THREATS, IN MILLION EURO. ID Threat Impact range 1 Malware [2.1360, 2.3941] 2 Web-based attacks [1.8156, 2.0381] 3 Denial of services [1.4151, 1.5842] 4 Malicious insiders [1.2816, 1.4329] 5 Phishing and social engineering [1.1748, 1.3172] 6 Malicious code [1.1659, 1.2994] 7 Stolen devices [0.77875, 0.87576] 8 Ransomware [0.48060, 0.53845] 9 Botnets [0.31684, 0.35600] B. Case study with FAIR We have considered the same scenario described in [8, Chapter 8]. For the sake of simplicity, we have assumed that the Loss Magnitude is only caused by primary losses and then ignored secondary risk. Clearly, this assumption can be removed without changing the rationale of the model. Concerning the Loss Magnitude, we have not changed the minimum, maximum, and most likely values (and neither we have changed the conﬁdence level) for the considered cate- gories in [8, Chapter 8]: productivity, response, replacement, ﬁnes & judgements, reputation and competitive advantage. The only categories having non-zero impact values are re- sponse and replacement and, for the sake of completeness, we have reported them in Table VI. Also in this case, we have considered a dummy organization in the “Healthcare” sector. Simulating a different posture assessment from the previous section, we have found a complexity index equal to 5.2 and a maturity index equal to 6.9. Being in the healthcare sector, the organization is considered again very highly attractive. The values of B, U and L are once again set as −2, 0.97 and 0.03, respectively. The analysis is carried out on daily intervals within a year, i.e., t = 365 and ∆t = 1, and we have considered navg = 84. Notice that this value is signiﬁcantly larger than those adopted for the examples in Section VI-A, just in view of enlarging the ensemble of considered scenarios. Running the Monte Carlo simulation, we obtain the Loss Magnitudes in Fig. 6. Each point corresponds to a different simulation. The red circle in the ﬁgure represents the mean value (for both axes). Deﬁning the k-th percentile as the score below which the k% of the scores fall in the given distribution, we have found that the 10-th percentile and the 90-th percentile, for this example, are approximately 18, 000 C and 58, 000 C, respectively. The results of the Monte Carlo 10-2 10-1 LEF 2.5 3 3.5 4 4.5 5 5.5 6 104 Fig. 6. Loss magnitude for the considered organization, which is very highly attractive, when the complexity index is 5.2 and the maturity index is 6.9, obtained through the FAIR method. simulation are also summarized in Table VII where, besides the minimum, the maximum and the mean, also the mode (that is the most frequent value occurred in the Monte Carlo simulation, differently from the most likely value, that is the result of the statistical inference for the same quantity) is reported. Clearly, ﬁxed t, a smaller value of ∆t increases the number of possible attacks per year (which is now 365), thus increasing the number of possible values of the LEF, in its turn. In order to evaluate the impact of different values of the maturity index, complexity index and navg, we have ﬁxed two of these parameters and let the other change, obtaining Figs. 7, 8 and 9, respectively. The results are coherent with our analysis: on the one hand, for increasing values of the complexity and of navg, the total loss exposure increases; on the other hand, the total loss exposure decreases when the organization has increasing values of the maturity. VII. COMPARISON AND DISCUSSION In this section we perform both a quantitative and a quali- tative comparison of MAGIC with other methods. A. Quantitative comparison Let us consider the same organization in the “Healthcare” sector as in Section VI-A, for which a posture assessment has 13 TABLE VI IMPACT RANGE OF THE CONSIDERED EVENT, IN C. Category Minimum Maximum Most Likely Conﬁdence Response 2, 750 8, 250 22, 000 20 Replacement 20, 000 30, 000 50, 000 20 TABLE VII SUMMARY OF THE RESULTS OF THE MONTE CARLO SIMULATION, WHEN THE COMPLEXITY INDEX IS 5.2 AND THE MATURITY INDEX IS 6.9. Minimum Mean Mode Maximum Primary Loss Events per year 1 9.5 8 27 Primary Loss Magnitude (C) 28, 709 39, 045 32, 948 52, 822 Total Loss Exposure (C) 35, 369 369, 260 295, 520 1, 141, 600 0 1 2 3 4 5 6 7 8 Complexity 0 0.5 1 1.5 2 2.5 3 3.5 106 Minimum Mean Mode Maximum Fig. 7. Minimum, mode, mean and maximum of the total loss exposure, for different values of the complexity index. 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 Maturity 0 1 2 3 4 5 6 106 Minimum Mean Mode Maximum Fig. 8. Minimum, mode, mean and maximum of the total loss exposure, for different values of the maturity index. returned a complexity index equal to 5. The maturity index associated to each of the considered threats is given in Table IV (third column). Owing to its sector, the organization is considered very highly attractive. We have computed the likelihood of occurrence of cyber incidents caused by the same threats using the basic version 40 60 80 100 120 140 160 180 200 220 240 0 0.5 1 1.5 2 2.5 3 106 Minimum Mean Mode Maximum Fig. 9. Minimum, mode, mean and maximum of the total loss exposure, for different values of navg. of CVSS-based risk assessment method, relying on the funda- mentals described in [27]. The low level metrics we use for the comparison are those given in [27], which we also report in Table VIII for the sake of completeness. The assignment of the numerical values to these CVSS low level metrics, which is the possibly subjective phase of this approach, has been performed by means of a brainstorming session, based on the experience of the authors and on available evidences. It must be stressed that some assignments are not subjective: for example, the Access Vector of stolen devices or malicious insiders is objectively “Local”. However, this is neither true for all metrics, nor for all threats. Lastly, a team of certiﬁed external experts, taking as inputs the status of the organization used for the posture assessment, in order to ﬁnd the maturity, complexity and attractiveness in- dexes, has provided estimates for the likelihood of occurrence of cyber incidents deriving from the nine considered threats. This expert-based approach is used in many modern cyber- risk assessment methods, such as HTMA, FAIR, MAGERIT, CRAMM, and many others. The results obtained through MAGIC and the aforemen- tioned approaches are compared in Table IX, where we denote the likelihoods resulting from the newly proposed method, the CVSS-based method and the expert estimates as LC, LCVSS and LEXPERT, respectively. 14 TABLE VIII LOW LEVEL METRICS FOR LIKELIHOOD ASSESSMENT USING CVSS. Metric CVSS Metric evaluation Numerical value Access Vector (AV) Base Local 0.4 Adjacent 0.6 Network 1 Authentication (Au) Base Multiple 0.5 Single 0.55 None 1 Access Complexity (AC) Base High 0.5 Medium 0.75 Low 1 Exploitability (E) Temporal Unproven 0.85 Proof of concept 0.9 Functional 0.95 High 1 Not deﬁned 1 Report Conﬁdence (RC) Temporal Unconﬁrmed 0.9 Uncorroborated 0.9 Conﬁrmed 1 Not deﬁned 1 TABLE IX LIKELIHOOD OF OCCURRENCE OF THE CONSIDERED THREATS, OBTAINED USING DIFFERENT METHODS. ID Threat LC LCVSS LEXPERT 1 Malware 0.86 0.90 0.80 2 Web-based attacks 0.61 0.64 0.75 3 Denial of services 0.92 0.95 0.90 4 Malicious insiders 0.97 0.15 0.55 5 Phishing and social engineering 0.92 0.90 0.95 6 Malicious code 0.52 0.81 0.70 7 Stolen devices 0.78 0.09 0.45 8 Ransomware 0.72 0.99 0.85 9 Botnets 0.86 0.86 0.80 We notice that, in several cases, the three considered methods provide comparable results. However, in some cases, the numerical methods (MAGIC and the CVSS-based) give contrasting results. We argue this is due to the incapability of the CVSS-based method to catch that different organizations may react differently to the same threats, if the state of their organizational infrastructure is dissimilar. In fact, the CVSS- based method considers only marginally and indirectly the degree of compliance of the organization to the best practices for threat prevention, whereas MAGIC takes it into account in the preliminary posture assessment. In contrast, on the one hand, the external experts evaluation seems very balanced in most situations but, on the other hand, as already asserted, it is subjective, in that a different team of experts might return signiﬁcantly different results. Moreover, the involvement of external experts is costly, it is time consuming, and requires a signiﬁcant amount of data to be processed, which may not be available. B. Qualitative comparison In this section we provide a qualitative comparison of several cyber risk assessment and/or management methods. The considered approaches, the evaluated metrics and the results of the assessment are shown in Table X. In short, we notice that among the most relevant existing risk assessment/management methods, most of them require the subjective estimates of calibrated or external experts. Clearly, the extensive use of external experts (described in Section VII-A) restricts the range of the organizations that can afford it to those in the medium and large scale, making the associated methods barely applicable to small organiza- tions. Some methods, such as MEHARI or MAGERIT, rely on internal experts, like our method, but they implement only qualitative approaches. Therefore, they do not provide a numerical estimate of the likelihood of occurrence of cyber incidents, leading to results which might need an involved interpretation and may not be easily reproducible. Even in the CVSS-based methods, where the likelihood is numerically estimated based on internal experts analyses, the process is still subjective, since there is no automatic and universal way to assign scores to the threats exploiting the considered vulnerabilities. Moreover, none of the considered methods directly relate the likelihood of occurrence of cyber incidents to the posture of the organization. MAGIC tries to overcome all these shortcomings, by providing a numerical approach for the likelihood assessment, relying on the simple initial task of some internal experts compiling questionnaires about the status of the target organization. VIII. CONCLUSION Most existing cyber risk assessment methods are affected by subjectivity or require a high level of expertise from the organizations, which makes them barely practical in real-case scenarios. We have tackled this issue by proposing a novel 15 TABLE X QUALITATIVE COMPARISON BETWEEN THE CONSIDERED METHODS. LEGEND: RM=RISK MANAGEMENT; RA=RISK ASSESSMENT; LA=LIKELIHOOD ASSESSMENT. Method Scope Likelihood Assessment Assessors Target Organizations ISO/IEC 27005:2018 RM Guidelines only External experts All NIST SP 800-30 RA Guidelines only External experts Governmental* EBIOS RM Qualitative Team of different stakeholders All CRAMM RA Qualitative External experts Large scale* OCTAVE RM Qualitative Internal experts All MEHARI RM Qualitative Internal experts All MAGERIT RM Numerical Calibrated experts Public Administration* IRAM2 RM Qualitative Internal experts All IT-Grundschutz RM Qualitative Internal and external experts All CORAS RA Qualitative or numerical Internal and external experts Medium and large scale HTMA RA Numerical Calibrated experts Medium and large scale FAIR RA Numerical Calibrated experts Medium and large scale LiSRA RA Numerical External experts All Fuzzy-logic based LA Numerical External experts Medium and large scale CVSS-based LA Numerical Internal experts All MAGIC LA Numerical Internal experts All * Can be extended to other types of organizations. probabilistic method, called MAGIC that, based on the orga- nization posture, estimates the likelihood (or the frequency) of occurrence of a cyber incident or, more generally, of a list of cyber incidents. Through numerical simulations we have shown how MAGIC can be plugged into statistical cyber risk assessment methods, like HTMA and FAIR, to eventually assess the risk as the likelihood of occurrence of cyber incidents combined with their impact. We have performed both qualitative and quantitative comparisons with alternative approaches, showing that our method is reliable and general, in the sense that it can provide inputs to other risk assessment and/or risk management methods. Moreover, MAGIC tries to catch all the advantages of existing likelihood assessment methods. In conclusion, due to its simplicity and rather low computational demand, we argue that MAGIC can be applied to any type of organization (small, medium, or large), without any loss of generality. ACKNOWLEDGMENT The authors are very grateful to Giovanni Libertini and to the team of cybersecurity analysts of Ancharia S.r.l. for their precious insights. REFERENCES [1] G. Rafaiani, M. Battaglioni, M. Baldi, F. Chiaraluce, G. Libertini, L. Spalazzi, and G. Cancellieri, “A functional approach to cyber risk assessment,” in Proc. AEIT 2021 International Annual Conference, Second Virtual Edition, Oct. 2021. [2] National Institute of Standards and Technology (NIST), “Special pub- lication 800-30 revision 1 - information security: Guide for conducting risk assessments,” Sep. 2012. [3] International Organization for Standardization (ISO), “ISO 31000:2018 - risk management - guidelines,” Feb. 2018. [4] ——, “ISO/IEC 27005:2018 information technology - security tech- niques - information security risk management,” Jul. 2019. [5] P. Shamala, R. Ahmad, and M. Yusoff, “A conceptual framework of info structure for information security risk assessment (ISRA),” Journal of Information Security and Applications, vol. 18, no. 1, pp. 45–52, 2013, sETOP’2012 and FPS’2012 Special Issue. [6] M. Stamatelatos, “Probabilistic risk assessment: What is it and why is it worth performing it?” NASA Ofﬁce of Safety and Mission Assurance, Tech. Rep., 2000. [7] D. W. Hubbard and R. Seiersen, How to Measure Anything in Cyberse- curity Risk. John Wiley & Sons Inc, 2016. [8] J. Freund and J. Jones, Measuring and Managing Information Risk: A FAIR Approach. Butterworth-Heinemann, 2014. [9] P. Santini, G. Gottardi, M. Baldi, and F. Chiaraluce, “A data-driven approach to cyber risk assessment,” Security and Communication Net- works, vol. 2019, Article ID 6716918, 2019. [10] National Cybersecurity Agency of France (ANSSI), “EBIOS Risk Man- ager,” https://www.ssi.gouv.fr/en/guide/ebios-risk-manager-the-method. [11] Z. Yazar, “A qualitative risk analysis and management tool - CRAMM,” SANS InfoSec Reading Room White Paper, vol. 11, pp. 12–32, 2002. [12] C. Alberts, A. Dorofee, J. Stevens, and C. Woody, “Introduction to the OCTAVE Approach,” https://resources.sei.cmu.edu/library/asset-view.cfm?assetid=51546, Carnegie-Mellon Univ Pittsburgh Pa Software Engineering Inst, Tech. Rep., 2003. [13] CLUSIF (Club de la S´ecurit´e de l’Information Franc¸ais), “MEHARI (MEthod for Harmonized Analysis of RIsk).” [Online]. Available: https://www.enisa.europa.eu/topics/threat-risk-management/risk-management/current [14] Ministerio de Hacienda y Administraciones Publicas (Spanish Ministry of Finance and Public Administrations), “MAGERIT – version 3.0 Methodology for Information Systems Risk Analysis and Management,Book I - The Method,” https://administracionelectronica.gob.es/pae Home/dam/jcr:80b16a91-75b1-432d-ab2 [15] Information Security Forum, “Information Risk Assessment Methodology 2 (IRAM2).” [Online]. Available: https://www.securityforum.org/solutions-and-insights/information-risk-assessmentme [16] Bundesamt f¨ur Sicherheit in der Informationstechnik (BSI), “BSI Standard 100-3 - Risk analysis based on IT-Grundschutz.” [Online]. Available: https://www.bsi.bund.de/SharedDocs/Downloads/EN/BSI/Grundschutz/International/b [17] SourceForge, “The CORAS Method,” http://coras.sourceforge.net/. [18] O. Giuca, T. Popescu, A. Popescu, G. Prostean, and D. Popescu, “A survey of cybersecurity risk management frameworks,” in Soft Computing Applications. SOFA 2018. Advances in Intelligent Systems and Computing, V. Balas, L. Jain, M. Balas, and S. Shahbazova, Eds. Springer, Cham, 2021, vol. 1221. [19] D. Gritzalis, G. Iseppi, A. Mylonas, and V. Stavrou, “Exiting the risk assessment maze: A meta-survey,” ACM Comput. Surv., vol. 51, no. 1, pp. 1–30, Jan. 2019. [20] M. Khosravi-Farmad and A. Ghaemi-Bafghi, “Bayesian decision network-based security risk management framework,” Journal of Net- work and Systems Management, vol. 28, no. 4, pp. 1794–1819, Oct. 2020. [21] C. Schmitz and S. Pape, “LiSRA: Lightweight security risk assessment for decision support in information security,” Computers & Security, vol. 90, 101656, Mar. 2020. [22] A. P. Henriques de Gusm˜ao, L. Camara e Silva, M. Mendonc¸a Silva, T. Poleto, and A. P. Cabral Seixas Costa, “Information security risk analysis model using fuzzy decision theory,” International Journal of Information Management, vol. 36, no. 1, pp. 25–34, Feb. 2016. 16 [23] A. Handa, S. Mukhopadhyay, S. Mallick, N. Kumar, S. Shukla, R. Minz, S. Nagarmat, and R. Rakesh, “Cyber risk assessment of networked cyber assets using probabilistic model checking,” in Proc. 2019 IEEE Conference on Information and Communication Technology, Allahabad, India, Dec. 2019. [24] M. Aksu, M. Dilek, E. Tatli, K. Bicakci, H. Dirik, M. Demirezen, and T. Aykir, “A quantitative CVSS-based cyber security risk assessment methodology for IT systems,” in Proc. 2017 International Carnahan Conference on Security Technology (ICCST), Madrid, Spain, Oct. 2017. [25] C. Alberts, A. Dorofee, J. Stevens, and C. Woody, “OCTAVE-S im- plementation guide, Version 1.0,” Manuel ´electronique. Pittsburg, PA,: Software Engineering Institute, Carbegie Mellon university, 2005. [26] R. Caralli, J. Stevens, L. Young, and W. Wilson, “Introducing OCTAVE Allegro: Improving the Information Security Risk Assessment Process,” http://resources.sei.cmu.edu/library/asset-view.cfm?AssetID=8419, Soft- ware Engineering Institute, Carnegie Mellon University, Pittsburgh, PA, Tech. Rep. CMU SEI-2007-TR-012, 2007. [27] P. Mell, K. Scarfone, and S. Romanosky, “A complete guide to the common vulnerability scoring system version 2.0,” https://tsapps.nist.gov/publication/get pdf.cfm?pub id=51198, 2007. [28] National Institute of Standards and Technology (NIST), “Special pub- lication 800-53a revision 5 - assessing security and privacy controls in information systems and organizations,” Jan. 2022. [29] Center of Internet Security (CIS), “CIS Controls v.8,” May 2021. [30] European Union Agency For Network and Information Security (ENISA), “Handbook on security of personal data processing,” Dec. 2017. [31] National Institute of Standards and Technology (NIST), “Framework for improving critical infrastructure cybersecurity - version 1.1,” Apr. 2018. [32] Federal Financial Institutions Examination Council (FFIEC), “Cyberse- curity assessment tool,” May 2017. [33] Associazione Italiana per la Sicurezza Informatica, “Rapporto Clusit 2021 sulla sicurezza ICT in Italia,” Oct. 2021, in Italian. [34] D. Fekedulegn, M. M. Siurtain, and J. Colbert, “Parameter estimation of nonlinear growth models in forestry,” Silva Fennica, vol. 33, no. 4, pp. 327–336, Dec. 1999. [35] P. Verboon and G. J. Ygram Peters, “Applying the generalized logis- tic model in single case designs: Modeling treatment-induced shifts,” Behavior Modiﬁcation, vol. 44, no. 1, pp. 27–48, Jan. 2020. [36] Ponemon Institute LLC-Accenture, “Cost of cyber crime study, Techni- cal Report,” 2017.