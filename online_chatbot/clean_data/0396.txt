arXiv:2312.17221v1 [cs.CR] 28 Dec 2023 Scalable and automated Evaluation of Blue Team cyber posture in Cyber Ranges Federica Bianchi, Enrico Bassetti, Angelo Spognardi Abstract Cyber ranges are virtual training ranges that have emerged as indis- pensable environments for conducting secure exercises and simulating real or hypothetical scenarios. These complex computational infrastructures enable the simulation of attacks, facilitating the evaluation of defense tools and methodologies and developing novel countermeasures against threats. One of the main challenges of cyber range scalability is the exercise evalu- ation that often requires the manual intervention of human operators, the White team. This paper proposes a novel approach that uses Blue and Red team reports and well-known databases to automate the evaluation and assessment of the exercise outcomes, overcoming the limitations of existing assessment models. Our proposal encompasses evaluating vari- ous aspects and metrics, explicitly emphasizing Blue Teams’ actions and strategies and allowing the automated generation of their cyber posture. 1 Introduction In the face of escalating cyber threats, organizations must continually fortify their defenses against malicious actors. Cyber range exercises have emerged as vital training grounds, enabling security teams to simulate realistic attack sce- narios and assess their preparedness. Typically involving Blue Teams defending infrastructure and Red Teams launching simulated attacks, these exercises oﬀer hands-on experience. However, evaluating exercise results has been challeng- ing, relying on a combination of service metrics and manual grading by human experts. This approach is time-consuming, error-prone, and limits prompt feed- back on Blue Team responses. Also, the grading phase often receives little attention, hindering the potential for comprehensive improvement of defensive strategies [16, 15]. As cybersecurity becomes increasingly paramount, there is a pressing need for a robust evaluation metric, automated processes, and objective insights into Blue Team performance. To address these limitations, this paper presents a novel approach that lever- ages reports and well-known cybersecurity databases to automate the evaluation and assessment of cyber range exercise results, explicitly focusing on the Blue Team’s performance. Our proposed framework exploits the power of automa- tion and addresses the limitations of manual evaluation methods to oﬀer an 1 eﬃcient and objective means of assessing various aspects and metrics related to the exercise. The primary contribution of this research lies in developing a comprehensive and scalable approach that automates the evaluation process. By producing a tree-based representation of attack and defense reports, we propose to evaluate a cyber exercise leveraging the comparison between the models of the diﬀerent Red and Blue teams’ actions. Our approach allows the automated comparison and evaluation of multiple Blue teams in parallel, helping the White team in the exercise evaluation and enabling organizations to extract valuable insights from the results promptly. The contribution provided concerns, among other things, the possibility of evaluating various aspects and metrics related to the Blue Team, particularly those related to the correctness and accuracy of the response to Red Team attacks. This paper is structured as follows: Section 2 provides a background of the current evaluation methods and their limitations; Section 3 provides the general idea for our proposal; Sections 4, 5 and 6 describe in details the core of our proposal, i.e., how to deﬁne the cyber range reports for the teams and how to process them. Finally, Section 7 concludes the paper and discusses ideas for future research. 2 Background and related works The current evaluation methods in cyber ranges vary in their approaches, with a growing emphasis on automating scoring to reduce manual eﬀorts. Typically, cyber ranges display exercise progress through dashboards, and the evaluation is primarily based on the number of successfully completed challenges. Some platforms, such as I-tee [14], Hack The Box (HTB) [6], iCTF [13], Kypo [4], and Locked Shields [10] use scoring systems based on goal completion and additional metrics like service availability, integrity or conﬁdentiality. However, the limitations of these methods are evident. Automatic scor- ing engines rely on basic calculations tied to points obtained achieving speciﬁc objectives or on metrics, neglecting the detailed performance and strategies em- ployed by participants. Such evaluations often lack a comprehensive assessment of participants’ capabilities. Andreolini et al. [1] proposed a scoring system that evaluates participants’ actions by comparing them against “ideal” actions deﬁned by instructors. This involves constructing reference and trainee graphs representing ideal and partic- ipant actions. While a step forward, this approach has its own set of limitations. Firstly, it may not fully capture the strategies and defenses of the Blue team, mainly since they depend on the red team’s attacks, which can be decided ongoing and thus cannot be modeled a priori from the graphs entered by the instructors. Secondly, the evaluation process is not fully automatic, requiring continuous manual work by instructors to deﬁne reference graphs and adapt them to evolving exercises. The subsequent chapters will describe an alternative approach that aims to 2 automatically evaluate the Blue team’s results with minimal manual contribu- tion by the white team. 3 Automatic evaluation of exercises We propose a new approach for automatically evaluating cyber-range exercises, focusing on the Blue Team results. This new approach comprises well-deﬁned templates for Red and Blue Teams reports, a set of procedures to derive the ﬁnal score, and a visualization named Cyber Posture. The pipeline for the automatic evaluation can be formalized as follows: (1) collection of reports from Blue and Red Teams; (2) deﬁnition of Refer- ence/Response Graphs from reports; (3) automatic evaluation of multiple inter- mediate scores from deﬁned graphs; (4) computation of the ﬁnal score and the Cyber Posture. We start by deﬁning the structure for the reports. Then, we explain how to automatically build Reference and Response Graphs from team reports when the exercise ends: both are necessary to the score deﬁnition. Finally, we explain how to compute scores and the Cyber Posture from graphs data. 4 Team reports Most modern cyber ranges are equipped with a reporting system, which gener- ates reports during the exercise. Reports are produced by the Red Team when they attack and are matched with reports from the Blue Team describing the detected attack. Since one task of the White Team is to evaluate the Blue Teams based on these reports, our goal is to lighten the load of the White Team for the evaluation by making these phases automatic. The structures we propose for Blue and Red Team reports are based on the components of the MITRE ATT&CK [12] Matrix (accessed via STIX [3]), a database containing knowledge collected by the security community about tactics, techniques, and procedures used by attackers, together with the cor- responding possible mitigations and detections. Reports structures prioritize simplicity, ease of ﬁlling, and unobtrusiveness, especially for the Blue Team, en- hancing and eﬃcient and eﬀective report writing and the accuracy of automated analysis. The Red Team report template includes ﬁelds such as attack objective, tech- niques, sub-techniques, target of the attack, start time, and outcome. It could also arbitrarily contain Desirable Mitigation and the Desirable Detection, which are ﬁlled by the White Team either at the beginning of the exercise, or at the end of the simulation, before the automatic evaluation phase is initiated. Additionally, the White Team can also choose to assign weights in [0, 1] to speciﬁc ﬁelds in the report- Tactic, Techniques, Sub-Techniques, Desirable Mitigations, and Desirable Detection. These weights impact the Blue Team evaluation by assigning priorities to speciﬁc aspects that they may want to 3 better assess. If unassigned, automatic weights will be applied during evaluation (Section 6). The Desirable Detection and Desirable Mitigations optional ﬁelds only gain signiﬁcance if the White Team assigns weights to the abovementioned ﬁelds. In this way, a Blue Team achieves an higher score by precisely executing one of those mitigations and detections. Anyways, even if it used a mitigation or detection technique not listed by the White Team but still present in the ATT&CK for that attack, it would still take a higher score than if it had not performed proper mitigation or detection. The Blue Team report template consists of presumed tactics, techniques, sub-techniques, applied mitigations, detection types, target attacked, and de- tection start time. 5 From reports to ADTrees Once the teams ﬁll the reports, the cyber range scoring system processes them to produce two graphs, Reference Graph and Response Graph, for each report. These two graphs are used in the scoring phase to calculate the total score assigned to each Blue Team. We named the structure of these graphs Repor- tADTree, as they are constructed as a variant of ADTree [5, 2, 11], a directed tree modeling attack-defense scenarios using two types of nodes: attack and defense nodes. The attack nodes consist of tactics, techniques, and sub-techniques nodes, as deﬁned in the MITRE ATT&CK. In ReportADTrees, the initial vertex is always the tactic, followed by techniques, which can branch into mitigations, detections, and sub-techniques. Sub-techniques, in turn, may have their own mitigations and detections as child nodes. We deﬁne Reference Graph as a reference for the evaluation of attack re- sponse and detection by the exercise participants. The graph is assembled using the attack information from the Red Team report and the ATT&CK database, following the ReportADTree structure. Each ReportADTree node has the weight assigned by the White Team in the Red Team report or auto- matically generated ones if not present. We also deﬁne the Response Graph that evaluates the Blue Team’s response to Red Team attacks: it is compared to the corresponding reference graph to assign a score to the Blue Team. The Response Graph is constructed using the defense information extracted from the ﬁelds of a Blue Team report, with a process similar to that described for the Reference Graph. However, only the Mitigations and Detection deﬁned by the Blue Team report are added as child nodes of the Techniques or Sub-techniques they refer to. 4 6 Evaluation The automatic evaluation phase starts once the Reference and Response graphs have been constructed. We propose to evaluate diﬀerent factors to capture the variety of aspects of an exercise. We deﬁne multiple intermediate scores and an aggregated ﬁnal score for each Blue Team evaluation. Factors include attack management, attack strategy comprehension, knowledge of (sub-)techniques, accuracy in identifying techniques, responsiveness, and metrics such as avail- ability and integrity. The evaluation aims to measure the Blue Team’s ability to respond to Red Team attacks, understand attack strategies, and accurately identify and mitigate techniques, ultimately contributing to an aggregated ﬁnal score. The evaluation starts with an initial phase, which aims to modify the ref- erence and the response graphs to enable a direct evaluation based solely on response graph nodes and their associated weights. The White Team can spec- ify the weights manually; they are automatically generated if missing. Once all reference graph weights are set, the two graphs are compared using a breadth-ﬁrst search to assign weights to response graph nodes that the Blue Team has correctly individuated. In contrast, nodes without matches in the reference graph are removed. For techniques and sub-techniques, even if there is not an exact match with the response graph, we can still estimate the accuracy of the guesses of the Blue Team. The idea is to compute the distance between the correct Technique and the one guessed by the Blue Team, using CAPEC [8] from MITRE, a public database of known attack patterns, via existing CAPEC- ATT&CK mappings [9]. The score calculation involves deﬁning multiple intermediate scores to eval- uate factors mentioned earlier, with the option to associate weights with each intermediate score based on exercise objectives. Once all these intermediate scores are computed, they are averaged into a unique Final Score. The ﬁrst intermediate score is the Comprehension Score, an indicator that measures the knowledge of (sub-)techniques and the attack strategy comprehen- sion. It assesses the Blue Team’s precision in identifying the techniques and sub-techniques used by the Red Team in the attack and the Blue Team’s com- prehension of the overall attack strategy and tactic. The second one is the Defense Score; it represents the attack management factor, which is the Blue Team’s ability to understand how to respond to Red Team attacks. The third is the Implementation Score, which assesses whether the Blue Team implemented the identiﬁed mitigation. While the Comprehension Score evaluates the team’s ability to identify the correct mitigations for techniques or sub-techniques, the Implementation Score focuses explicitly on the actual execution of those mitiga- tions. Finally, the last score is the Responsiveness Score, which represents the interval between the attack beginning time reported in the Red Team report and the beginning time reported in the Blue Team report. From the information gathered, it is possible to deﬁne an overall picture of the capabilities that each Blue Team developed during the exercise, in addition to individual scores. This aspect is often referred to as Cyber Posture [7]. The 5 measures reﬂect the Blue Team’s overall defensive ability, oﬀering a comprehen- sive view of the exercise outcomes and individual evaluation aspects. 7 Conclusions and future work We presented a novel approach to automate the evaluation and assessment of cy- ber range exercises, explicitly focusing on Blue Teams. The proposed framework leverages custom reports, graphs, and well-known databases, such as MITRE ATT&CK and CAPEC, to provide a comprehensive and scalable solution that addresses the limitations of traditional manual evaluation methods. The au- tomation of the evaluation phase addresses the challenges of time-consuming and error-prone manual evaluations. As future work, we are designing a fully working scoring platform for cyber ranges, integrating a GUI for receiving and evaluating the reports. Moreover, we plan to reﬁne and expand the evaluation metrics used in the automated framework, identifying additional indicators and benchmarks that can provide a more comprehensive assessment of the Blue Team’s performance. Finally, we plan to integrate machine learning and artiﬁcial intelligence techniques to en- hance the automation process by enabling intelligent analysis and interpretation of the evaluation results. 8 Acknowledgments This work was partially supported by project “Prebunking: predicting and mit- igating coordinated inauthentic behaviors in social media” project, funded by Sapienza University of Rome. References [1] M. Andreolini, V. G. Colacino, M. Colajanni, and M. Marchetti. A frame- work for the evaluation of trainee performance in cyber range exercises. Mobile Networks and Applications, 25:236–247, 2020. [2] Bagnato, Alessandra and Kordy, B and Meland, Per H˚akon and Schweitzer, Patrick. Attribute decoration of attack–defense trees. International Journal of Secure Software Engineering (IJSSE), 3:1–35, 01 2012. [3] S. Barnum. Standardizing cyber threat intelligence information with the structured threat information expression (STIX). Mitre Corporation, 11:1– 22, 2012. [4] P. ˇCeleda, J. ˇCegan, J. Vykopal, D. Tovarˇn´ak, et al. Kypo–a platform for cyber defence exercises. M&S Support to Operational Tasks Including War Gaming, Logistics, Cyber Defence. NATO Science and Technology Organization, 2015. 6 [5] B. Kordy, S. Mauw, S. Radomirovi´c, and P. Schweitzer. Attack–defense trees. Journal of Logic and Computation, 24(1):55–87, 2014. [6] H. T. B. Ltd. Hack the box (HTB), 2023. Last Accessed: 5 Jul 2023. [7] D. Massa. Cyber range: Virtual hacking warfare, 2019. Last Accessed: 5 Jul 2023. [8] MITRE. CAPEC, 2007. Last Accessed: 5 Jul 2023. [9] MITRE. CAPEC related projects, 2007. Last Accessed: 5 Jul 2023. [10] N. C. C. D. C. of Excellence. Locked shields, 2010. Last Accessed: 5 Jul 2023. [11] B. Schneier. Attack trees. Dr Dobb’s Journal-Software Tools for the Pro- fessional Programmer, 24(12):21–31, 1999. [12] B. E. Strom, A. Applebaum, D. P. Miller, K. C. Nickels, A. G. Pennington, and C. B. Thomas. MITRE ATT&CK: Design and philosophy. In Technical report. The MITRE Corporation, 2018. [13] E. Trickel, F. Disperati, E. Gustafson, F. Kalantari, M. Mabey, N. Tiwari, Y. Safaei, A. Doup´e, and G. Vigna. Shell we play a game? CTF-as-a-service for security education. In 2017 USENIX Workshop on Advances in Security Education (ASE 17), Vancouver, BC, Aug. 2017. USENIX Association. [14] C. Willems and C. Meinel. Online assessment for hands-on cyber security training in a virtual lab. In Proceedings of the 2012 IEEE Global Engineer- ing Education Conference (EDUCON), pages 1–10. IEEE, 2012. [15] M. M. Yamin and B. Katt. Modeling and executing cyber security exercise scenarios in cyber ranges. Comput. Secur., 116(C), may 2022. [16] M. M. Yamin, B. Katt, and V. Gkioulos. Cyber ranges and security testbeds: Scenarios, functions, tools and architecture. Computers & Se- curity, 88:101636, 2020. 7