A Formal Approach to Cyber-Physical Attacks Ruggero Lanotte Dipartimento di Scienza e Alta Tecnologia Università dell’Insubria, Como, Italy Massimo Merro and Riccardo Muradore Dipartimento di Informatica Università degli Studi di Verona, Italy Luca Viganò Department of Informatics King’s College London, UK Abstract—We apply formal methods to lay and streamline theoretical foundations to reason about Cyber-Physical Systems (CPSs) and cyber-physical attacks. We focus on integrity and DoS attacks to sensors and actuators of CPSs, and on the timing aspects of these attacks. Our contributions are threefold: (1) we deﬁne a hybrid process calculus to model both CPSs and cyber-physical attacks; (2) we deﬁne a threat model of cyber-physical attacks and provide the means to assess attack tolerance/vulnerability with respect to a given attack; (3) we formalise how to estimate the impact of a successful attack on a CPS and investigate possible quantiﬁcations of the success chances of an attack. We illustrate deﬁnitions and results by means of a non-trivial engineering application. I. INTRODUCTION Context and motivation: Cyber-Physical Systems (CPSs) are integrations of networking and distributed computing systems with physical processes that monitor and control entities in a physical environment, with feedback loops where physical processes affect computations and vice versa. For example, in real-time control systems, a hierarchy of sensors, actuators and control processing components are connected to control stations. Different kinds of CPSs include supervisory control and data acquisition (SCADA), programmable logic controllers (PLC) and distributed control systems. In recent years there has been a dramatic increase in the number of attacks to the security of cyber-physical and critical systems, e.g., manipulating sensor readings and, in general, inﬂuencing physical processes to bring the system into a state desired by the attacker. Many (in)famous examples have been so impressive to make the international news, e.g.: the Stuxnet worm, which reprogrammed PLCs of nuclear centrifuges in Iran [5] or the attack on a sewage treatment facility in Queensland, Australia, which manipulated the SCADA system to release raw sewage into local rivers and parks [18]. As stated in [8], the concern for consequences at the physical level puts CPS security apart from standard information security, and demands for ad hoc solutions to properly address such novel research challenges. The works that have taken up these challenges range from proposals of different notions of cyber-physical security and attacks (e.g., [3], [8], [11], to name a few) to pioneering extensions to CPS security of standard formal approaches (e.g., [3], [4], [22]). However, to the best of our knowledge, a systematic formal approach to cyber-physical attacks is still to be fully developed. Background: The dynamic behaviour of the physical plant of a CPS is often represented by means of a discrete-time state-space model consisting of two equations of the form xk+1 = Axk + Buk + wk and yk = Cxk + ek , where xk ∈Rn is the current (physical) state, uk ∈Rm is the input (i.e., the control actions implemented through actuators) and yk ∈Rp is the output (i.e., the measurements from the sensors). The uncertainty wk ∈Rn and the measurement error ek ∈Rp represent perturbation and sensor noise, respectively, and A, B, and C are matrices modelling the dynamics of the physical system. Here, the next state xk+1 depends on the current state xk and the corresponding control actions uk, at the sampling instant k ∈N. The state xk cannot be directly observed: only its measurements yk can be observed. The physical plant is supported by a communication network through which the sensor measurements and actuator data are exchanged with controller(s) and supervisor(s) (e.g., IDSs), which are the cyber components (also called logics) of a CPS. Contributions: In this paper, we focus on a formal treatment of both integrity and Denial of Service (DoS) attacks to physical devices (sensors and actuators) of CPSs, paying particular attention to the timing aspects of these attacks. The overall goal of the paper is to apply formal methodologies to lay theoretical foundations to reason about and statically detect attacks to physical devices of CPSs. Our contributions are threefold. The ﬁrst contribution is the deﬁnition of a hybrid process calculus, called CCPSA, to formally specify both CPSs and cyber-physical attacks. In CCPSA, CPSs have two components: a physical component denoting the physical plant (also called environment) of the system, and containing information on state variables, actuators, sensors, evolution law, etc., and a cyber component that governs access to sensors and actuators, channel-based communication with other cyber components. Thus, channels are used for logical interactions between cyber components, whereas sensors and actuators make possible the interaction between cyber and physical components. CCPSA adopts a discrete notion of time [9] and it is equipped with a labelled transition semantics (LTS) that allows us to observe both physical events (system deadlock and violations of safety conditions) and cyber events (channel communications). Based on our LTS, we deﬁne two trace-based system preorders: a trace preorder, ⊑, and a timed variant, ⊑m..n, for m, n ∈ N+ ∪∞, which takes into account discrepancies of execution traces within the time interval m..n. As a second contribution, we formalise a threat model that speciﬁes attacks that can manipulate sensor and/or actuator arXiv:1611.01377v2 [cs.CR] 21 Apr 2017 Plant wk Controller & IDS Actuators Sensors ek ua k xk uk ya k yk Fig. 1. Our threat model for CPSs signals in order to drive a CPS into an undesired state [19]. Cyber-physical attacks typically tamper with both the physical (sensors and actuators) and the cyber layer. In our threat model, communication cannot be manipulated by the attacker, who instead may compromise (unsecured) physical devices, which is our focus. As depicted in Figure 1, our attacks may affect directly the sensor measurements or the controller commands. • Attacks on sensors consist of reading and eventually replacing yk (the sensor measurements) with ya k. • Attacks on actuators consist of reading, eavesdropping and eventually replacing the controller commands uk with ua k, affecting directly the actions the actuators may execute. We group attacks into classes. A class of attacks takes into account both the malicious activity I on physical devices and the timing parameters m and n of the attack: begin and end of the attack. We represent a class C as a total function C ∈[I →P(m..n)]. Intuitively, for ι ∈I, C(ι) ⊆m..n denotes the set of time instants when an attack of class C may tamper with the device ι. As observed in [11], timing is a critical issue in CPSs because the physical state of a system changes continuously over time, and as the system evolves in time, some states might be more vulnerable to attacks than others. For example, an attack launched when the target state variable reaches a local maximum (or minimum) may have a great impact on the whole system behaviour [12]. Furthermore, not only the timing of the attack but also the duration of the attack is an important parameter to be taken into consideration in order to achieve a successful attack. For example, it may take minutes for a chemical reactor to rupture [20], hours to heat a tank of water or burn out a motor, and days to destroy centrifuges [5]. In order to make security assessments on our CPSs, we adopt a well-known approach called Generalized Non Deducibility on Composition (GNDC) [6]. Thus, in CCPSA, we say that a CPS Sys tolerates a cyber-physical attack A if Sys ∥A ⊑Sys . In this case, the presence of the attack A, does not affect the whole (physical and logical) observable behaviour of the system Sys, and the attack can be considered harmless. On the other hand, we say that a CPS Sys is vulnerable to a cyber-physical attack A of class C ∈[I →P(m..n)] if there is a time interval m′..n′ in which the attack becomes observable (obviously, m′ ≥m). Formally, we write: Sys ∥A ⊑m′..n′ Sys . We provide sufﬁcient criteria to prove attack toler- ance/vulnerability to attacks of an arbitrary class C. We deﬁne a notion of “most powerful attack” of a given class C, Top(C), and prove that if a CPS tolerates Top(C) then it tolerates all attacks A of class C. Similarly, if a CPS is vulnerable to Top(C), in the time interval m′..n′, then no attacks of class C can affect the system out of that time interval. This is very useful when checking for attack tolerance/vulnerability with respect to all attacks of a given class C. As a third contribution, we formalise how to estimate the impact of a successful attack on a CPS and investigate possible quantiﬁcations of the chances for an attack of being successful when attacking a CPS. This is important since, in industrial CPSs, before taking any countermeasure against an attack, engineers typically ﬁrst try to estimate the impact of the attack on the system functioning (e.g., performance and security) and weigh it against the cost of stopping the plant. If this cost is higher than the damage caused by the attack (as is sometimes the case), then engineers might actually decide to let the system continue its activities even under attack. We thus provide a metric to estimate the deviation of the system under attack with respect to expected behaviour, according to its evolution law and the uncertainty of the model. Then, we prove that the impact of the most powerful attack Top(C) represents an upper bound for the impact of any attack A of class C. We introduce a non-trivial running example taken from an engineering application and use it to illustrate our deﬁnitions and cases of CPSs that tolerate certain attacks, and of CPSs that suffer from attacks that drag them towards undesired behaviours. We remark that while we have kept the example simple, it is actually far from trivial and designed to describe a wide number of attacks, as will become clear below. All the results exhibited in the paper have been formally proved (due to lack of space, proofs are given in the appendix). Moreover, the behaviour of our running example and of most of the cyber-physical attacks appearing in the paper have been simulated in MATLAB. Organisation: In § II, we give syntax and semantics of CCPSA. In § III, we deﬁne cyber-physical attacks and provide sufﬁcient criteria for attack tolerance/vulnerability. In § IV, we estimate the impact of attacks on CPSs, and investigate possible quantiﬁcations of the success chances of an attack. In § V, we draw conclusions and discuss related and future work. II. THE CALCULUS In this section, we introduce our Calculus of Cyber-Physical Systems and Attacks, CCPSA, which extends the Calculus of Cyber-Physical Systems deﬁned in [2] with speciﬁc features to formalise and study attacks to physical devices. Let us start with some preliminary notations. We use x, xk ∈ X for state variables, c, d ∈C for communication channels, a, ak ∈A for actuator devices, s, sk ∈S for sensors devices, and p, q for both sensors and actuators (generically called physical devices). Values, ranged over by v, v′ ∈V, are built from basic values, such as Booleans, integers and real numbers. Actuator names are metavariables for actuator devices like valve, light, etc. Similarly, sensor names are metavariables for sensor devices, e.g., a sensor thermometer. 2 Given a set of names N, we write RN to denote the set of functions [N →R] assigning a real to each name in N. For ξ ∈RN , n ∈N and v ∈R, we write ξ[n 7→v] for the function ψ ∈RN such that ψ(m) = ξ(m), for any m ̸= n, and ψ(n) = v. Finally, we distinguish between real intervals, such as (m, n], for m ∈R and n ∈R∪∞, and integer intervals, written m..n, for m ∈N and n ∈N ∪∞. As we will adopt a discrete notion of time, we will use integer intervals to denote time intervals. Deﬁnition 1 (Cyber-physical system). In CCPSA, a cyber- physical system consists of two components: a physical environment E that encloses all physical aspects of a system and a cyber component P that interacts with sensors and actuators of the system, and can communicate, via channels, with other cyber components of the same or of other CPSs. Given a set S of secured physical devices of E, we write E ⋊⋉S P to denote the resulting CPS, and use M and N to range over CPSs. We write E ⋊⋉P when S = ∅. In a CPS E ⋊⋉S P, the “secured” devices in S are accessed in a protected way and hence they cannot be attacked.1 Let us now deﬁne physical environments E and cyber components P in order to formalise our proposal for modelling (and reasoning about) CPSs and cyber-physical attacks. Deﬁnition 2 (Physical environment). Let ˆ X ⊆X be a set of state variables, ˆ A ⊆A be a set of actuators, and ˆS ⊆S be a set of sensors. A physical environment E is an 8-tuple ⟨ξx, ξu, ξw, evol, ξe, meas, inv, safe⟩, where: • ξx ∈R ˆ X is the state function, • ξu ∈R ˆ A is the actuator function, • ξw ∈R ˆ X is the uncertainty function, • evol : R ˆ X × R ˆ A × R ˆ X →2R ˆ X is the evolution map, • ξe ∈R ˆ S is the sensor-error function, • meas : R ˆ X × R ˆ S →2R ˆ S is the measurement map, • inv : R ˆ X →{true, false} is the invariant function, • safe : R ˆ X →{true, false} is the safety function. All the functions deﬁning an environment are total functions. The state function ξx returns the current value (in R) associated to each state variable of the system. The actuator function ξu returns the current value associated to each actuator. The uncertainty function ξw returns the uncertainty associated to each state variable. Thus, given a state variable x ∈ˆ X, ξw(x) returns the maximum distance between the real value of x and its representation in the model. Later in the paper, we will be interested in comparing the accuracy of two systems. Thus, for ξw, ξ′ w ∈R ˆ X , we will write ξw ≤ξ′ w if ξw(x) ≤ξ′ w(x), for any x ∈ˆ X. Similarly, we write ξw + ξ′ w to denote the function ξ′′ w ∈R ˆ X such that ξ′′ w(x) = ξw(x) + ξ′ w(x), for any x ∈ˆ X. Given a state function, an actuator function, and an un- certainty function, the evolution map evol returns the set of next admissible state functions. It models the evolution law of 1The presence of battery-powered devices interconnected through wireless networks prevents the en-/decryption of all packets due to energy constraints. the physical system, where changes made on actuators may reﬂect on state variables. Since we assume an uncertainty in our models, evol does not return a single state function but a set of possible state functions. evol is obviously monotone with respect to uncertainty: if ξw ≤ξ′ w then evol(ξx, ξu, ξw) ⊆ evol(ξx, ξu, ξ′ w). Both the state function and the actuator function are supposed to change during the evolution of the system, whereas the uncer- tainty function is constant. Note that, although the uncertainty function is constant, it can be used in the evolution map in an arbitrary way (e.g., it could have a heavier weight when a state variable reaches extreme values). Another possibility is to model the uncertainty function by means of a probability distribution. The sensor-error function ξe returns the maximum error associated to each sensor in ˆS. Again due to the presence of the sensor-error function, the measurement map meas, given the current state function, returns a set of admissible measurement functions rather than a single one. The invariant function inv represents the conditions that the state variables must satisfy to allow for the evolution of the system. A CPS whose state variables don’t satisfy the invariant is in deadlock. The safety function safe represents the conditions that the state variables must satisfy to consider the CPS in a safe state. Intuitively, if a CPS gets in an unsafe state, then its functionality may get compromised. In the following, we use a speciﬁc notation for the re- placement of a single component of an environment with a new one of the same kind; for instance, for E = ⟨ξx, ξu, ξw, evol, ξe, meas, inv, safe⟩, we write E[ξw ←ξ′ w] to denote ⟨ξx, ξu, ξ′ w, evol, ξe, meas, inv, safe⟩. Let us now introduce a running example to illustrate our approach. We remark that while we have kept the example simple, it is actually far from trivial and designed to describe a wide number of attacks. A more complex example (say, with n sensors and m actuators) wouldn’t have been more instructive but just made the paper more dense. Example 1 (Physical environment of the CPS Sys). Consider a CPS Sys in which the temperature of an engine is maintained within a speciﬁc range by means of a cooling system. The physical environment Env of Sys is constituted by: (i) a state variable temp containing the current temperature of the engine, and a state variable stress keeping track of the level of stress of the mechanical parts of the engine due to high temperatures (exceeding 9.9 degrees); this integer variable ranges from 0, meaning no stress, to 5, for high stress; (ii) an actuator cool to turn on/off the cooling system; (iii) a sensor st (such as a thermometer or a thermocouple) measuring the temperature of the engine; (iv) an uncertainty δ = 0.4 associated to the only variable temp; (v) the evolution law for the two state variables: the variable temp is increased (resp., is decreased) of one degree per time unit if the cooling system is inactive (resp., active), whereas the variable stress contains an integer that is increased each time the current temperature is above 3 9.9 degrees, and dropped to 0 otherwise; (vi) an error ϵ = 0.1 associated to the only sensor st; (vii) a measurement map to get the values detected by sensor st, up to its error ϵ; (viii) an invariant function saying that the system gets faulty when the temperature of the engine gets out of the range [0, 50]; (ix) a safety function to say that the system moves to an unsafe state when the level of stress reaches the threshold 5. Formally, Env = ⟨ξx, ξu, ξw, evol, ξe, meas, inv⟩with: • ξx ∈R{temp,stress} and ξx(temp) = 0 and ξx(stress)=0; • ξu ∈R{cool} and ξu(cool) = oﬀ; for the sake of simplicity, we can assume ξu to be a mapping {cool} →{on, oﬀ} such that ξu(cool) = oﬀif ξu(cool) ≥0, and ξu(cool) = on if ξu(cool) < 0; • ξw∈R{temp,stress}, ξw(temp)=0.4=δ and ξw(stress)=0; • evol(ξi x, ξi u, ξw) is the set of ξ ∈R{temp,stress} such that: – ξ(temp) = ξi x(temp) + heat(ξi u, cool) + γ, with γ ∈ [−δ, +δ] and heat(ξi u, cool) = −1 if ξi u(cool) = on (active cooling), and heat(ξi u, cool) = +1 if ξi u(cool) = oﬀ(inactive cooling); – ξ(stress) = min(5 , ξi x(stress)+1) if ξi x(temp) > 9.9; ξ(stress) = 0, otherwise; • ξe ∈R{st} and ξe(st) = 0.1 = ϵ; • meas(ξi x, ξe)=  ξ : ξ(st)∈[ξi x(temp)−ϵ , ξi x(temp)+ϵ] ; • inv(ξx) = true if 0 ≤ξx(temp) ≤50; inv(ξx) = false, otherwise. • safe(ξx) = true if ξx(stress) < 5; safe(ξx) = false, if ξx(stress) ≥5 (the maximum value for stress is 5). Let us now formalise the cyber component of CPSs in CCPSA. Basically, we extend the timed process algebra TPL of [9] with two main ingredients: • two different constructs to read values detected at sensors and write values on actuators, respectively; • special constructs to represent malicious activities on physical devices. The remaining constructs are the same as those of TPL. Deﬁnition 3 (Processes). Processes are deﬁned as follows: P, Q ::= nil idle.P P ∥Q timeout⌊π.P⌋Q if (b) {P} else {Q} P\c H⟨˜w⟩. We write nil for the terminated process. The process idle.P sleeps for one time unit and then continues as P. We write P ∥Q to denote the parallel composition of concurrent threadsP and Q. The process timeout⌊π.P⌋Q, with π ∈ {snd c⟨v⟩, rcv c(x), read s(x), write a⟨v⟩, read Ep(x), write Ep⟨v⟩}, denotes preﬁxing with timeout. Thus, timeout⌊snd c⟨v⟩.P⌋Q sends the value v on channel c and, after that, it continues as P; otherwise, after one time unit, it evolves into Q. The process timeout⌊rcv c(x).P⌋Q is the obvious counterpart for reception. The process timeout⌊read s(x).P⌋Q reads the values detected by the sensor s, whereas timeout⌊write a⟨v⟩.P⌋Q writes on the actuator a. For π ∈{read Ep(x), write Ep⟨v⟩}, the process timeout⌊π.P⌋Q denotes the reading and the writing, respectively, of the physical device p (sensor or actuator) made by the attack. Thus, in CCPSA, attack processes have speciﬁc constructs to interact with physical devices. The process P\c is the channel restriction operator of CCS. It is quantiﬁed over the set C of communication channels but we sometimes write P\{c1, c2, . . . , cn} to mean P\c1\c2 · · · \cn. The process if (b) {P} else {Q} is the standard conditional, where b is a decidable guard. In processes of the form idle.Q and timeout⌊π.P⌋Q, the occurrence of Q is said to be time- guarded. The process H⟨˜w⟩denotes (guarded) recursion. We assume a set of process identiﬁers ranged over by H, H1, H2. We write H⟨w1, . . . , wk⟩to denote a recursive process H deﬁned via an equation H(x1, . . . , xk) = P, where (i) the tuple x1, . . . , xk contains all the variables that appear free in P, and (ii) P contains only guarded occurrences of the process identiﬁers, such as H itself. We say that recursion is time-guarded if P contains only time-guarded occurrences of the process identiﬁers. Unless explicitly stated our recursive processes are always time-guarded. In the two constructs timeout⌊rcv c(x).P⌋Q and timeout⌊read µ(x).P⌋Q, with µ ∈ {p, Ep}, the variable x is said to be bound. This gives rise to the standard notions of free/bound (process) variables and α-conversion. A term is closed if it does not contain free variables, and we assume to always work with closed processes: the absence of free variables is preserved at run-time. As further notation, we write T{v/x} for the substitution of all occurrences of the the free variable x in T with the value v. Note that in CCPSA, a processes might use sensors and/or actuators which are not deﬁned in the environment. To rule out ill-formed CPSs, we use the following deﬁnition. Deﬁnition 4 (Well-formedness). Given a process P and an environment E = ⟨ξx, ξu, ξw, evol, ξe, meas, inv, safe⟩, the CPS E ⋊⋉P is well-formed if: (i) for any sensor s mentioned in P, the function ξe is deﬁned on s; (ii) for any actuator a mentioned in P, the function ξu is deﬁned on a. Hereafter, we will always work with well-formed CPSs. Finally, we adopt some notational conventions. To model time-persistent preﬁxing, we write π.P for the process deﬁned via the equation Rcv = timeout⌊π.P⌋Rcv, where Rcv does not occur in P. We write timeout⌊π⌋Q as an abbreviation for timeout⌊π.nil⌋Q, and timeout⌊π.P⌋as an abbreviation for timeout⌊π.P⌋nil. We write snd c (resp., rcv c) when channel c is used for pure synchronisation. For k ≥0, we write idlek.P as a shorthand for idle.idle. . . . idle.P, where the preﬁx idle appears k consecutive times. We write if (b) {P} instead of if (b) {P} else {nil}. Let M = E ⋊⋉S P, we write M ∥Q for E ⋊⋉S (P ∥Q), and M\c for E ⋊⋉S P\c. We can now ﬁnalise our running example. Example 2 (Cyber component of the CPS Sys). Let us deﬁne the cyber component of the CPS Sys described in Example 1. We deﬁne two parallel processes: Ctrl and IDS. The former models the controller activity, consisting in reading the temperature sensor and in governing the cooling system via its actuator, whereas the latter models a simple 4 time 0 10 20 30 40 50 actual temperature (deg) 0 2 4 6 8 10 12 Fig. 2. Three possible evolutions of the CPS of Example 2. intrusion detection system that attempts to detect and signal abnormal behaviours of the system. Intuitively, Ctrl senses the temperature of the engine at each time slot. When the sensed temperature is above 10 degrees, the controller activates the coolant. The cooling activity is maintained for 5 consecutive time units. After that time, the controller synchronises with the IDS component via a private channel sync, and then waits for instructions, via a channel ins. The IDS component checks whether the sensed temperature is still above 10. If this is the case, it sends an alarm of “high temperature”, via a speciﬁc channel, and then says to Ctrl to keep cooling for other 5 time units; otherwise, if the temperature is not above 10, the IDS component requires Ctrl to stop the cooling activity. Ctrl = read st(x).if (x > 10) {Cooling} else {idle.Ctrl} Cooling = write cool⟨on⟩.idle5.Check Check = snd sync.rcv ins(y).if (y = keep_cooling) {idle5.Check} else {write cool⟨oﬀ⟩.idle.Ctrl} IDS = rcv sync.read st(x).if (x > 10) {snd alarm⟨high_temp⟩.snd ins⟨keep_cooling⟩. idle.IDS} else {snd ins⟨stop⟩.idle.IDS} . Thus, the whole CPS is deﬁned as: Sys = Env ⋊⋉(Ctrl ∥IDS)\{sync, ins} , where Env is the physical environment deﬁned in Example 1. We remark that, for the sake of simplicity, our IDS component is quite basic: for instance, it does not check wether the temperature is too low. However, it is straightforward to replace it with a more sophisticated one, containing more informative tests on sensor values and/or on actuators commands. A. Labelled transition semantics In this section, we provide the dynamics of CCPSA in terms of a labelled transition system (LTS) in the SOS style of Plotkin. Deﬁnition 5 introduces auxiliary operators on environments. Deﬁnition 5. Let E = ⟨ξx, ξu, ξw, evol, ξe, meas, inv, safe⟩. • read_sensor(E, s) def = {ξ(s) : ξ ∈meas(ξx, ξe)}, • update_act(E, a, v) def = E[ξu ←ξu[a 7→v]], • next(E) def = S ξ∈evol(ξx,ξu,ξw){E[ξx ←ξ]}, • inv(E) def = inv(ξx), • safe(E) def = safe(ξx). The operator read_sensor(E, s) returns the set of possible measurements detected by sensor s in the environment E; it returns a set of possible values rather than a single value due to the error ξe(s) of sensor s. update_act(E, a, v) returns the new environment in which the actuator function is updated in such a manner to associate the actuator a with the value v. next(E) returns the set of the next admissible environments reachable from E, by an application of evol. inv(E) checks whether the invariant is satisﬁed by the current values of the state variables (here, abusing notation, we overload the meaning of the function inv). safe(E) checks whether the safety conditions are satisﬁed by the current values of the state variables. In Table I, we provide transition rules for processes. Here, the meta-variable λ ranges over labels in the set {idle, τ, cv, cv, a!v, s?v, Ep!v, Ep?v, τ:p}. Rules (Outp), (Inpp) and (Com) serve to model channel communication, on some channel c. Rules (Write) and (Read) denote the writing/reading of some data on the physical device p. Rule (ESensWrite E) models an integrity attack on sensor s, where the controller of s is supplied with a fake value v provided by the attack. Rule (EActRead E) models a DoS attack to the actuator a, where the update request of the controller is intercepted by the attacker and it never reaches the actuator. Rule (Par) propagates untimed actions over parallel components. Rules (Res), (Rec), (Then) and (Else) are standard. The following four rules model the passage of time. The symmetric counterparts of rules (Com) and (Par) are omitted. In Table II, we lift the transition rules from processes to systems. Except for rule (Deadlock), all rules have a common premise inv(E): a system can evolve only if the invariant is satisﬁed. Here, actions, ranged over by α, are in the set {τ, cv, cv, idle, deadlock, unsafe}. These actions denote: internal activities (τ); logical activities, more precisely, channel transmission (cv and cv); the passage of time (idle); and two speciﬁc physical events: system deadlock (deadlock) and the violation of the safety conditions (unsafe). Rules (Out) and (Inp) model transmission and reception, with an external system, on a channel c. Rule (SensReadSec) models the reading of the current data detected at a secured sensor s, whereas rule (SensReadUnsec) models the reading of an unsecured sensor s. In this case, since the sensor is not secured, the presence of a malicious action Es!w prevents the reading of the sensor. We already said that rule (ESensWrite E) of Table I models integrity attacks on an unsecured sensor s, however, together with rule (SensReadUnsec), it also serves to model DoS attacks on an unsecured sensor s, as the controller of s cannot read its correct value if the attacker is currently supplying a fake value for it. Rule (ESensRead E) allows the attacker to read the conﬁdential value detected at an unsecured sensor s. Rule (ActWriteSec) models the writing of a value v on a secured actuator a, whereas rule (ActWriteUnsec) models the writing on a unsecured actuator a. Again, if the actuator is unsecured, the presence of an attack (capable of performing an action Ea?v) prevents the correct access to the actuator by the controller. Rule (EActWrite E) models an integrity attack to an unsecured actua- 5 TABLE I LTS FOR PROCESSES (Outp) − timeout⌊snd c⟨v⟩.P⌋Q cv −−− →P (Inpp) − timeout⌊rcv c(x).P⌋Q cv −−− →P{v/x} (Com) P cv −−− →P ′ Q cv −−− →Q′ P ∥Q τ −−→P ′ ∥Q′ (Par) P λ −−→P ′ λ ̸= idle P ∥Q λ −−→P ′ ∥Q (Write) µ ∈{p, Ep} timeout⌊write µ⟨v⟩.P⌋Q µ!v −−−− →P (Read) µ ∈{p, Ep} timeout⌊read µ(x).P⌋Q µ?v −−−− →P{v/x} (ESensWrite E) P Es!v −−−−→P ′ Q s?v −−−− →Q′ P ∥Q τ:s −−−→P ′ ∥Q′ (EActRead E) P a!v −−−→P ′ Q Ea?v −−−−− →Q′ P ∥Q τ:a −−−→P ′ ∥Q′ (Res) P λ −−→P ′ λ ̸∈{cv, cv} P\c λ −−→P ′\c (Rec) P{ ˜ w/˜x} λ −−→Q H(˜x) = P H⟨˜w⟩ λ −−→Q (Then) JbK = true P λ −−→P ′ if (b) {P} else {Q} λ −−→P ′ (Else) JbK = false Q λ −−→Q′ if (b) {P} else {Q} λ −−→Q′ (TimeNil) − nil idle −−−→nil (Delay) − idle.P idle −−−→P (Timeout) − timeout⌊π.P⌋Q idle −−−→Q (TimePar) P idle −−−→P ′ Q idle −−−→Q′ P ∥Q idle −−−→P ′ ∥Q′ tor a, where the attack updates the actuator with a fake value. Note that our operational semantics ensures a preemptive power to preﬁxes of the form write Ep⟨v⟩and read Ep(x) on unsecured devices p. This because an attack process can always prevent the regular access to a unsecured physical device (sensor or actuator) by its controller. Proposition 1 (Attack preemptiveness). Let M = E ⋊⋉S P. • If there is Q such that P Es!v −−−−→Q, with s ̸∈S, then there is no M ′ such that M τ −−→M ′ by an application of the rule (SensReadUnsec). • If there is Q such that P Ea?v −−−−→Q, with a ̸∈S, then there is no M ′ such that M τ −−→M ′ by an application of the rule (ActWriteUnsec). Rule (Tau) lifts non-observable actions from processes to systems. This includes communications channels and attacks’ accesses to unsecured physical devices. A similar lifting occurs in rule (Time) for timed actions, where next(E) returns the set of possible environments for the next time slot. Thus, by an application of rule (Time) a CPS moves to the next physical state, in the next time slot. Rule (Deadlock) is introduced to signal the violation of the invariant. When the invariant is violated, a system deadlock occurs and then, in CCPSA, the system emits a special action deadlock, forever. Similarly, rule (Safety) is introduced to detect the violation of safety conditions. In this case, the system may emit a special action unsafe and then continue its evolution. Now, having deﬁned the actions that can be performed by a system, we can easily concatenate these actions to deﬁne the possible execution traces of the system. Formally, given a trace t = α1 . . . αn, we will write t −− →as an abbreviation for α1 −−− →. . . αn −−−→. In the following, we will use the function #idle(t) to get the number of occurrences of the action idle in t. The notion of trace allows us to provide a formal deﬁnition of soundness for CPSs: a CPS is said to be sound if it never deadlocks and never violates the safety conditions. Deﬁnition 6 (System soundness). Let M be a well-formed CPS. We say that M is sound if whenever M t −− →M ′, for some t, both actions deadlock and unsafe never occur in t. In our security analysis, we will focus on sound CPSs. For instance, Proposition 2 says that our running example Sys is sound and it never transmits on the channel alarm. Proposition 2. Let Sys be the CPS deﬁned in Example 2. If Sys t −− →Sys′, for some trace t=α1 . . . αn, then αi ∈{τ, idle}, for any i ∈{1, . . . , n}. Actually, we can be quite precise on the temperature reached by Sys before and after the cooling: in each of the 5 rounds of cooling, the temperature will drop of a value laying in the real interval [1−δ, 1+δ], where δ is the uncertainty. Proposition 3. Let Sys be the CPS deﬁned in Example 2. For any execution trace of Sys, we have: • when Sys turns on the cooling, the value of the state variable temp ranges over (9.9 , 11.5]; • when Sys turns off the cooling, the value of the variable temp ranges over (2.9, 8.5]. B. Behavioural semantics We recall that the observable activities in CCPSA are: time passing, system deadlock, violation of safety conditions, and 6 TABLE II LTS FOR CPSS (Out) P cv −−− →P ′ inv(E) E ⋊⋉S P cv −−− →E ⋊⋉S P ′ (Inp) P cv −−− →P ′ inv(E) E ⋊⋉S P cv −−− →E ⋊⋉S P ′ (SensReadSec) P s?v −−−→P ′ s ∈S inv(E) v ∈read_sensor(E, s) E ⋊⋉S P τ −−→E ⋊⋉S P ′ (SensReadUnsec) P s?v −−−→P ′ s ̸∈S P Es!v −−−− → ̸ inv(E) v ∈read_sensor(E, s) E ⋊⋉S P τ −−→E ⋊⋉S P ′ (ESensRead E) P Es?v −−−−→P ′ s ̸∈S inv(E) v ∈read_sensor(E, s) E ⋊⋉S P τ −−→E ⋊⋉S P ′ (ActWriteSec) P a!v −−−→P ′ a ∈S inv(E) E′ = update_act(E, a, v) E ⋊⋉S P τ −−→E′ ⋊⋉S P ′ (ActWriteUnsec) P a!v −−−→P ′ a ̸∈S P Ea?v −−−−→ ̸ inv(E) E′ = update_act(E, a, v) E ⋊⋉S P τ −−→E′ ⋊⋉S P ′ (EActWrite E) P Ea!v −−−−→P ′ a ̸∈S inv(E) E′ = update_act(E, a, v) E ⋊⋉S P τ −−→E′ ⋊⋉S P ′ (Tau) (P τ −−→P ′) ∨(P τ:p −−−→P ′ p ̸∈S) inv(E) E ⋊⋉S P τ −−→E ⋊⋉S P ′ (Deadlock) ¬ inv(E) E ⋊⋉S P deadlock −−−−−− →E ⋊⋉S P (Time) P idle −−−→P ′ E ⋊⋉S P τ −−→ ̸ inv(E) E′ ∈next(E) E ⋊⋉S P idle −−−→E′ ⋊⋉S P ′ (Safety) ¬ safe(E) inv(E) E ⋊⋉S P unsafe −−−−− →E ⋊⋉S P channel communication. Having deﬁned a labelled transition semantics, we are ready to formalise our behavioural semantics, based on execution traces. We adopt a standard notation for weak transitions: we write =⇒for ( τ −−→)∗, whereas α ==⇒means =⇒ α −−→=⇒, and ﬁnally ˆα =⇒denotes =⇒if α = τ and α =⇒otherwise. Given a trace t = α1. . .αn, we write t −− →for α1 −−− →. . . αn −−−→, and ˆt == ⇒as an abbreviation for c α1 === ⇒. . . c αn ===⇒. Deﬁnition 7 (Trace preorder). We write M ⊑N if whenever M t −− →M ′, for some t, there is N ′ such that N ˆt == ⇒N ′. Remark 1. Unlike standard trace semantics, our trace preorder is able to observe deadlock thanks to the presence of the rule (Deadlock) and the special action deadlock: if M ⊑N and M eventually deadlocks then also N must eventually deadlock. As we are interested in examining timing aspects of attacks, such as beginning and duration, we propose a timed variant of ⊑up to (a possibly inﬁnite) time interval. Intuitively, we write M ⊑m..n N if the CPS N simulates the execution traces of M, except for the time interval m..n. Deﬁnition 8 (Trace preorder up to a time interval). We write M ⊑m..n N, for m ∈N+ and n ∈N+ ∪∞, with m ≤n, if the following conditions hold: • m is the minimum integer for which there is a trace t, with #idle(t) = m −1, such that M t −− →and N ̸ ˆt == ⇒; • n is the inﬁmum element of N+ ∪∞, n ≥m, such that whenever M t1 −−→M ′, with #idle(t1) = n −1, there is t2, with #idle(t1) = #idle(t2), such that N t2 −−→N ′, for some N ′, and M ′ ⊑N ′. In the second item, note that inf(∅) = ∞. Thus, if M ⊑m..∞ N then N simulates M only in the ﬁrst m −1 time slots. Finally, note that we could have equipped CCPSA with a (bi)simulation-based behavioural semantics rather than a trace-based one, as done in [2] for a core of CCPSA with no security features; however, our trace semantics is simpler than (bi)simulation and it is sensitive to deadlocks of CPSs. Thus, it is fully adequate for the purposes of this paper. III. CYBER-PHYSICAL ATTACKS In this section, we use CCPSA to formalise a threat model where attacks can manipulate sensor and/or actuator signals in order to drive a sound CPS into an undesired state [19]. An attack may have different levels of access to physical devices depending on the model assumed. For example, it might be able to get read access to the sensors but not write access; or it might get write-only access to the actuators but not read-access. This level of granularity is very important to model precisely how attacks can affect a CPS [4]. For simplicity, in this paper 7 we don’t represent attacks on communication channels as our focus is on attacks to physical devices. The syntax of our cyber-physical attack is a slight restriction of that for processes: in terms of the form timeout⌊π.P⌋Q, we require π ∈{write Ep⟨v⟩, read Ep(x)}. Thus, we provide a syntactic way to distinguish attacks from genuine processes. Deﬁnition 9 (Honest system). A CPS E ⋊⋉S P is honest if P is honest, where P is honest if it does not contain preﬁxes of the form write Ep⟨v⟩or read Ep(x). We group cyber-physical attacks in classes that describe both the malicious activity and the timing aspects of the attack. Thus, let I be a set of malicious activities on a number of physical devices, m ∈N+ be the time slot when an attack starts, and n ∈N+ ∪∞be the time slot when the attack ends, we say that an attack A is of class C ∈[I →P(m..n)] if: (1) all possible malicious actions of A coincide with those contained in I, (2) the ﬁrst of those actions may occur in the m-th time slot (i.e., after m−1 idle-actions), and (3) the last of those actions may occur in the n-th time slot (i.e., after n−1 idle-actions). Actually, for ι ∈I, C(ι) returns a (possibly empty) set of time instants when the attack tamper with the device ι; this set is contained in m..n. A class C is always a total function. Deﬁnition 10 (Class of attacks). Let I = {Ep? : p ∈S ∪A}∪ {Ep! : p ∈S ∪A} be the set of all possible malicious activities on physical devices. Let m ∈N+, n ∈N+ ∪∞, with m ≤n. An attack A is of class C ∈[I →P(m..n)] whenever: • C(ι) = {k : A t −− → ιv −−→A′ ∧k = #idle(t) + 1}, for ι ∈I; • m = inf{ k : k ∈C(ι) ∧ι ∈I }; • n = sup{ k : k ∈C(ι) ∧ι ∈I }. According to the approach proposed in [6], we can say that an attack A affects a sound CPS M if the execution of the composed system M ∥A differs from that of the original system M, in an observable manner. Basically, a cyber-physical attack can inﬂuence the system under attack in at least two different ways: • The system M ∥A might deadlock when M may not; this means that the attack A affects the availability of the system. We recall that in the context of CPSs, deadlock is a particular severe physical event. • The system M ∥A might have non-genuine execution traces containing observables (violations of safety con- ditions or communications on channels) that cannot be reproduced by M; here the attack affects the integrity of the system behaviour. Deﬁnition 11 (Attack tolerance/vulnerability). Let M be an honest and sound CPS. We say that M is tolerant to an attack A if M ∥A ⊑M. We say M is vulnerable to an attack A if there is a time interval m..n, with m ∈N+ and n ∈N+ ∪∞, such that M ∥A ⊑m..n M. Thus, if a system M is vulnerable to an attack A of class C ∈[I →P(m..n)], during the time interval m′..n′, then the attack operates during the interval m..n but it inﬂuences the system under attack in the time interval m′..n′ (obviously, m′ ≥m). If n′ is ﬁnite we have a temporary attack, otherwise we have a permanent attack. Furthermore, if m′ −n is big enough and n −m is small, then we have a quick nasty attack that affects the system late enough to allow attack camouﬂages [8]. On the other hand, if m′ is signiﬁcantly smaller than n, then the attack affects the observable behaviour of the system well before its termination and the CPS has good chances of undertaking countermeasures to stop the attack. Finally, if M ∥A t −− → deadlock −−−−−−− →, for some trace t, the attack A is called lethal, as it is capable to halt (deadlock) the CPS M. This is obviously a permanent attack. Note that, according to Deﬁnition 11, the tolerance (or vulnerability) of a CPS also depends on the capability of the IDS component to detect and signal undesired physical behaviours. In fact, the IDS component might be designed to detect abnormal physical behaviours going well further than deadlocks and violations of safety conditions. In the following, we say that an attack is stealthy if it is able to drive the CPS under attack into an incorrect physical state (either deadlock or violation of the safety conditions) without being noticed by the IDS component. In the rest of this section, we present a number of different attacks to the CPS Sys described in Example 2. Example 3. Consider the following DoS/Integrity attack on the (controller of) the actuator cool, of class C ∈[I →P(m..m)] with C(Ecool?) = C(Ecool!) = {m} and C(ι) = ∅, for ι ̸∈ {Ecool?, Ecool!}; we call the attack Am: idlem−1.timeout⌊read Ecool(x).if(x=oﬀ){write Ecool⟨oﬀ⟩}⌋. Here, the attack Am operates exclusively in the m-th time slot, when it tries to steal the cooling command (on or off) coming from the controller, and fabricates a fake command to turn off the cooling system. In practice, if the controller sends a command to turn off the coolant, nothing bad will happen as the attack will put the same message back. When the controller sends (in the m-th time slot) a command to turn the cooling on, the attack will drop the command. We recall that the controller will turn on the cooling only if the sensed temperature is greater than 10 (and hence temp > 9.9); this may happen only if m > 8. Since the command to turn the cooling on is never re-sent by Ctrl, the temperature will continue to rise, and after 4 time units the system will violate the safety conditions emitting an action unsafe, while the IDS component will start sending alarms every 5 time units, until the whole system deadlocks because the temperature reaches the threshold of 50 degrees. Proposition 4. Let Sys be the CPS deﬁned in Example 2, and Am be the attack deﬁned in Example 3. Then, • Sys ∥Am ⊑Sys, for m ≤8, • Sys ∥Am ⊑m+4..∞Sys, for m > 8. In this case, the IDS component of Sys is effective enough to detect the attack with only one time unit delay. 8 Example 4. Consider the following DoS/Integrity attack to the (controller of) sensor st, of class C ∈[I →P(2..∞)] such that C(Est?) = {2}, C(Est!) = 2..∞and C(ι) = ∅, for ι ̸∈{Est!, Est?}: A = idle.timeout⌊read Est(x).B⟨x⟩⌋ B(y) = timeout⌊write Est⟨y⟩.idle.B⟨y⟩⌋B⟨y⟩ Here, the attack A does the following actions in sequence: (i) she sleeps for one time unit, (ii) in the following time slot, she reads the current temperature v at sensor st, and (iii) for the rest of her life, she keeps sending the same temperature v to the controller of st. In the presence of this attack, the process Ctrl never activates the Cooling component (and, hence, nor the IDS component, which is the only one which could send an alarm) as it will always detect a temperature below 10. Thus, the compound system Sys ∥A will move to an unsafe state until the invariant will be violated and the system will deadlock. Indeed, in the worst scenario, after ⌈9.9 1+δ⌉= ⌈9.9 1.4⌉= 8 idle- actions (in the 9-th time slot) the value of temp will be above 9.9, and after further 5 idle-actions (in the 14-th time slot) the system will violate the safety conditions emitting an unsafe action. After = ⌈50 1.4⌉= 36 idle-actions, in the 37-th time slot, the invariant may be broken because the state variable temp may reach 50.4 degrees, and the system will also emit a deadlock action. Thus, Sys ∥A ⊑14..∞Sys. This is a lethal attack, as it causes a shut down of the system. It is also a stealthy attack as it remains unnoticed until the end. In this attack, the IDS component is completely ineffective as the sensor used by the component is compromised, and there is not way for the IDS to understand whether the sensor is under attack. A more sophisticated IDS might have a representation of the plant to recognise abnormal evolutions of the sensed temperature. In such case, the IDS might switch on a second sensor, hoping that this one has not been compromised yet. Another possibility for the designer of the CPS is to secure the sensor. Although this is not always possible, as encryption/decryption of all packets depends on energy constraints of the device. Our semantics ensures that secured devices cannot be attacked, as stated by the following proposition. Proposition 5. Let M = E ⋊⋉S P be an honest and sound CPS. Let C ∈[I →P(m..n)], with {p : C(Ep?)∪C(Ep!) ̸= ∅} ⊆S. Then M ∥A ⊑M, for any attack A of class C. Now, let us examine a similar but less severe attack. Example 5. Consider the following DoS/Integrity attack to the controller of sensor st, of class C ∈[I →P(1..n)], for n > 0, with C(Est!) = C(Est?) = 1..n and C(ι) = ∅, for ι ̸∈{Est!, Est?}: An = timeout⌊read Est(x).timeout⌊{write Est⟨x−2⟩. idle.An−1⌋An−1⌋An−1 with A0 = nil. In this attack, for n consecutive time slots, An sends to the controller the current sensed temperature decreased by an offset 2. The effect of this attack on the system depends on the duration n of the attack itself: • for n ≤8, the attack is harmless as the variable temp may not reach a (critical) temperature above 9.9; • for n = 9, the variable temp might reach a temperature above 9.9 in the 9-th time slot, and the attack would delay the activation of the cooling system of one time slot; as a consequence, the system might get into an unsafe state in the time slots 14 and 15, but no alarm will be ﬁred. • for n ≥10, the system may get into an unsafe state in the time slot 14 and in the following n −7 time slots; this is not a stealthy attack as the IDS will ﬁre the alarm at most two time slots later (in the 16-th time slot); this is a temporary attack which ends in the time slot n + 7. Proposition 6. Let Sys be the CPS deﬁned in Example 2, and An be the attack deﬁned in Example 5. Then: • Sys ∥An ⊑Sys, for n ≤8, • Sys ∥An ⊑14..15 Sys, for n = 9, • Sys ∥An ⊑14..n+7 Sys, for n ≥10. A. A technique for proving attack tolerance/vulnerability In this subsection, we provide sufﬁcient criteria to prove attack tolerance/vulnerability to attacks of an arbitrary class C. Actually, we do more than that, we provide sufﬁcient criteria to prove attack tolerance/vulnerability to all attacks of a class C′ which is somehow “weaker” than a given class C. Deﬁnition 12. Let C1, C2 ∈[I →P(m..n)] be two classes of attacks. We say that C1 is weaker than C2, written C1 ⪯C2, if C1(ι) ⊆C2(ι), for any ι ∈I. The idea is to deﬁne a notion of most powerful attack (also called top attacker) of a given class C, such that, if a CPS M tolerates the most powerful attack of class C then it also tolerates any attack of class C′, with C′ ⪯C. We will provide a similar condition for attack vulnerability: let M be a CPS vulnerable to Top(C) in the time interval m1..n1; then, for any attack A of class C′, with C′ ⪯C, if M is vulnerable to A then it is so for a smaller time interval m2..n2 ⊆m1..n1. Our notion of top attacker has two extra ingredients with respect to the cyber-physical attacks seen up to now: (i) nondeterminism, and (ii) time-unguarded recursive processes. This extra power of the top attacker is not a problem as we are looking for sufﬁcient criteria. For what concerns nondeterminism, we assume a generic pro- cedure rnd() that given an arbitrary set Z returns an element of Z chosen in a nondeterministic manner. This procedure allows us to express nondeterministic choice, P ⊕Q, as an abbreviation for the process if (rnd({true, false})) {P} else {Q}. Thus, let ι ∈{Ep? : p ∈S ∪A} ∪{Ep! : p ∈S ∪A}, m ∈N+, n ∈N+ ∪∞, with m ≤n, and T ⊆m..n, we deﬁne the attack process Att(ι, k, T ) as the attack which may achieve 9 the malicious activity ι, at the time slot k, and which tries to do the same in all subsequent time slots of T . Formally, Att(Ep?, k, T ) = if (k ∈T ) {(timeout⌊read Ep?(x).Att(Ep?, k, T )⌋ Att(Ep?, k+1, T )) ⊕idle.Att(Ep?, k+1, T )} else if (k < sup(T )) {idle.Att(Ep?, k+1, T )} else {nil} Att(Ep!, k, T ) = if (k ∈T ) {(timeout⌊write Ep!⟨rnd(R)⟩.Att(Ep!, k, T )⌋ Att(Ep!, k+1, T )) ⊕idle.Att(Ep!, k+1, T )} else if (k < sup(T )) {idle.Att(Ep!, k+1, T )} else {nil} . Note that for T = ∅we assume sup(T ) = −∞. We can now use the deﬁnition above to formalise the notion of most powerful attack of a given class C. Deﬁnition 13 (Top attacker). Let C ∈[I →P(m..n)] be a class of attacks. We deﬁne Top(C) = Q ι∈I Att(ι, 1, C(ι)) as the most powerful attack, or top attacker, of class C. The following result provides soundness criteria for attack tolerance and attack vulnerability. Theorem 1 (Soundness criteria). Let M be an honest and sound CPS, C an arbitrary class of attacks, and A an attack of a class C′, with C′ ⪯C. • If M ∥Top(C) ⊑M then M ∥A ⊑M. • If M ∥Top(C) ⊑m1..n1 M then either M ∥A ⊑M or M ∥A ⊑m2..n2 M, with m2..n2 ⊆m1..n1. Corollary 1. Let M be an honest and sound CPS, and C a class of attacks. If Top(C) is not lethal for M then any attack A of class C′, with C′ ⪯C, is not a lethal attack for M. If Top(C) is not a permanent attack for M then any attack A of class C′, with C′ ⪯C, is not a permanent attack for M. IV. IMPACT OF AN ATTACK In the previous section, we have grouped cyber-physical attacks by focussing on the physical devices under attack and the timing aspects of the attack (Deﬁnition 10). Then, we have provided a formalisation of when a CPS should be considered tolerant/vulnerable to an attack (Deﬁnition 11). In this section, we show that it is important not only to demonstrate the tolerance (or vulnerability) of a CPS with respect to certain attacks, but also to evaluate the disruptive impact of those attacks on the CPS [7]. The goal of this section is twofold: to provide a metric to estimate the impact of a successful attack on a CPS, and to investigate possible quantiﬁcations of the chances for an attack of being successful when attacking a CPS. As to the metric, we focus on the ability that an attack may have to drag a CPS out of the correct behaviour modelled by its evolution map, with the given uncertainty. Recall that evol is monotone with respect to the uncertainty. Thus, an increase of the uncertainty may translate into a widening of the range of the possible behaviours of the CPS. Fig. 3. Simulation of Sys[δ ←19 20 ]. In the following, for M = E ⋊⋉S P, we write M[ψ ←ψ′] to mean E[ψ ←ψ′] ⋊⋉S P. Proposition 7 (Monotonicity). Let M = E ⋊⋉S P be an honest and sound CPS, and ξw the uncertainty of E. If ξw ≤ξ′ w and M t −− →M ′ then M[ξw ←ξ′ w] t −− →M ′[ξw ←ξ′ w]. However, a wider uncertainty in the model doesn’t always correspond to a widening of the possible behaviours of the CPS. In fact, this depends on the intrinsic tolerance of a CPS with respect to changes in the uncertainty function. Deﬁnition 14 (System ξ-tolerance). An honest and sound CPS M = E ⋊⋉S P, where ξw is the uncertainty of E, is ξ-tolerant, for ξ ∈R ˆ X and ξ ≥0, if ξ = sup  ξ′ : M[ξw ←ξw+η] ⊑M, for any 0 ≤η ≤ξ′ . Intuitively, if a CPS M has been designed with a given uncertainty ξw, but M is actually ξ-tolerant, with ξ > 0, then the uncertainty ξw is somehow underestimated: the real uncertainty of M is given by ξw+ξ. This information is quite important when trying to estimate the impact of an attack on a CPS. In fact, if a system M has been designed with a given uncertainty ξw, but M is actually ξ-tolerant, with ξ > 0, then an attack has (at least) a “room for maneuver” ξ to degrade the whole CPS without being observed (and hence detected). Thus, in general, the tolerance ξ should be as small as possible. Let Sys be the CPS of Example 2. In the rest of the section, with an abuse of notation, we will write Sys[δ ←γ] to denote Sys where the uncertainty of the variable temp is γ. Example 6. The CPS Sys of Example 2 is 1 20-tolerant. This because, sup  ξ′ : Sys[δ ←δ+η] ⊑Sys, for 0 ≤η ≤ξ′ is equal to 1 20. Since δ + ξ = 8 20 + 1 20 = 9 20, the proof of this statement relies on the following proposition. Proposition 8. Let Sys be the CPS of Example 2. Then: • Sys[δ ←γ] ⊑Sys, for γ ∈( 8 20, 9 20), • Sys[δ ←γ] ̸⊑Sys, for γ > 9 20. Figure 3 shows an evolution of Sys[δ ←29 30]: the red box denotes a violation of the safety conditions because the cooling cycle wasn’t sufﬁcient to drop the (sensed) temperature below 10 (here, the controller imposes 5 further time units of cooling). 10 Everything is in place to deﬁne our metric to estimate the impact of an attack. Deﬁnition 15 (Impact). Let M = E ⋊⋉S P be an honest and sound CPS, where ξw is the uncertainty of E. We say that an attack A has deﬁnitive impact ξ on the system M if ξ = inf  ξ′ : ξ′ ∈R ˆ X ∧ξ′>0 ∧M ∥A ⊑M[ξw ←ξw+ξ′] . It has pointwise impact ξ on the system M at time m if ξ= inf  ξ′ : ξ′∈R ˆ X ∧M ∥A ⊑m..n M[ξw ←ξw+ξ′], n∈N∪∞ . Intuitively, with this deﬁnition, we can establish either the deﬁnitive (and hence maximum) impact of the attack A on the system M, or the impact at a speciﬁc time m. In the latter case, by deﬁnition of ⊑m..n, there are two possibilities: either the impact of the attack keeps growing after time m, or in the time interval m+1, the system under attack deadlocks. The impact of Top(C) provides an upper bound for the impact of all attacks of class C′, with C′ ⪯C. Theorem 2 (Top attacker’s impact). Let M be an honest and sound CPS, and C an arbitrary class of attacks. Let A be an attack of class C′, with C′ ⪯C. • The deﬁnitive impact of Top(C) on M is greater than or equal to the deﬁnitive impact of A on M. • If Top(C) has pointwise impact ξ on M at time m, and A has pointwise impact ξ′ on M at time m′, with m′ ≤m, then ξ′ ≤ξ. Example 7. Let us consider the attack A of Example 4. Then, A has a deﬁnitive impact of 8.5 on the CPS Sys deﬁned in Example 2. Formally, 8.5 = inf  ξ′ : ξ′ > 0 ∧Sys ∥A ⊑ Sys[δ ←δ+ξ′] . Here, the attack can prevent the activation of the cooling system, and the temperature will keep growing until the CPS before enters continuously in an unsafe state and eventually deadlocks. Since δ + ξ = 0.4 + 8.5 = 8.9, the proof of this statement relies on the following proposition. Proposition 9. Let Sys be the CPS deﬁned in Example 2, and A be the attack deﬁned in Example 7. Then: • Sys ∥A ̸⊑Sys[δ ←γ], for γ ∈(0.4, 8.9), • Sys ∥A ⊑Sys[δ ←γ], for γ > 8.9. Deﬁnition 15 provided an instrument to estimate the impact of a successful attack. However, there is at least another question that a CPS designer could ask: “Is there a way to estimate the chances that an attack will be successful during the execution of my CPS?” To paraphrase in a more operational manner: how many execution traces of my CPS are prone to be attacked by a speciﬁc attack? For instance, consider again the simple attack Am proposed in Example 3: idlem−1.timeout⌊read Ecool(x).if (x=oﬀ) {write Ecool⟨oﬀ⟩}⌋. Here, in the m-th time slot the attack tries to eavesdrop a command to turn on the cooling. The attack is very quick and condensed in a single time slot. The question is: what are the chances of success of such a quick attack? Fig. 4. A quantitative analysis of the attack of Example 3. Figure 4 provides a representation of an experiment in MATLAB where we launched 10000 executions of our CPS in isolation, lasting 700 time units each. From the aggregated data contained in this graphic, we note that after a transitory phase (whose length depends on several things: the uncertainty δ, the initial state of the system, the length of the cooling activity, etc.) that lasts around 300 time slots, the rate of success of the attack Am is around 10%. The reader may wonder why exactly the 10%. This depends on the periodicity of our CPS, as in average the cooling is activated every 10 time slots. This example shows that, as pointed out in [8], the effec- tiveness of a cyber-physical attack depends on the information the attack has about the functionality of the whole CPS. For instance, if the attacker were not aware of the exact periodicity of the CPS, she might try, if possible, to repeat the attack on more consecutive time slots. In this case, the left graphic of Figure 5 says that the rate of success of the attack increases linearly with the length of the attack itself (data obtained by attacking the CPS after the transitory period). Thus, if the attack of Example 3 were iterated for 10 time slots, say A10 m = idlem−1.B10 Bi = timeout⌊read Ecool(x).if (x = oﬀ) {write Ecool⟨oﬀ⟩.idle.Bi−1}⌋Bi−1, for 1 ≤i ≤10 with B0 = nil, the rate of success would be almost 100%. Finally, consider a generalisation of the attack of Example 5: Ak 0 = nil Ak n = timeout⌊read Est(x).timeout⌊write Est⟨x−k⟩. idle.Ak n−1⌋Ak n−1⌋Ak n−1 for 1 ≤n ≤15 and 2 ≤k ≤10. Here, the attack decreases the sensed temperature of an offset k. Now, suppose to launch this attack after, say, 300 time slots (i.e., after the transitory phase). Formally, we deﬁne the attack: Bk n = idle300.Ak n. In this case, the right graphic of Figure 5 provides a graphical representation of the percentage of alarms on 5000 execution traces lasting 100 time units each. Thus, for instance, an attack lasting n = 8 time units with an offset k = 5 affects around 40% of the execution traces of the CPS. V. CONCLUSIONS, RELATED AND FUTURE WORK We have provided formal theoretical foundations to reason about and statically detect attacks to physical devices of CPSs. 11 Fig. 5. A quantitative analysis of two different attacks. To that end, we have proposed a hybrid process calculus, called CCPSA, as a formal speciﬁcation language to model physical and cyber components of CPSs as well as cyber-physical attacks. Based on CCPSA and its labelled transition semantics, we have formalised a threat model for CPSs by grouping attacks in classes, according to the target physical devices and two timing parameters: begin and duration of the attacks. Then, we relied on the trace semantics of CCPSA to assess attack tolerance/vulnerability with respect to a given attack. Along the lines of GNDC [6], we deﬁned a notion of top attacker, Top(C), of a given class of attacks C, which has been used to provide sufﬁcient criteria to prove attack tolerance/vulnerability to all attacks of class C (and weaker ones). Here, would like to mention that in our companion paper [2] we developed a bisimulation congruence for a simpler version of the calculus where security features have been completely stripped off. For simplicity, in the current submission, we adopted as main behavioural equivalence trace equivalence instead of bisimulation. We could switch to a bisimulation semantics, preserved by parallel composition, which would allow us to scale our veriﬁcation method (Theorem 1) to bigger systems. Finally, we have provided a metric to estimate the impact of a successful attack on a CPS together with possible quantiﬁcations of the success chances of an attack. We proved that the impact of the most powerful attack Top(C) represents an upper bound for the impact of any attack A of class C (and weaker ones). We have illustrated our concepts by means of a running example, focusing in particular on a formal treatment of both integrity and DoS attacks to sensors and actuators of CPSs. Our example is simple but far from trivial and designed to describe a wide number of attacks. Related work: Among the 118 papers discussed in the comprehensive survey [24], 50 adopt a discrete notion of time similar to ours, 13 a continuous one, 48 a quasi-static time model, and the rest use a hybrid time model. Most of these papers investigate attacks on CPSs and their protection by relying on simulation test systems to validate the results. We focus on the papers that are most related to our work. Huang et al. [10] were among the ﬁrst to propose threat models for CPSs. Along with [11], [12], they stressed the role played by timing parameters on integrity and DoS attacks. Alternative threat models are discussed in [7], [8], [19]. In particular, Gollmann et al. [8] discussed possible goals (equipment damage, production damage, compliance violation) and stages (access, discovery, control, damage, cleanup) of cyber-physical attacks. In the current paper, we focused on the damage stage, where the attacker already has a rough idea of the plant and the control architecture of the target CPS. A number of works use formal methods for CPS security, although they apply methods, and most of the time have goals, that are quite different from ours. Burmester et al. [3] employed hybrid timed automata to give a threat framework based on the traditional Byzantine faults model for crypto-security. However, as remarked in [19], cyber-physical attacks and faults have inherently distinct characteristics. Faults are considered as physical events that affect the system behaviour, where simultaneous events don’t act in a coordinated way; cyber-attacks may be performed over a signiﬁcant number of attack points and in a coordinated way. In [21], Vigo presented an attack scenario that addresses some of the peculiarities of a cyber-physical adversary, and discussed how this scenario relates to other attack models popular in the security protocol literature. Then, in [22], [23] Vigo et al. proposed an untimed calculus of broadcasting processes equipped with notions of failed and unwanted communication. These works differ quite considerably from ours, e.g., they focus on DoS attacks without taking into consideration timing aspects or impact of the attack. Cómbita et al. [4] and Zhu and Basar [25] applied game theory to capture the conﬂict of goals between an attacker who seeks to maximise the damage inﬂicted to a CPS’s security and a defender who aims to minimise it [13]. Finally, there are three recent papers that were developed in parallel to ours: [14], [16], [17]. Rocchetto and Tippenhaur [16] introduced a taxonomy of the diverse attacker models proposed for CPS security and outline requirements for generalised attacker models; in [17], they then proposed an extended Dolev-Yao attacker model suitable for CPSs. In their approach, physical layer interactions are modelled as abstract interactions between logical components to support reasoning on the physical-layer security of CPSs. This is done by introducing 12 additional orthogonal channels. Time is not represented. Nigam et al. [14] work around the notion of Timed Dolev- Yao Intruder Models for Cyber-Physical Security Protocols by bounding the number of intruders required for the automated veriﬁcation of such protocols. Following a tradition in security protocol analysis, they provide an answer to the question: How many intruders are enough for veriﬁcation and where should they be placed? They also extend the strand space model to CPS protocols by allowing for the symbolic representation of time, so that they can use the tool Maude [15] along with SMT support. Their notion of time is however different from ours, as they focus on the time a message needs to travel from an agent to another. The paper does not mention physical devices, such as sensors and/or actuators. Future work: While much is still to be done, we believe that our paper provides a stepping stone for the development of formal and automated tools to analyse the security of CPSs. We will consider applying, possibly after proper enhancements, existing tools and frameworks for automated security protocol analysis, resorting to the development of a dedicated tool if existing ones prove not up to the task. We will also consider further security properties and concrete examples of CPSs, as well as other kinds of cyber-physical attackers and attacks, e.g., periodic attacks. This will allow us to reﬁne the classes of attacks we have given here (e.g., by formalising a type system amenable to static analysis), and provide a formal deﬁnition of when a CPS is more secure than another so as to be able to design, by progressive reﬁnement, secure variants of a vulnerable CPSs. We also aim to extend the preliminary quantitative analysis we have given here by developing a suitable behavioural theory ensuring that our trace semantics considers also the probability of a trace to actually occur. We expect that the discrete time stochastic hybrid systems of [1] will be useful to that extent. REFERENCES [1] A. Abate, S. Amin, M. Prandini, J. Lygeros, and S. Sastry. Probabilistic reachability and safe sets computation for discrete time stochastic hybrid systems. In CDC, pages 258–263. IEEE, 2006. [2] Anonymous (anonymised in agreement with the PC chairs). A Calculus of Cyber-Physical Systems. In LATA 2017. Springer, 2017 (to appear). [3] M. Burmester, E. Magkos, and V. Chrissikopoulos. Modeling security in cyber-physical systems. IJCIP, 5(3-4):118–126, 2012. [4] L. F. Cómbita, J. Giraldo, A. A. Cárdenas, and N. Quijano. Response and reconﬁguration of cyber-physical control systems: A survey. In Automatic Control 2015, pages 1–6. IEEE, 2015. [5] N. Falliere, L. Murchu, and E. Chien. W32.Stuxnet Dossier, 2011. [6] R. Focardi and F. Martinelli. A uniform approach for the deﬁnition of security properties. In Symposium on Formal Methods, volume I, pages 794–813, 1999. [7] B. Genge, I. Kiss, and P. Haller. A system dynamics approach for assessing the impact of cyber attacks on critical infrastructures. Int. J. Critical Infrastructure Protection, 10:3–17, 2015. [8] D. Gollmann, P. Gurikov, A. Isakov, M. Krotoﬁl, J. Larsen, and A. Winnicki. Cyber-Physical Systems Security: Experimental Analysis of a Vinyl Acetate Monomer Plant. In CPSS, pages 1–12. ACM, 2015. [9] M. Hennessy and T. Regan. A process algebra for timed systems. I&C, 117(2):221–239, 1995. [10] Y. Huang, A. A. Cárdenas, S. Amin, Z. Lin, H. Tsai, and S. Sastry. Understanding the physical and economic consequences of attacks on control systems. IJCIP, 2(3):73–83, 2009. [11] M. Krotoﬁl and A. A. Cárdenas. Resilience of Process Control Systems to Cyber-Physical Attacks. In NordSec, LNCS 8208, pages 166–182. Springer, 2013. [12] M. Krotoﬁl, A. A. Cárdenas, J. Larsen, and D. Gollmann. Vulnerabilities of cyber-physical systems to stale data - Determining the optimal time to launch attacks. Int. J. Critical Infrastructure Protection, 7(4):213–232, 2014. [13] M. Manshaei, Q. Zhu, T. Alpcan, T. Basar, and J.-P. Hubaux. Game theory meets network security and privacy. ACM Comput. Surv., 45(3):25, 2013. [14] V. Nigam, C. Talcott, and A. A. Urquiza. Towards the Automated Veriﬁcation of Cyber-Physical Security Protocols: Bounding the Number of Timed Intruders. In Esorics, LNCS 9878-9. Springer, 2016. [15] P. C. Ölveczky and J. Meseguer. Semantics and pragmatics of real-time maude. Higher-Order and Symbolic Computation, 20(1-2):161–196, 2007. [16] M. Rocchetto and N. O. Tippenhauer. On Attacker Models and Proﬁles for Cyber-Physical Systems. In Esorics, LNCS 9878-9. Springer, 2016. [17] M. Rocchetto and N. O. Tippenhauer. CPDY: Extending the Dolev-Yao Attacker with Physical-Layer Interactions. In ICFEM 2016, to appear. [18] J. Slay and M. Miller. Lessons Learned from the Maroochy Water Breach. In Critical Infrastructure Protection, IFIP 253, pages 73–82. Springer, 2007. [19] A. Teixeira, I. Shames, J. Sandberg, and K. H. Johansson. A secure control framework for resource-limited adversaries. Automatica, 51:135– 148, 2015. [20] U.S. Chemical Safety and Hazard Investigation Board, T2 Laboratories Inc. Reactive Chemical Explosion: Final Investigation Report. Report No. 2008-3-I-FL, 2009. [21] R. Vigo. The Cyber-Physical Attacker. In SAFECOMP, LNCS 7613, pages 347–356. Springer, 2012. [22] R. Vigo. Availability by Design: A Complementary Approach to Denial- of-Service. PhD thesis, DTU Compute, Danish Technical University, 2015. [23] R. Vigo, F. Nielson, and H. Riis Nielson. Broadcast, denial-of-service, and secure communication. In IFM, LNCS 7940, pages 412–427. Springer, 2013. [24] Y. Zacchia Lun, A. D’Innocenzo, I. Malavolta, and M. D. Di Benedetto. Cyber-Physical Systems Security: a Systematic Mapping Study. CoRR, abs/1605.09641, 2016. [25] Q. Zhu and T. Basar. Game-theoretic methods for robustness, security, and resilience of cyberphysical control systems: games-in-games principle for optimal cross-layer resilient control systems. IEEE Control Systems, 35(1):46–65, 2015. APPENDIX A. Proof of § II In order to prove Proposition 2 and Proposition 3, we use the following lemma that formalises the invariant properties binding the state variable temp with the activity of the cooling system. Intuitively, when the cooling system is inactive the value of the state variable temp lays in the real interval [0, 11.5]. Furthermore, if the coolant is not active and the variable temp lays in the real interval (10.1, 11.5], then the cooling will be turned on in the next time slot. Finally, when active the cooling system will remain so for k ∈1..5 time slots (counting also the current time slot) with the variable temp being in the real interval (9.9 −k∗(1+δ), 11.5 −k∗(1−δ)]. Lemma 1. Let Sys be the system deﬁned in Example 2. Let Sys = Sys1 t1 −−→ idle −−−→Sys2 t2 −−→ idle −−−→· · · tn−1 −−−−− → idle −−−→Sysn such that the traces tj contain no idle-actions, for any j ∈ 1..n−1, and for any i ∈1..n, Sysi = Ei ⋊⋉Pi with Ei = ⟨ξi x, ξi u, δ, evol, ϵ, meas, inv⟩. Then, for any i ∈1..n−1, we have the following: 13 1) if ξi u(cool) = oﬀthen ξi x(temp) ∈ [0, 11.1 + δ]; with ξi x(stress) = 0 if ξi x(temp) ∈[0, 10.9 + δ], and ξi x(stress) = 1, otherwise; 2) if ξi u(cool) = oﬀand ξi x(temp) ∈(10.1, 11.1 + δ] then, in the next time slot, ξi+1 u (cool) = on and ξi+1 x (stress) ∈ 1..2; 3) if ξi u(cool) = on then ξi x(temp) ∈(9.9−k∗(1+δ), 11.1+ δ −k∗(1−δ)], for some k ∈1..5 such that ξi−k u (cool) = oﬀand ξi−j u (cool) = on, for j ∈0..k−1; moreover, if k ∈ 1..3 then ξi x(stress) ∈1..k+1, otherwise, ξi x(stress) = 0. Proof. Let us write vi and si to denote the values of the state variables temp and stress, respectively, in the systems Sysi, i.e., ξi x(temp) = vi and ξi x(stress) = si. Moreover, we will say that the coolant is active (resp., is not active) in Sysi if ξi u(cool) = on (resp., ξi u(cool) = oﬀ). The proof is by mathematical induction on n, i.e., the number of idle-actions of our traces. The case base n = 1 follows directly from the deﬁnition of Sys. Let us prove the inductive case. We assume that the three statements hold for n −1 and prove that they also hold for n. 1) Let us assume that the cooling is not active in Sysn. In this case, we prove that vn ∈[0, 11.1 + δ], with and sn = 0 if vn ∈[0, 10.9 + δ], and sn = 1 otherwise. We consider separately the cases in which the coolant is active or not in Sysn−1 • Suppose the coolant is not active in Sysn−1 (and not active in Sysn). By the induction hypothesis we have vn−1 ∈[0, 11.1 + δ]; with sn−1 = 0 if vn−1 ∈[0, 10.9+δ], and sn−1 = 1 otherwise. Furthermore, if vn−1 ∈(10.1, 11.1+δ], then, by the induction hypothesis, the coolant must be active in Sysn. Since we know that in Sysn the cooling is not active, it follows that vn−1 ∈[0, 10.1] and sn = 0. Furthermore, in Sysn the temperature will increase of a value laying in the real interval [1−δ, 1+δ] = [0.6, 1.4]. Thus, vn will be in [0.6, 11.1 + δ] ⊆[0, 11.1 + δ]. Moreover, if vn−1 ∈[0, 9.9], then the state variable stress is not incremented and hence sn = 0 with vn ∈ [0+1−δ , 9.9+1+δ] = [0.6 , 10.9+δ] ⊆[0 , 10.9+δ]. Otherwise, if vn−1 ∈(9.9, 10.1], then the state variable stress is incremented, and hence sn = 1. • Suppose the coolant is active in Sysn−1 (and not active in Sysn). By the induction hypothesis, vn−1 ∈(9.9 −k ∗(1 + δ), 11.1 + δ −k ∗(1 −δ)] for some k ∈1..5 such that the coolant is not active in Sysn−1−k and is active in Sysn−k, . . . , Sysn−1. The case k ∈{1, . . . , 4} is not admissible. In fact if k ∈{1, . . . , 4} then the coolant would be active for less than 5 idle-actions as we know that Sysn is not active. Hence, it must be k = 5. Since δ = 0.4 and k = 5, it holds that vn−1 ∈(9.9 −5 ∗1.4, 11.1 + 0.4 −5 ∗0.6] = (2.8, 8.6] and sn−1 = 0. Moreover, since the coolant is active for 5 time slots, in Sysn−1 the controller and the IDS synchronise together via channel sync and hence the IDS checks the temperature. Since vn−1 ∈(2.8, 8.6] the IDS process sends to the controller a command to stop the cooling, and the controller will switch off the cooling system. Thus, in the next time slot, the temperature will increase of a value laying in the real interval [1 −δ, 1 + δ] = [0.6, 1.4]. As a consequence, in Sysn we will have vn ∈[2.8 + 0, 6, 8.6 + 1.4] = [3.4, 10] ⊆[0, 11.1 + δ]. Moreover, since vn−1 ∈(2.8, 8.6] and sn−1 = 0, we derive that the state variable stress is not increased and hence sn = 0, with vn ∈[3.4, 10] ⊆[0, 10.9 + δ]. 2) Let us assume that the coolant is not active in Sysn and vn ∈(10.1, 11.1+δ]; we prove that the coolant is active in Sysn+1 with sn+1 ∈1..2. Since the coolant is not active in Sysn, then it will check the temperature before the next time slot. Since vn ∈(10.1, 11.1+δ] and ϵ = 0.1, then the process Ctrl will sense a temperature greater than 10 and the coolant will be turned on. Thus, the coolant will be active in Sysn+1. Moreover, since vn ∈(10.1, 11.1 + δ], and sn could be either 0 or 1, the state variable stress is increased and therefore sn+1 ∈1..2. 3) Let us assume that the coolant is active in Sysn; we prove that vn ∈(9.9 −k ∗(1 + δ), 11.1 + δ −k ∗(1 −δ)] for some k ∈1..5 and the coolant is not active in Sysn−k and active in Sysn−k+1, . . . , Sysn. Moreover, we have to prove that if k ≤3 then sn ∈1..k+1, otherwise, if k > 3 then sn = 0. We prove the ﬁrst statement. That is, we prove that vn ∈ (9.9−k∗(1+δ), 11.1+δ−k∗(1−δ)], for some k ∈1..5, and the coolant is not active in Sysn−k, whereas it is active in the systems Sysn−k+1, . . . , Sysn. We separate the case in which the coolant is active in Sysn−1 from that in which is not active. • Suppose the coolant is not active in Sysn−1 (and active in Sysn). In this case k = 1 as the coolant is not active in Sysn−1 and it is active in Sysn. Since k = 1, we have to prove vn ∈(9.9 −(1 + δ), 11.1 + δ −(1 −δ)]. However, since the coolant is not active in Sysn−1 and is active in Sysn it means that the coolant has been switched on in Sysn−1 because the sensed temperature was above 10 (since ϵ = 0.1 this may happen only if vn−1 > 9.9). By the induction hypothesis, since the coolant is not active in Sysn−1, we have that vn−1 ∈ [0, 11.1 + δ]. Therefore, from vn−1 > 9.9 and vn−1 ∈ [0, 11.1 + δ] it follows that vn−1 ∈(9.9, 11.1 + δ]. Furthermore, since the coolant is active in Sysn, the temperature will decrease of a value in [1 −δ, 1 + δ] and therefore vn ∈(9.9 −(1 + δ), 11.1 + δ −(1 −δ)], which concludes this case of the proof. • Suppose the coolant is active in Sysn−1 (and active in Sysn as well). By the induction hypothesis, there is h ∈1..5 such that vn−1 ∈(9.9 −h ∗(1 + δ), 11.1 + δ −h ∗(1 −δ)] and 14 the coolant is not active in Sysn−1−h and is active in Sysn−h, . . . , Sysn−1. The case h = 5 is not admissible. In fact, since δ = 0.4, if h = 5 then vn−1 ∈(9.9 −5 ∗1.4, 11.1 + δ −5 ∗ 0.6] = (2.8, 8.6]. Furthermore, since the cooling system has been active for 5 time instants, in Sysn−1 the controller and the IDS synchronise together via channel sync, and the IDS checks the received temperature. As vn−1 ∈(2.8, 8.6], the IDS sends to the controller via channel ins the command stop. This implies that the controller should turn off the cooling system, in contradiction with the hypothesis that the coolant is active in Sysn. Hence, it must be h ∈1..4. Let us prove that for k = h + 1 we obtain our result. Namely, we have to prove that, for k = h + 1, (i) vn ∈(9.9 −k ∗(1 + δ), 11.1 + δ −k ∗(1 −δ)], and (ii) the coolant is not active in Sysn−k and active in Sysn−k+1, . . . , Sysn. Let us prove the statement (i). By the induction hypothesis, it holds that vn−1 ∈(9.9 −h ∗(1 + δ), 11.1 + δ −h ∗(1 −δ)]. Since the coolant is active in Sysn, the temperature will decrease Hence, vn ∈(9.9−(h+1)∗(1+δ), 11.1+δ−(h+1)∗(1−δ)]. Therefore, since k = h + 1, we have that vn ∈ (9.9 −k ∗(1 + δ), 11.1 + δ −k ∗(1 −δ)]. Let us prove the statement (ii). By the induction hypothesis the coolant is not active in Sysn−1−h and it is active in Sysn−h, . . . , Sysn−1. Now, since the coolant is active in Sysn, for k = h + 1, we have that the coolant is not active in Sysn−k and is active in Sysn−k+1, . . . , Sysn, which concludes this case of the proof. Thus, we have proved that vn ∈(9.9 −k ∗(1 + δ), 11.1 + δ −k ∗(1 −δ)], for some k ∈1..5; moreover, the coolant is not active in Sysn−k and active in the systems Sysn−k+1, . . . , Sysn. It remains to prove that sn ∈1..k+1 if k ≤3, and sn = 0, otherwise. By inductive hypothesis, since the coolant is not active in Sysn−k, we have that sn−k ∈0..1. Now, for k ∈[1..2], the temperature could be greater than 9.9. Hence if the state variable stress is either increased or reset, then sn ∈1..k+1, for k ∈1..3. Moreover, since for k ∈3..5 the temperature is below 9.9, it follows that sn = 0 for k > 3. Proof of Proposition 2. Since δ = 0.4, by Lemma 1 the value of the state variable temp is always in the real interval [0, 11.5]. As a consequence, the invariant of the system is never violated and the system never deadlocks. Moreover, after 5 time units of cooling, the state variable temp is always in the real interval (9.9 −5 ∗1.4, 11.1 + 0.4 −5 ∗0.6] = (2.9, 8.5]. Hence, the process IDS will never transmit on the channel alarm. Finally, by Lemma 1 the maximum value reached by the state variable stress is 4 and therefore the system does not reach unsafe states. Proof of Proposition 3. Let us prove the two statements sepa- rately. • Since ϵ = 0.1, if process Ctrl senses a temperature above 10 (and hence Sys turns on the cooling) then the value of the state variable temp is greater than 9.9. By Lemma 1, the value of the state variable temp is always less than or equal to 11.1 + δ. Therefore, if Ctrl senses a temperature above 10, then the value of the state variable temp is in (9.9, 11.1 + δ]. • By Lemma 1 (third item), the coolant can be active for no more than 5 time slots. Hence, by Lemma 1, when Sys turns off the cooling system the state variable temp ranges over (9.9 −5 ∗(1 + δ), 11.1 + δ −5 ∗(1 −δ)]. B. Proofs of § III Proof of Proposition 4. We distinguish the two cases, depend- ing on m. • Let m ≤8. We recall that the cooling system is activated only when the sensed temperature is above 10. Since ϵ = 0.1, when this happens the state variable temp must be at least 9.9. Note that after m−1 ≤7 idle-actions, when the attack tries to interact with the controller of the actuator cool, the variable temp may reach at most 7 ∗(1 + δ) = 7 ∗1.4 = 9.8 degrees. Thus, the cooling system will not be activated and the attack will not have any effect. • Let m > 8. By Proposition 2, the system Sys in isolation may never deadlock, it does not get into an unsafe state, and it may never emit an output on channel alarm. Thus, any execution trace of the system Sys consists of a sequence of τ-actions and idle-actions. In order to prove the statement it is enough to show the following four facts: – the system Sys ∥Am may not deadlock in the ﬁrst m + 3 time slots; – the system Sys ∥Am may not emit any output in the ﬁrst m + 3 time slots; – the system Sys ∥Am may not enter in an unsafe state in the ﬁrst m + 3 time slots; – the system Sys ∥Am has a trace reaching un unsafe state from the (m+4)-th time slot on, and until the invariant gets violated and the system deadlocks. The ﬁrst three facts are easy to show as the attack may steal the command addressed to the actuator cool only in the m-th time slot. Thus, until time slot m, the whole system behaves correctly. In particular, by Proposition 2 and Proposition 3, no alarms, deadlocks or violations of safety conditions occur, and the temperature lies in the expected ranges. Any of those three actions requires at least further 4 time slots to occur. Indeed, by Lemma 1, when the cooling is switched on in the time slot m, the variable stress might be equal to 2 and hence the system might not enters in an unsafe state in the ﬁrst m + 3 time 15 slots. Moreover, an alarm or a deadlock needs more than 3 time slots and hence no alarm can occur in the ﬁrst m + 3 time slots. Let us show the fourth fact, i.e., that there is a trace where the system Sys ∥Am enters into an unsafe state starting from the (m+4)-th time slot and until the invariant gets violated. Firstly, we prove that for all time slots n, with 9 ≤n < m, there is a trace of the system Sys ∥Am in which the state variable temp reaches the values 10.1 in the time slot n. The fastest trace reaching the temperature of 10.1 degrees requires ⌈10.1 1+δ ⌉= ⌈10.1 1.4 ⌉= 8 time units, whereas the slowest one ⌈10.1 1−δ ⌉= ⌈10.1 0.6 ⌉= 17 time units. Thus, for any time slot n, with 9 ≤n ≤18, there is a trace of the system where the value of the state variable temp is 10.1. Now, for any of those time slots n there is a trace in which the state variable temp is equal to 10.1 in all time slots n+10i < m, with i ∈N. Indeed, when the variable temp is equal to 10.1 the cooling might be activated. Thus, there is a trace in which the cooling system is activated. We can always assume that during the cooling the temperature decreases of 1 + δ degrees per time unit, reaching at the end of the cooling cycle the value of 5. This entails that the trace may continue with 5 time slots in which the variable temp is increased of 1 + δ degrees per time unit; reaching again the value 10.1. Thus, for all time slots n, with 9 ≤n < m, there is a trace of the system Sys ∥Am in which the state variable temp is 10.1 in n. As a consequence, we can suppose that in the m−1-th time slot there is a trace in which the value of the variable temp is 10.1. Since ϵ = 0.1, the sensed temperature lays in the real interval [10, 10.2]. Let us focus on the trace in which the sensed temperature is 10 and the cooling system is not activated. In this case, in the m-th time slot the system may reach a temperature of 10.1 + (1 + δ) = 11.5 degrees and the variable stress is 1. The process Ctrl will sense a temperature above 10 sending the command cool!on to the actuator cool. Now, since the attack Am is active in that time slot (m > 8), the command will be stolen by the attack and it will never reach the actuator. Without that dose of coolant, the temperature of the system will continue to grow. As a consequence, after further 4 time units of cooling, i.e. in the m+4-th time slot, the value of the state variable stress may be 5 and the system enters in an unsafe state. After 1 time slots, in the time slot m + 5, the controller and the IDS synchronise via channel sync, the IDS will detect a temperature above 10, and it will ﬁre the output on channel alarm saying to process Ctrl to keep cooling. But Ctrl will not send again the command cool!on. Hence, the temperature would continue to increase and the system remains in an unsafe state while the process IDS will keep sending of alarm(s) until the invariant of the environment gets violated. Proof of Proposition 5. By induction on the length of the trace. In order to prove Proposition 6, we introduce Lemma 2. This is a variant of Lemma 1 in which the CPS Sys runs in parallel with the attack An deﬁned in Example 5. Here, due to the presence of the attack, the temperature is 2 degrees higher when compared to the system Sys in isolation. Lemma 2. Let Sys be the system deﬁned in Example 2 and An be the attack of Example 5. Let Sys ∥An = Sys1 t1 −−→ idle −−−→. . . Sysn−1 tn−1 −−−−− → idle −−−→Sysn such that the traces tj contain no idle-actions, for any j ∈ 1..n−1, and for any i ∈1..n Sysi = Ei ⋊⋉Pi with Ei = ⟨ξi x, ξi u, δ, evol, ϵ, meas, inv⟩. Then, for any i ∈1..n−1 we have the following: • if ξi u(cool) = oﬀ, then ξi x(temp) ∈[0, 11.1 + 2 + δ]; • if ξi u(cool) = oﬀand ξi x(temp) ∈(10.1+2, 11.1+2+δ], then we have ξi+1 u (cool) = on; • if ξi u(cool) = on, then ξi x(temp) ∈(9.9 + 2 −k ∗(1 + δ), 11.1+2+δ −k∗(1−δ)], for some k ∈1..5, such that ξi−k u (cool) = oﬀand ξi−j u (cool) = on, for j ∈0..k−1. Proof. Similar to the proof of Lemma 1. Now, everything is in place to prove Proposition 6. Proof of Proposition 6. Let us proceed by case analysis. • Let 0 ≤n ≤8. In the proof of Proposition 4, we remarked that the system Sys in isolation may sense a temperature greater than 10 only after 8 idle-actions, i.e., in the 9-th time slot. However, the life of the attack is n ≤8, and in the 9-th time slot the attack is already terminated. As a consequence, starting from the 9-th time slot the system will correctly sense the temperature and it will correctly activate the cooling system. • Let n = 9. The maximum value that may be reached by the state variable temp after 8 idle-actions, i.e., in the 9-th time slot, is 8∗(1+δ) = 8∗1.4 = 11.2. However, since in the 9-th time slot the attack is still alive, the process Ctrl will sense a temperature below 10 and the system will move to the next time slot and the state variable stress is incremented. Then, in the 10-th time slot, when the attack is already terminated, the maximum temperature the system may reach is 11.2+(1+δ) = 12.6 degrees and the state variable stress is equal to 1. Thus, the process Ctrl will sense a temperature greater than 10, activating the cooling system and incrementing the state variable stress. As a consequence, during the following 4 time units of cooling, the value of the state variable temp will be at most 12.6−4∗(1−δ) = 12.6−4∗0.6 = 10.2, and hence in the 14-th time slot, the value of the state variable stress is 5. As a consequence, the system will enter in an unsafe state. In the 15-th time slot, the value of the state variable stress is still equal to 5 and the system will still be in an unsafe state. However, the value of the state variable temp will be at most 12.6 −5 ∗(1 −δ) = 12.6 −5 ∗0.6 = 9.6 16 which will be sensed by process IDS as at most 9.7 (sensor error ϵ = 0.1). As a consequence, no alarm will be turned on and the variable stress will be reset. Moreover, the invariant will be obviously always preserved. As in the current time slot the attack has already termi- nated, from this point in time on, the system will behave correctly with neither deadlocks or alarms. • Let n ≥10. In order to prove that Sys ∥An ≃[14,n+7] Sys, it is enough to show that: – the system Sys ∥An does not deadlock; – the system Sys ∥An may not emit any output in the ﬁrst 13 time slots; – there is a trace in which the system Sys ∥An enters in an unsafe state in the 14-th time slot; – there is a trace in which the system Sys ∥An is in an unsafe state in the (n+7)-th time slot; – the system Sys ∥An does not have any execution trace emitting an output along channel alarm or entering in an unsafe state after the n + 7-th time slot. As regards the ﬁrst fact, since δ = 0.4, by Lemma 2 the temperature of the system under attack will always remain in the real interval [0, 13.5]. Thus, the invariant is never violated and the trace of the system under attack cannot contain any deadlock-action. Moreover, when the attack terminates, if the temperature is in [0, 9.9], the system will continue his behaviour correctly, as in isolation. Otherwise, since the temperature is at most 13.5, after a possible sequence of cooling cycles, the temperature will reach a value in the interval [0, 9.9], and again the system will continue its behaviour correctly, as in isolation. Concerning the second and the third facts, the proof is analogous to that of case n = 9. Concerning the fourth fact, ﬁrstly we prove that for all time slots m, with 9 < m ≤n, there is a trace of the system Sys ∥An in which the state variable temp reaches the values 12 in the time slot m. Since the attack is alive at that time, and ϵ = 0.1, when the variable temp will be equal to 12 the sensed temperature will lay in the real interval [9.9, 10.1]. The fastest trace reaching the temperature of 12 degrees requires ⌈12 1+δ⌉= ⌈12 1.4⌉= 9 time units, whereas the slowest one ⌈12 1−δ⌉= ⌈12 0.6⌉= 20 time units. Thus, for any time slot m, with 9 < m ≤21, there is a trace of the system where the value of the state variable temp is 12. Now, for any of those time slots m there is a trace in which the state variable temp is equal to 12 in all time slots m + 10i < n, with i ∈N. As already said, when the variable temp is equal to 12 the sensed temperature lays in the real interval [9.9, 10.1] and the cooling might be activated. Thus, there is a trace in which the cooling system is activated. We can always ﬁnd a trace where during the cooling the temperature decreases of 1 + δ degrees per time unit, reaching at the end of the cooling cycle the value of 5. Thus, the trace may continue with 5 time slots in which the variable temp is increased of 1 + δ degrees per time unit; reaching again the value 12. Thus, for all time slots m, with 9 < m ≤n, there is a trace of the system Sys ∥An in which the state variable temp has value 12 in the time slot m. Therefore, we can suppose that in the n-th time slot the variable temp is equal to 12 and, since the maximum increment of temperature is 1.4, the the variable stress is at least equal to 1. Since the attack is alive and ϵ = 0.1, in the n-th time slot the sensed temperature will lay in [9.9, 10.1]. We consider the case in which the sensed temperature is less than 10 and hence the cooling is not activated. Thus, in the n+1-th time slot the system may reach a temperature of 12 + 1 + δ = 13.4 degrees and the process Ctrl will sense a temperature above 10, and it will activate the cooling system. In this case, the variable stress will be increased. As a consequence, after further 5 time units of cooling, i.e. in the n+6-th time slot, the value of the state variable temp may reach 13.5 −5 ∗(1 −δ) = 10.4 and the alarm will be ﬁred and the variable stress will be still equal to 5. Therefore, in the n+7-th time slot the variable stress will be still equal to 5 and the system will be in an unsafe state. Concerning the ﬁfth fact, by Lemma 2, in the n+1-th time slot the attack will be terminated and the system may reach a temperature that is, in the worst case, at most 13.5. Thus, the cooling system may be activated and the variable stress will be increased. As a consequence, in the n+7-th time slot, the value of the state variable temp may be at most 13.5 −6 ∗(1 −δ) = 13.5 −6 ∗0.6 = 9.9 and the variable stress will be reset to 0. Thus, after the n + 7-th time slot, the system will behave correctly, as in isolation. In order to prove Theorem 1, we introduce the following lemma. Lemma 3. Let M be an honest and sound CPS, C a class of attacks, and A an attack of an arbitrary class C′ ⪯C. Whenever M ∥A t −− →M ′ ∥A′, then M ∥Top(C) t −− →M ′ ∥ Y ι∈I Att(ι, #idle(t)+1, C(ι)) . Proof. Let us denote with Toph(C) the attack process Y ι∈I Att(ι, h, C(ι)). Obviously, Top1(C) = Top(C). The proof is by mathematical induction on the length k of the trace t. Base case k = 1. This means t = α, for some action α. We proceed by case analysis on the action α. • α = cv. Since the attacker A does not use a communica- tion channel, from M ∥A cv −−− →M ′ ∥A′ we can derive 17 that A = A′ and M cv −−− →M ′. Hence by rules (Par) and (Out), we derive M ∥Top(C) cv −−− →M ′ ∥Top1(C) = M ′ ∥Top(C). • α = cv. This case is similar to the previous one. • α = τ. There are several sub-cases. – Let M ∥A τ −−→M ′ ∥A′ be derived by an application of rule (SensReadSec). Since the attacker A performs only malicious actions on physical devices, from M ∥A τ −−→M ′ ∥A′ we can derive that A = A′ and P s?v −−−→P ′, for some processes P and P ′ such that M = E ⋊⋉S P and M ′ = E ⋊⋉S P ′. Hence by an application of rules (Par) and (SensReadSec) we derive M ∥Top(C) τ −−→M ′ ∥Top1(C) = M ′ ∥Top(C). – Let M ∥A τ −−→M ′ ∥A′ be derived by an application of rule (ActWriteSec). This case is similar to the previous one. – Let M ∥A τ −−→M ′ ∥A′ be derived by an application of rule (SensReadUnSec). Since the attacker A performs only malicious actions, from M ∥A τ −−→M ′ ∥A′ we can derive that A = A′ and P s?v −−−→P ′ for some processes P and P’ such that M = E ⋊⋉S P and M ′ = E ⋊⋉S P ′. By considering rnd({true, false}) = false for any process Att(ι, 1, C(ι)), we have that Top(C) can perform only a idle action, and Top(C) Es!v −−−− → ̸ . Hence by an application of rules (Par) and (SensReadUnSec) we derive M ∥Top(C) τ −−→M ′ ∥ Top1(C) = M ′ ∥Top(C). – Let M ∥A τ −−→M ′ ∥A′ be derived by an application of rule (ActWriteUnSec). This case is similar to the previous one. – Let M ∥A τ −−→M ′ ∥A′ be derived by an application of rule (ESensRead E). Since M is sound it follows that M = M ′ and A Es?v −−−−→A′. This entails 1 ∈C′(Es?) ⊆ C(Es?), and Top(C) Es?v −−−−→Top1(C) = Top(C) by assuming rnd({true, false}) = true for the process Att(Es?, 1, C(Es?)). Hence, by an application of rules (Par) and (ESensRead E) we derive M ∥Top(C) τ −−→ M ′ ∥Top1(C) = M ′ ∥Top(C). – Let M ∥A τ −−→M ′ ∥A′ be derived by an application of rule (EActWrite E). Since M is sound it follows that M = M ′ and A Ea!v −−−−→A′. As a consequence, 1 ∈ C′(Ea!) ⊆C(Ea!), and Top(C) Ea!v −−−−→Top1(C) = Top(C) by assuming rnd({true, false})=true and rnd(R)=v for the process Att(Ea!, 1, C(Ea!)). Thus, by an ap- plication of rules (Par) and (EActWrite E) we derive M ∥Top(C) τ −−→M ′ ∥Top1(C) = M ′ ∥Top(C). – Let M ∥A τ −−→M ′ ∥A′ be derived by an application of rule (Tau). Let M = E ⋊⋉S P and M ′ = E′ ⋊⋉S P ′. There are two possibilities: either (i) P ∥A τ −−→P ′ ∥ A′, or (ii) P ∥A τ:p −−−→P ′ ∥A′. In the case (i), by inspection of Table I and by deﬁnition of attacker, it follows that A cannot perform τ-action since A does not use channel communication and performs only malicious actions. Hence P τ −−→P ′ and, by an application of rules (Par) and (Tau), we derive M ∥Top(C) τ −−→M ′ ∥Top1(C) = M ′ ∥Top(C). In the case (ii), since M is sound and A can performs only malicious actions, we have that either (i) P s?v −−−→ P ′ and A Es!v −−−− →A′ or, (ii) P a!v −−−→P ′ and A Ea?v −−−−→ A′. We consider the case (i) P s?v −−−→P ′ and A Es!v −−−− → A′; the case (ii) is similar. Since A Es!v −−−− →A′, we derive 1 ∈C′(Es!) ⊆C(Es!), and Top(C) Es!v −−−− →Top1(C) = Top(C) by assuming rnd({true, false})=true and rnd(R)=v for the process Att(Es!, 1, C(Es!)). Thus, by an ap- plication of rules (ESensWrite E) and (Tau) we derive M ∥Top(C) τ −−→M ′ ∥Top1(C) = M ′ ∥Top(C). • α = idle. This implies that the transition M ∥A idle −−−→ M ′ ∥A′ is derived by an application of rule (Time). From M ∥A idle −−−→M ′ ∥A′ we derive M idle −−−→M ′. Hence, it sufﬁces to prove that Top(C) idle −−−→Top2(C) and M ∥Top(C) τ −−→ ̸ . First, let us prove that Top(C) idle −−−→Top2(C). We consider two cases: 1 ∈C(ι) and 1 ̸∈C(ι). Let 1 ∈ C(ι). The transition Att(ι, 1, C(ι)) idle −−−→Att(ι, 2, C(ι)) can be derived by assuming rnd({true, false}) = false. Moreover, since rnd({true, false}) = false the pro- cess Att(ι, 1, C(ι)) can perform only a idle action. Let 1 ̸∈C(ι). Also in this case the process Att(ι, 1, C(ι)) can perform only a idle action. As a consequence, e Att(ι, 1, C(ι)) idle −−−→Att(ι, 2, C(ι)). Thus, Top(C) idle −−−→Top2(C) . Let us prove now that M ∥Top(C) τ −−→ ̸ . Since M ∥ A τ −−→ ̸ it follows that M τ −−→ ̸ . Moreover, since Top(C) can perform only a idle action then, by deﬁnition of rule (Time), it follows that M ∥Top(C) τ −−→ ̸ . • α = deadlock. This case is not possible, because M ∥ A deadlock −−−−−−− →M ′ ∥A′ would entail M deadlock −−−−−−− →M ′. But M is sound and it cannot deadlock. • α = unsafe. Again, this case is not possible because M is sound. Inductive case: k > 1. We have to prove that M ∥A t −− →M ′ ∥A′ implies M ∥ Top(C) t −− →M ′ ∥Top#idle(t)+1(C). 18 Since the length of t is greater than 1, it follows that t = t′α, for some t′ and α. Hence, there are M ′′ and A′′ such that M ∥A t′ −−→M ′′ ∥A′′ α −−→M ′ ∥A′ . By the induction hypothesis, it follows that M ∥Top(C) t′ −−→ M ′′ ∥Top#idle(t′)+1(C). To get the result it is enough to show that M ′′ ∥A′′ α −−→M ′ ∥A′ implies M ′′ ∥ Top#idle(t′)+1(C) α −−→M ′ ∥Top#idle(t)+1(C). The reasoning is similar to that followed in the base case, except for α = deadlock and α = unsafe. We prove the case α = deadlock, the other is similar. Let M = E ⋊⋉S P. The transition M ′′ ∥A deadlock −−−−−−− →M ′ ∥ A′ must be derived by an application of rule (Deadlock). This implies that that M ′′ = M ′, A′′ = A′ and inv(E) = false. Thus, by an application of rule (Deadlock) we derive M ′′ ∥Top#idle(t′)+1(C) deadlock −−−−−−− →M ′ ∥Top#idle(t′)+1(C). Since #idle(t) + 1 = #idle(t′) + #idle(deadlock) + 1 = #idle(t′) + 1 we have that M ′′ ∥Top#idle(t′)+1(C) deadlock −−−−−−− → M ′ ∥Top#idle(t)+1(C). As required. Everything is ﬁnally in place to prove Theorem 1. Proof of Theorem 1. The top attacker Top(C) can mimic any execution trace of any attack A of class C′, with C′ ⪯C. Thus, by Lemma 3, if M ∥A t −− →, for some trace t, then M ∥Top(C) t −− →as well. For any M and A, either M ∥A ⊑M or M ∥A ⊑m2..n2 M, for some m2 and n2 (m2 = 1 and n2 = ∞if the two systems are completely unrelated). Suppose by contradiction that M ∥A ̸⊑M and M ∥A ⊑m2..n2 M, with m2..n2 ̸⊆ m1..n1. There are two cases: either n1 = ∞or n1 ∈N+. If n1 = ∞then m2 < m1. Since M ∥A ⊑m2..n2 M, by Deﬁnition 8 there is a trace t, with #idle(t) = m2−1, such that M ∥A t −− →and M ̸ ˆt == ⇒. By Lemma 3, this entails M ∥Top(C) t −− →. Since M ̸ ˆt == ⇒and #idle(t) = m2−1 < m1, this contradicts M ∥Top(C) ⊑m1..n1 M. If n1 ∈N+ then m2 < m1 and/or n1 < n2, and we reason as in the previous case. C. Proofs of § IV In order to prove Proposition 8, we need a couple of lemmas. Lemma 4 is a variant of Lemma 1. Here the behaviour of Sys is parametric on the uncertainty. Lemma 4. Let Sys be the system deﬁned in Example 2, and 0.4 < γ ≤ 9 20. Let Sys[δ ←γ] = Sys1 t1 −−→ idle −−−→Sys2 · · · tn−1 −−−−− → idle −−−→Sysn such that the traces tj contain no idle-actions, for any j ∈ 1..n−1, and for any i ∈1..n Sysi = Ei ⋊⋉Pi with Ei = ⟨ξi x, ξi u, γ, evol, ϵ, meas, inv⟩. Then, for any i ∈1..n−1 we have the following: • if ξi u(cool) = oﬀthen ξi x(temp) ∈[0, 11.1 + γ] and ξi x(stress) = 0 if ξi x(temp) ∈[0, 10.9+γ] and, otherwise, ξi x(stress) = 1; • if ξi u(cool) = oﬀand ξi x(temp) ∈(10.1, 11.1 + γ] then ξi+1 u (cool) = on and ξi+1 x (stress) ∈1..2; • if ξi u(cool) = on then ξi x(temp) ∈(9.9−k∗(1+γ), 11.1+ γ−k∗(1−γ)], for some k ∈1..5, such that ξi−k u (cool) = oﬀand ξi−j u (cool) = on, for j ∈0..k−1; moreover, if k ∈ 1..3 then ξi x(stress) ∈1..k+1, otherwise, ξi x(stress) = 0. Proof. Similar to the proof of Lemma 1. The crucial difference w.r.t. the proof of Lemma 1 is limited to the second part of the third item. In particular the part saying that ξi x(stress) = 0, when k ∈4..5. Now, after 3 time units of cooling, the state variable stress lays in the integer interval 1..k+1 = 1..4. Thus, in order to have ξi x(stress) = 0, when k ∈4..5, the temperature in the third time slot of the cooling must be less than or equal to 9.9. However, from the ﬁrst statement of the third item we deduce that, in the third time slot of cooling, the state variable temp reaches at most 11.1 + γ −3 ∗(1 −γ) = 8.1 + 4γ. Thus, Hence we have that 8.1 + 4γ ≤9.9 for γ ≤ 9 20. The following lemma is a variant of Proposition 2. Lemma 5. Let Sys be the system deﬁned in Example 2 and γ such that 0.4 < γ ≤ 9 20. If Sys[δ ←γ] t −− →Sys′, for some t = α1 . . . αn, then αi ∈{τ, idle}, for any i ∈1..n. Proof. By Lemma 4, the temperature will always lay in the real interval [0, 11.1 + γ]. As a consequence, since γ ≤ 9 20, the system will never deadlock. Moreover, after 5 idle action of coolant the state variable temp is in (9.9 −5 ∗(1 + γ), 11.1 + γ −5 ∗(1 −γ)] = (4.9 −5γ , 6.1 + 6γ]. Since ϵ = 0.1, the value detected from the sensor will be in the real interval (4.8−5γ , 6.2+6γ]. Thus, the temperature sensed by IDS will be at most 6.2 + 6γ ≤ 6.2 + 6 ∗9 20 ≤10, and no alarm will be ﬁred. Finally, the maximum value that can be reached by the state variable stress is k+1m for k = 3. As a consequence, the system will not reach an unsafe state. The following Lemma is a variant of Proposition 3. Here the behaviour of Sys is parametric on the uncertainty. Lemma 6. Let Sys be the system deﬁned in Example 2 and γ such that 0.4 < γ ≤ 9 20. Then, for any execution trace of Sys[δ ←γ] we have the following: • if either process Ctrl or process IDS senses a temperature above 10 then the value of the state variable temp ranges over (9.9, 11.1 + γ]; • when the process IDS tests the temperature the value of the state variable temp ranges over (9.9 −5 ∗(1 + γ), 11.1 + γ −5 ∗(1 −γ)]. Proof. As to the ﬁrst statement, since ϵ = 0.1, if either process Ctrl or process IDS senses a temperature above 10 then the value of the state variable temp is above 9.9. By Lemma 4, the state variable temp is less than or equal to 11.1 + γ. Therefore, if either process Ctrl or process IDS sense a 19 temperature above 10 then the value of the state variable temp is in (9.9, 11.1 + γ]. Let us prove now the second statement. When the process IDS tests the temperature then the coolant has been active for 5 idle actions. By Lemma 4, the state variable temp ranges over (9.9 −5 ∗(1 + γ), 11.1 + γ −5 ∗(1 −γ)]. Everything is ﬁnally in place to prove Proposition 8. Proof of Proposition 8. For (1) we have to show that Sys[δ ← γ] ⊑Sys, for γ ∈( 8 20, 9 20). But this obviously holds by Lemma 5. As regards item (2), we have to prove that Sys[δ ←γ] ̸⊑ Sys, for γ > 9 20. By Proposition 2 it is enough to show that the system Sys[δ ←γ] has a trace which either (i) sends an alarm, or (ii) deadlocks, or (iii) enters in an unsafe state. We can easily build up a trace for Sys[δ ←γ] in which, after 10 idle- actions, in the 11-th time slot, the value of the state variable temp is 10.1. In fact, it is enough to increase the temperature of 1.01 degrees for the ﬁrst 10 rounds. Notice that this is an admissible value since, 1.01 ∈[1 −γ, 1 + γ], for any γ > 9 20. Being 10.1 the value of the state variable temp, there is an execution trace in which the sensed temperature is 10 (recall that ϵ = 0.1) and hence the cooling system is not activated but the state variable stress will be increased. In the following time slot, i.e., the 12-th time slot, the temperature may reach at most the value 10.1 + 1 + γ and the state variable stress is 1. Now, if 10.1 + 1 + γ > 50 then the system deadlocks. Otherwise, the controller will activate the cooling system, and after 3 time units of cooling, in the 15-th time slot, the state variable stress will be 4 and the variable temp will be at most 11.1 + γ −3(1 −γ) = 8.1 + 4γ. Thus, there is an execution trace in which the temperature is 8.1 + 4γ, which will be greater than 9.9 being γ > 9 20. As a consequence, in the next time slot, the state variable stress will be 5 and the system will enter in an unsafe state. This is enough to derive that Sys[δ ←γ] ̸⊑Sys, for γ > 9 20. Proof of Theorem 2. Consider the case of the deﬁnitive impact. By Lemma 3, if M ∥A t −− →then M ∥Top(C) t −− →. This entails M ∥A ⊑M ∥Top(C). Thus, if M ∥ Top(C) ⊑M[ξw ←ξw+ξ], for ξ ∈R ˆ X , ξ > 0, then M ∥A ⊑M[ξw ←ξw+ξ], by transitivity of ⊑. The proof in the case of the pointwise impact is by contradiction. Suppose ξ′ > ξ. Since Top(C) has a pointwise impact ξ at time m, it follows that ξ is given by: inf  ξ′′ : ξ′′∈R ˆ X ∧M ∥Top(C) ⊑m..n M[ξw ← ξw+ξ′′], n∈N∪∞ . Similarly, since A has a pointwise impact ξ′ at time m′, it follows that ξ′ is given by inf  ξ′′ : ξ′′∈R ˆ X ∧M ∥A ⊑m′..n M[ξw ←ξw+ξ′′], n∈N∪∞ . Now, if it were m = m′ then ξ ≥ξ′ because M ∥A t −− → entails M ∥Top(C) t −− →., by an application of Lemma 3. This is contradiction with the fact that ξ < ξ′, Thus, it must be m′ < m. Now, since both ξ and ξ′ are the inﬁmum functions and since ξ′ > ξ, there exist ξ and ξ′ such that ξ ≤ξ ≤ξ′ ≤ξ′ and M ∥Top(C) ⊑m..n M[ξw ←ξw+ξ], for some n, and M ∥A ⊑m′..n′ M[ξw ←ξw+ξ′], for some n′. Hence, from M ∥A ⊑m′..n′ M[ξw ←ξw+ξ′], we have that there exists a trace t with #idle(t) = m′ −1 such that M ∥A t −− →and M[ξw ←ξw+ξ′] ̸ ˆt == ⇒. Since ξ ≤ξ′, by monotonicity (Proposition 7), we deduce that M[ξw ←ξw+ξ] ̸ ˆt == ⇒. Moreover, by Lemma 3, M ∥A t −− → entails M ∥Top(C) t −− →. Summarising, there exists a trace t with #idle(t) = m′ −1 such that M ∥Top(C) t −− →and M[ξw ←ξw+ξ] ̸ ˆt == ⇒. However, this fact and m′ < m is in contradiction with M ∥Top(C) ⊑m..n M[ξw ←ξw+ξ], for some n. This is enough to derive the statement. Proof of Proposition 9. Let us prove the ﬁrst sub-result. As demonstrated in Example 4, we know that Sys ∥A ⊑14..∞ Sys because in the 14-th time slot the compound system will violate the safety conditions emitting an unsafe-action until the invariant will be violated. No alarm will be emitted. Since the system keeps violating the safety condition the temperature must remain greater than 9.9. As proved for Lemma 4 we can prove that we have that the temperature is less than or equal to 10.1+γ. Hence, in the time slot before getting in deadlock, the temperature of the system is in the real interval (9.9, 10.1 + γ]. To deadlock with one idle action and from a temperature in the real interval (9.9, 10.1 + γ], either the temperature reaches a value greater than 50 (namely, 10.1 + γ + 1 + γ > 50) or the temperature reaches a value less than 0 (namely, 9.9 −1 −γ < 0 ). Since γ ≤8.9, both cases can not occur. Thus, we have that Sys ∥A ̸⊑Sys[δ ←γ] . Let us prove the second sub-result. That is, Sys ∥A ⊑Sys[δ ←γ] for γ > 8.9. We demonstrate that whenever Sys ∥A t −− →, for some trace t, then Sys[δ ←γ] ˆt == ⇒as well. We will proceed by case analysis on the kind of actions contained in t. We distinguish three possible cases. • The trace t contains contains only τ-, idle-, unsafe- and deadlock-actions. As discussed in Example 4, Sys ∥ A ⊑14..∞ Sys because in the 14-th time slot the system will violate the safety conditions emitting an unsafe-action until the invariant will be broken. No alarm will be emitted. Note that, when Sys ∥A enters in an unsafe state then the temperature is at most 9.9 + (1 + δ) + 5(1 + δ) = 9.9 + 6(1.4) = 18.3. Moreover, the fastest execution trace, reaching an unsafe state, deadlocks just after ⌈50−18.3 1+δ ⌉= ⌈31,7 1.4 ⌉= 23 idle- actions. Hence, there are m, n ∈N, with m ≥14 and n ≥m + 23, such that the trace t of Sys ∥A satisﬁes the following conditions: (i) in the time interval 1..m −1 the 20 trace t of is composed by τ- and idle-actions; (ii) in the time interval m..(n −1), the trace t is composed by τ-, idle- and unsafe- actions; in the n-th time slot the trace t deadlocks. By monotonicity (Proposition 7), it is enough to show that such a trace exists for Sys[δ ←γ], with 8.9 < γ < 9. In fact, if this trace exists for 8.9 < γ < 9, then it would also exist for γ ≥9. In the following, we show how to build the trace of Sys[δ ←γ] which simulates the trace t of Sys ∥A. We build up the trace in three steps: (i) the sub-trace from time slot 1 to time slot m−6; (ii) the sub-trace from the time slot m−5 to the time slot n−1; (iii) the ﬁnal part of the trace reaching the deadlock. (i) As γ > 8.9 (and hence 1 + γ > 9.9), the system may increment the temperature of 9.9 degrees after a single idle-action. Hence, we choose the trace in which the system Sys[δ ←γ], in the second time slot, reaches the temperature equal to 9.9. Moreover, the system may maintain this temperature value until the (m−6)- th time slot (indeed 0 is an admissible increasing since 0 ∈[1 −γ, 1 + γ] ⊇[−7.9, 10.9]) . Obviously, with a temperature equal to 9.9, only τ- and idle-actions are possible. (ii) Let k ∈R such that 0 < k < γ −8.9 (such k exists since γ > 8.9). We may consider an increment of the temperature of k. This implies that in the (m−5)-th time slot, the system Sys[δ ←γ] may reach the temperature 9.9 + k. Note that k is an admissible increment since 0 < k < γ −8.9 and 8.9 < γ < 9 entails k ∈(0, 0.1). Moreover, the system may maintain this temperature value until the (n−1)-th time slot (indeed, as said before, 0 is an admissible increment). Summarising from the (m−5)-th time slot to the (n−1)-th time slot, the temperature may remain equal to 9.9+k ∈(9.9, 10). As a consequence, from the m-th time slot to the (n−1)- th time slot the system Sys[δ ←γ] may enter in an unsafe state (i.e., safe(E) = false). Thus, an unsafe- action may be performed in the time interval m..(n−1). Moreover, since ϵ = 0.1 and the temperature is e 9.9 + k ∈(9.9, 10), we can always assume that the cooling is not activated until the (n−1)-th time slot. This implies that neither alarm nor deadlock occur. (iii) At this point, since in the (n−1)-th time slot the temperature is equal to 9.9 + k ∈(9.9, 10) (recall that k ∈(0, 1)), the cooling may be activated. We may consider a decrement of 1 + γ. In this manner, in the n-th time slot the system may reach a temperature of 9.9 + k −(1 + γ) < 9.9 + 0 −1 −8.9 = 0 degrees, and the system Sys[δ ←γ] will deadlock. Summarising, for any γ > 8.9 the system Sys[δ ←γ] can mimic any trace t of Sys ∥A. • The trace t contains contains only τ-, idle- and unsafe- actions. This case is similar to the previous one. • The trace t contains only τ-, idle- and alarm-actions. This case cannot occur. In fact, as discussed in Example 4, the process Ctrl never activates the Cooling component (and hence also the IDS component, which is the only one that could send an alarm) since it will always detect a temperature below 10. • The trace t contains only τ- and idle-actions. If the system Sys ∥A has a trace t that contains only τ- and idle-actions, then, by Proposition 2, the system Sys in isolation must have a similar trace with the same number of idle-actions. By an application of Proposition 7, as δ < γ, any trace of Sys can be simulated by Sys[δ ←γ]. As a consequence, Sys[δ ←γ] ˆt == ⇒. This is enough to derive that: Sys ∥A ⊑Sys[δ ←γ] , which concludes the proof. 21