Designing Robust Cyber-Defense Agents with Evolving Behavior Trees Nicholas Potteiger, Ankita Samaddar, Hunter Bergstrom and Xenofon Koutsoukos Department of Computer Science Vanderbilt University Nashville, TN, USA {nicholas.potteiger, ankita.samaddar, hunter.m.bergstrom, xenofon.koutsoukos}@vanderbilt.edu Abstract—Modern network defense can benefit from the use of autonomous systems, offloading tedious and time-consuming work to agents with standard and learning-enabled components. These agents, operating on critical network infrastructure, need to be robust and trustworthy to ensure defense against adaptive cyber-attackers and, simultaneously, provide explanations for their actions and network activity. However, learning-enabled components typically use models, such as deep neural networks, that are not transparent in their high-level decision-making lead- ing to assurance challenges. Additionally, cyber-defense agents must execute complex long-term defense tasks in a reactive manner that involve coordination of multiple interdependent subtasks. Behavior trees are known to be successful in modelling interpretable, reactive, and modular agent policies with learning- enabled components. In this paper, we develop an approach to design autonomous cyber defense agents using behavior trees with learning-enabled components, which we refer to as Evolving Behavior Trees (EBTs). We learn the structure of an EBT with a novel abstract cyber environment and optimize learning-enabled components for deployment. The learning-enabled components are optimized for adapting to various cyber-attacks and deploying security mechanisms. The learned EBT structure is evaluated in a simulated cyber environment, where it effectively mitigates threats and enhances network visibility. For deployment, we develop a software architecture for evaluating EBT-based agents in computer network defense scenarios. Our results demonstrate that the EBT-based agent is robust to adaptive cyber-attacks and provides high-level explanations for interpreting its decisions and actions. Index Terms—Cybersecurity, Autonomous Systems, Behavior Tree, Reinforcement Learning, Machine Learning, Genetic Pro- gramming I. INTRODUCTION Modern network defense is an increasingly difficult task due to a multitude of novel and diverse cyber-attacks and the scale of systems. Developers have crafted solutions such as alert monitors and decision logic rules to alleviate human cognitive fatigue, but this is not viable or scalable as new attacks are discovered. The capabilities and strategies of cyber-defense continue to grow and, as such, fully autonomous cyber defense agents are being considered as an alternative to optimally utilize the resources needed to mitigate adversaries. However, there are challenges regarding the transparency of these agents and their robustness. Autonomous agents contain a mix of standard and learning- enabled components (LECs) trained with machine learning (ML) and reinforcement learning (RL). These LECs are nor- mally modeled with neural networks that lack the ability to provide high-level explanations and struggle to complete long- term objectives with multiple subtasks. Cyber-defense breaks down into a variety of subroutines (analysis, monitoring, restoring, deploying decoys, etc.) that a single component will struggle to represent and optimize. Additionally, it is unclear how autonomous agents will adapt to multiple varieties of cyber-attacks. Adversaries are versatile, they may use one or more attacks on the system targeting multiple components. The adversary may use one strategy at the beginning to explore the system in a low-detection manner and then launch a full- scale attack after they have discovered enough knowledge. The autonomous agent will need to be robust to these scenarios and explain its current perspective of the adversary’s behavior. An effective way of representing a reactive control policy between subtasks in the hierarchy is using Behavior Trees (BTs) [2]. BTs are structures for modeling complex control policies with advantages of modularity, reactivity, and ex- plainability. The reactive nature of the BT allows for explicit switching between multiple behaviors in quick succession depending on environment changes. The LECs and other components can be coordinated and modelled as a policy we refer to as an Evolving Behavior Tree (EBT). An EBT can jointly optimize and model the control flow of optimized components. The structure of the control of behaviors in an EBT is typically manually developed, but due to the complexity of multiple subtasks and their dependencies, construction can prove tedious and potentially infeasible. Recent work has focused on automatic construction of BTs using genetic pro- gramming (GP) [3], [4] or large-language models [5], [6]. In particular, the works in GP develop abstract environments for computational efficiency that map to a realistic simulation. In this paper, we develop an autonomous cyber-defense agent that leverages the hierarchical structure of EBTs for robustness against dynamic cyber-attacks. We design the agent in three stages: (1) learning the high-level control structure of the EBT, (2) optimization of LECs for robustness, and (3) integration and deployment to a realistic cyber environment. The research objective of (1) is to enhance scalability by generating control structures without requiring detailed knowl- edge of network components. This approach enables learning arXiv:2410.16383v1 [cs.AI] 21 Oct 2024 Fig. 1: CybORG CAGE Challenge Scenario 2 [1]; Subnet 1 has five user hosts; Subnet 2 has three enterprise servers and the cyber-defender; Subnet 3 has three operational hosts and one critical operational server. the modular structure of the EBT prior to optimizing LECs, thereby avoiding unnecessary retraining. For (1), we utilize GP and develop a novel abstract cyber-environment referred to as the Cyber-Firefighter to map to a realistic cyber-defense sim- ulation scenario and evaluate the structural performance. The research objective of (2) is to develop generalizable behaviors that are robust to uncertainties in a realistic computer network environment. The EBT contains LECs for choosing cyber- agent actions and determining the red agent strategy based on a non-deterministic network state. We develop a software architecture for the construction and integration of an EBT for a computer network defense scenario. In (3), we deploy and evaluate our autonomous cyber-defense agent in a realistic simulation environment for robustness and explainability. The main technical contributions of our work are: • Design, optimization, and deployment of an EBT for autonomous cyber-defense of a computer network. The EBT structure is designed using GP with a novel abstract cyber environment, the Cyber-Firefighter, that maps to cyber-defense. The EBT structure is generalizable and contains capabilities for decoy deployment, attacker strat- egy detection, and selection of cyber-operations. • An evaluation of the learned EBT structure in the Cyber- Firefighter to demonstrate high-level control performance against an attacker in a network. The GP algorithm in tan- dem with the Cyber-Firefighter is successful at learning an EBT structure that maximizes the performance metric (fitness) to promote mitigation and visibility of an attack. • Development of a software architecture to support the construction and deployment of EBTs on a computer network. The architecture utilizes a blackboard of data sources in a publish-subscribe method to facilitate in- teraction between the EBT and computer network en- vironment. The computer network environment in this paper is CybORG [7], an abstracted version of a computer network, compatible with ML and RL algorithms. • An evaluation of the robustness and explainability of the EBT in CybORG [7] using CAGE Challenge Scenario 2 [1]: a computer network task where the agent must defend against an adversarial agent. We develop an adversarial red agent for this scenario that switches strategies during execution to evaluate the adaptation of our approach. The EBT is successful at defending against dynamic attacks with a 39% increase in the average reward compared to a state-of-the-art method in CybORG CAGE Challenge Scenario 2. The explainable nature of the EBT allows us to monitor key events, such as when the strategy switches or a decoy is deployed, and model transitions between high-level subtasks. II. RELATED WORKS With the onset of sophisticated cyber-attacks, traditional se- curity measures often fall short. Thus, there is a need for more advanced and interpretable solutions. Nowadays, differ- ent RL methods are adopted to develop learning enabled cyber- defense policies in autonomous networks [8]. Different RL- based cyber-defense policies have used CybORG to enact their approaches. Foley et al. utilized a goal-conditioned hierarchi- cal RL (HRL) to select trained defense strategies in CybORG CAGE Challenge Scenario 2 [9]. The defense strategy in [9] is optimized via RL for each attacker strategy, following which a meta-policy selects between the defense strategies based on the attacker behavior at the beginning of a simulation. The winners of the CAGE Challenge Scenario 2, the CardiffUni, used a similar goal-conditioned approach but with an added focus on reducing action space and deploying decoys [9]. Wolk et al. presented an alternative where an ensemble approach aggregates the policy output [10]. Towards emulation, Molina- Markham et al. developed a novel tool, FARLAND, with a focus on realistic cyber-defense environments and curriculum learning for cyber-defense agents [11]. Unlike classic RL tasks where the agents are regularly rewarded for progress, the reward signals assigned to cyber-security tasks are distributed sparsely across each defense episode in the form of penalties. To overcome this gap, Elizabeth et al. presented a reward shaping policy in deep RL and evaluated the policy in Cy- bORG [12]. Another line of study is motivated by neurosymbolic AI ap- proaches which combine the pattern recognition capabilities of neural networks along with the explicit reasoning of symbolic systems. Neurosymbolic AI serves as an emerging area of research in autonomous cyber-defense [13]. An increasingly popular approach for designing neurosymbolic autonomous agents is using Behavior Trees (BTs) [2]. BTs are widely used in control architectures, computer games, robotics, etc [14]. RL can be employed to learn complex BT behaviors. It provides the flexibility and ability to discover innovative so- lutions for sub-tasks within BTs. Whether optimizing a single behavior or multiple learning-enabled behaviors within the tree, RL or HRL techniques are used to jointly optimize and generate policies for BT tasks. Recent efforts have focused on integrating HRL into the BT learning process [15], [16]. These works utilize hierarchical option policies [17] to coordinate a finite set of unknown subtasks in video game simulation environments. Our prior work applied goal-conditioned HRL and a manually constructed EBT for a maze navigation task [18]. In this work, we develop an approach that combines the EBTs with LECs to provide autonomous cyber-defense in computer networks. To the best of our knowledge, this is the first work in the literature that models autonomous cyber- defense agents using EBTs to analyze system behavior and apply appropriate mitigation tactics against adversaries. III. AUTONOMOUS CYBER-DEFENSE Autonomous cyber-defense is the long-term task of fortifying and mitigating a system against cyber-attackers without human intervention. It is a task that constantly needs to be iterated on as long as the system is operating, otherwise novel attacks could breach and disrupt critical resources. Cyber-defense agents can utilize a mix of security standards and LECs to successfully adapt and defend a system. LECs typically rely on function approximators, such as neural networks, trained with ML or RL to compute optimal actions. Existing algorithms work well for short-term tasks. However, as the task increases in complexity and longevity, new subtasks and capabilities are required that a single component struggles to optimize. Also, the components are not transparent, leading to challenges with assurance and trustworthiness. In a system with safety-critical resources, it is essential that we understand the behavior of an agent, or else there can be unintended consequences due to agent error. In general, autonomous cyber-defenders need to be prepared for a diversity of potential cyber-attacks. These attacks can derive from one or multiple actors. During an attack, an actor may decide that it should switch to another attack strategy based on information it has observed in the system. If the cyber-defender is not aware of this switch, the system may be vulnerable due to a lack of knowledge of defense against the new strategy. In the face of these challenges, the objective of this paper is to develop a neurosymbolic model representation of an autonomous cyber-defense agent that (1) captures an explicit hierarchy of subtasks and the control flow required to execute subtasks, (2) employs LECs for specific subtasks, and (3) adapts to multiple dynamic cyber-attacks. The model must allow optimization of the LECs and generalize to multiple Environment Sensors Effectors Goals/Specifications Adversarial Agents Workload/traffic Generators User Interface Cyber Operators Autonomous Agent Fig. 2: Autonomous Cyber-Defense Agent attack scenarios. Furthermore, after training, the model must be deployable to a system for evaluation. We consider CybORG [7], a complex network simulation environment that abstracts real-world scenarios, as our canon- ical problem for developing our model. The network scenario we consider is CAGE Challenge using Scenario 2 [1] as shown in Fig. 1. The network consists of three subnets: Subnet 1 consists of user hosts that are non critical, Subnet 2 consists of enterprise servers to support user activities on Subnet 1 and Subnet 3 consists of critical operational server and three user hosts. CybORG provides an interface to construct attacker (red) and defender (blue) agents with learning-enabled capabilities in a seamless manner. During simulation, the red and blue agents execute actions in parallel. The red agents can scan hosts and subnets, launch exploits, raise privileges, or disrupt a compromised host. To mitigate red agent behavior, the blue agents can take no action (Sleep), monitor the network (Monitor), analyze a host (Analyze), remove any malicious software from a host (Remove), restore a host back to a good state and remove the attacker (Restore), or deploy one of seven decoy service types on a host (Deploy Decoy). Different red agent strategies can be constructed using CybORG. CAGE Challenge Scenario 2 uses two types of pre- defined red agent strategies. The first is BLine, which uses full knowledge of the network to traverse towards the critical operational server. The second strategy is Meander that seeks to explore and disrupt the network, compromising each host before moving to the next one. We consider a third strategy RedSwitch that combines BLine and Meander. This strategy first instantiates an agent using the Meander strategy, then after a randomly selected amount of time, the attacker transitions to an agent using the BLine strategy. The intuition behind this red agent switching strategy is that an attacker may first explore the network (Meander) to discover its topology before deploying a more disruptive attacker (BLine). IV. OPTIMAL BT STRUCTURE FOR CYBER-DEFENSE We present a neurosymbolic approach to autonomous cyber- defense that is robust against dynamic cyber-attacks. The agent constructed interacts with the environment using cyber-agent actions from a set of capabilities to mitigate adversarial red agents. Fig. 2 presents the control flow of the autonomous cyber-defense agent. Given a goal or specification, a symbolic structure acts as a model to interact with the environment. The symbolic structure utilizes cyber actions and sensor informa- tion to take effective action(s) via effectors against adversarial agents in the environment. The symbolic structure in our autonomous cyber-defense agent is a BT. The BT allows us to reason about cyber-defense control at a high level and provides reactive switching to adapt based on environmental shifts. Additionally, BTs are modular, allowing new capabilities to be seamlessly integrated. In this section, we describe the optimization method uti- lizing GP to compute a BT structure for cyber-defense con- trol. The first stage of the neurosymbolic design approach, in Fig. 3 focuses on learning the structure and high-level control of behaviors in the BT. A key contribution of learning the structure is the development of a novel abstract cyber environment, Cyber-Firefighter, to capture high-level computer network defense. Another benefit of the abstract environment is efficient simulation for evaluation of the structure of a BT over a more computationally heavy realistic simulator. A. Cyber-Firefighter Abstract Environment We design an abstract environment, Cyber-Firefighter, a partially-observable pursuit-evasion game, where a defender must contain an attacker from spreading over a network. The pursuit-evasion game is inspired by the Firefighter [19], a game where a “firefighter” (defender) must optimally contain a spreading “fire” (attacker) in a network of “trees” (nodes) using full knowledge of the “fire” movements (attacker’s movements) and a “fire retardant” (mitigation action) is ex- ecuted to block the spread. The game terminates when the “fire” can no longer spread to new “trees”. In the Cyber-Firefighter game, the objective remains the same. However, to make the game similar to cyber-defense on a computer network, the “firefighter” does not have full knowledge of the “fire” movements (attacker’s movements). Therefore, we re-frame the problem as a partially observable environment and include two new actions to reveal information in the environment. 1) The “firefighter” can deploy “drones” (detectors) to detect the “fire” if it spreads into a “drone” detection zone. 2) The “firefighter” can activate a “drone” (per- form analysis) to increase the detection zone and gain further information from the environment. Once the information, {the “fire” has spread to} is revealed to the “trees” (nodes), the “fire retardant” (mitigation action) is placed (executed) to block the spread. Formally, the Cyber-Firefighter environment can be repre- sented as a network graph, G : (V, E), where V denotes the set of “trees” or nodes and E denotes the set of edges or connections through which the “fire” can spread. The neighbor of a node v is denoted by N(v) = {u | (v, u) ∈E; v, u ∈V }. Nr(v) denotes the nodes of the induced neighbor subgraph up to a radius r from node v. Table I shows the notations to describe the Cyber-Firefighter environment. Notation Description vf ∈V fire source T max burn ∈N maximum burning time of any node T t burn(v) ∈N burning time of node v at timestep t T total number of timesteps upto which the game is played r ∈N drone search radius Rt ⊂V set of retardant nodesa at timestep t, i.e., set of nodes that contain the “fire” spread using flame retardant It ⊂V set of visibility nodes at timestep t, i.e., set of nodes that are visible from the deployed drones Dt(v) ∈ {−1, 0, 1} status of a drone deployed on node v at timestep t, (−1 : no drone deployed on node v, 0 : drone is deployed but not activated, 1 : drone action is activated) TABLE I: Notation and Descriptions The state s, where s ∈S and S is the state space of the Cyber-Firefighter game, contains a snapshot of the burning time of visible nodes It, retardant nodes Rt, the status of the drones deployed on all the nodes in the network, and the network graph G. At each timestep t, an action at ∈A can be executed, where A is the action space. Each action at ∈A, is a tuple of two variables, (vt, typet), where vt ∈V and typet denotes the type of action taken and can take any value from {0 : Deploy Drone, 1 : Activate Search, 2 : Place Retardant}. Algorithm 1 presents the Cyber-Firefighter game execution with graph G, fire source vf, drone search radius r, state space S, action space A and a “firefighter” action policy π : S →A as inputs. At each timestep t, the game applies policy π on state st and gets an action as output (line 6). The action type typet is executed for node vt, updating visible I, drone status D(vt), and retardant R nodes in lines 7-19. Then the burn times T t burn(v) for all nodes v ∈V are updated in lines 20-27 and the state is updated to st+1 in line 30. The game terminates when the condition in line 5 is satisfied. Given a network graph G : (V, E) and a fire source vf, the objective of the “firefighter” π is to limit the number of “trees” with the maximum burn time (T max burn) over T timesteps using actions in A. The minimization objective function, M, can be represented as: M ≜min V |{T [0,T ] burn(u) = T max burn, ∀u ∈V }| (1) The action policy π of the “firefighter” selects the actions following Equation (1) (line 6). B. Cyber BT Behaviors The construction of a BT requires a set of behaviors as building blocks. The behaviors are generalizable, i.e., they can be mapped to our abstract environment as well as our realistic cyber-security environment, CybORG. There are a variety of behaviors that are used to construct the BTs [2]. In Learn BT Structure Optimize LECs Deployment Abstract Cyber Environment Realistic Cyber Environment Fig. 3: Robust Autonomous Cyber-Defense EBT Design Approach. Algorithm 1 Cyber-Firefighter(G : (V, E), vf, r, S, A, π) 1: Initialize I0, R0 to ∅and t to 0; 2: Initialize D0(v) to -1, ∀v ∈V ; 3: Initialize T 0 burn(vf) to T max burn; 4: s0 = {T 0 burn(v), ∀v ∈I0, R0, D0(v) ∀v ∈V, G} 5: while t < T || (t > 0 && T t−1 burn(v) = T t burn(v) ∀v ∈V ) do 6: vt, typet = π(st); ▷Select Action 7: if typet == 0 then ▷Deploy Drone 8: if Dt(vt) == −1 then 9: Set: Dt+1(vt) = 0, It+1 = It ∪{vt}; 10: end if 11: else if typet == 1 then ▷Activate Search 12: if Dt(vt) == 0 then 13: Set: Dt(vt) = 1, It+1 = It ∪{Nr(vt)}; 14: end if 15: else if typet == 2 then ▷Place Retardant 16: if vt ∈It && T t burn(vt) ̸= T max burn then 17: Set: Rt+1 = Rt ∪{vt}; 18: end if 19: end if 20: for u ∈V \ Rt do ▷Update Burn Time 21: for v′ ∈N(u) do 22: if T t burn(v′) == T max burn then 23: Set: T t+1 burn(u) = min(T t burn(u)+1, T max burn); 24: Break; 25: end if 26: end for 27: end for 28: B = T t+1 burn(v) ∀v ∈It+1; 29: O = Dt+1(v) ∀v ∈V ; 30: st+1 = {B, Rt+1, O, G}; ▷Update State 31: end while the construction of a BT, each timestep is known as a tick. A BT tick starts from the root behavior and follows a Depth-First Traversal. This traversal can shift depending on the returned status of child behaviors. The behavior(s) can return a status of Failure, Running or Success altering the execution traversal of the BT. Behaviors can be broadly classified into two groups: control behaviors and execution behaviors. Control behaviors are the internal behaviors in a BT that control the logical flow of switching between the behaviors. Execution behaviors are the leaf behaviors in a BT that execute the selected actions in the environment. The control behaviors in the BT can be either Sequences or Fallbacks. Sequences execute a set of child behaviors sequentially until all child behaviors return Success, return Failure otherwise. Fallbacks execute child behaviors until one child behavior returns Success, return Failure otherwise. The execution behaviors in the BT can be either Condition or Action behaviors. The return status of the execution behavior is determined based on its intended logical condition or user-defined functionality. After the status is returned, it propagates back up to the root, recursively updating the status of the parent control behaviors. We define five Action behaviors, to allow the BT to defend against an adversary: 1) SelectStrategy! selects a defense strategy based on an adversarial movement. 2) GetMetaAction! selects one of three defense behaviors using the selected defense strategy. 3) GetDetectorAction! deploys a detection mechanism in the environment to alert a local adversarial activity. 4) GetMitigateAction! prevents an adversary from achiev- ing their objective, such as blocking adversarial move- ment or restoring a network node to a previously “good” state. 5) GetAnalysisAction! monitors or analyzes the environ- ment retrieving new information that is unknown to the agent. Furthermore, there are four condition behaviors. There is one condition behavior for each of the three defense operation behaviors, to ensure a behavior is only enacted when chosen by GetMetaAction!. Additionally, there is a condition behavior for SelectStrategy! to ensure the strategy is only selected or shifted when necessary. To construct BTs for execution in the Cyber-Firefighter environment, we map the defined BT behaviors to actionable behaviors in the environment. The three defense behaviors map to the three “firefighter” actions. Each defense behavior deter- mines a node to deploy a drone (GetDetectorAction!), activate a drone for further visibility analysis (GetAnalysisAction!), or place fire retardant to block the fire spread (GetMitigate- Action!. GetMetaAction! selects which operation to perform using a strategy. In this paper, the strategy defines a state- action policy for choosing operations based on the current state of the fire and visibility of the network. SelectStrategy! determines the appropriate strategy to enact based on the state of the environment. For example, a strategy could be to repeat the process of deploying a drone, activating a drone, or placing a retardant. The behaviors can be arranged through learning or expert knowledge into the BT structure as described in Fig. 4. The root of the BT in Fig. 4 is a Sequence behavior with 7 child behaviors. From left-to-right the behaviors are executed. The first behavior is a Fallback behavior that determines if a new strategy should be selected given two children NotSelectStrategy? and SelectStrategy!. If NotSelectStrategy? returns a status of Failure indicating a strategy should be selected, then the Fallback will execute the second behavior SelectStrategy!. Then the root Sequence will execute the sec- ond child GetMetaAction! to select a cyber-operation behavior to perform. The next 5 child behaviors of the root Sequence relate to selecting and executing the correct cyber-operation behaviors in a similar manner. C. Learning BT Structure using Genetic Programming To learn the optimal structure of the EBT in Fig. 4, we use GP combined with suboptimal baseline BTs. The inputs to the GP are the pre-defined BT nodes for the Cyber-Firefighter environment and a baseline BT that only considers strategy selection. The abstract environment will be used for evaluation and computation of a fitness value for optimizing towards an efficient BT that mitigates the attacker spread on the network. More specifically, the GP algorithm is initialized with the set of execution behavior nodes and control behavior nodes. The BT is defined and generated randomly from these set of behaviors. The intention of the baseline BT is to provide a starting representation to encourage expansion and enhance training efficiency. After initialization, the GP algorithm selects, breeds, and evaluates BTs for a fixed number of generations using a fitness function, ultimately computing a solution that maximizes the fitness. The fitness function, F(x), is the primary component in GP that drives the definition of the optimal BT. We consider the “fire” impact, the visibility of the network, and the size of BT nodes as a fitness function for the Cyber-Firefighter environment. We define the fitness function as F(x) = cv|I| −cl|x| −cf T X t=0 |{v ∈V | T t burn(v) = T max burn}| (2) where |I| is the number of visible nodes in the environment. |x| is the number of BT behaviors in the BT. The “fire” impact is measured by the number of nodes with the maximum burn amount at each timestep t. The coefficients cv, cl, cf are assigned to each term to weigh the impact of each term on overall fitness. Optimizing F(x) maximizes the number of visible nodes and minimizes the number of BT behaviors and the cumulative “fire” impact over T timesteps. Therefore, optimizing toward the maximum F(x) encourages a compact and efficient BT that can gain visibility in the network and mitigate adversarial activity, by reducing “fire” spread, using a minimal number of behaviors. V. ROBUST BTS WITH LECS Behaviors from the abstract cyber environment must be trans- ferred and implemented for the realistic environment, Cy- bORG CAGE Challenge Scenario 2, as shown in Fig. 3. Behaviors that require long-term objectives, such as cyber- defense against unknown attackers over an extended period, benefit from the integration of optimized LECs for robustness and generalizability. There are two LECs for our approach that focus on decision-making for selecting a cyber action behavior and switching strategies. Optimal LECs are then connected to behaviors in the BT structure for deployment in a new structure that we refer to as an Evolving Behavior Tree (EBT). In this section, we first describe the transfer of behaviors to the realistic cyber environment, then derive the optimization techniques and policies for the LECs. At the end of the section we integrate the LECs with the BT structure to form the EBT for deployment. A. Behavior Transfer in Realistic Cyber Environment An optimal BT structure learned in an abstract environment must have its behaviors mapped to the realistic cyber envi- ronment for optimization and deployment. The realistic cyber environment employed in this paper is the CybORG CAGE Challenge Scenario 2. The cyber-defense actions are mapped to their appropriate behaviors. GetAnalysisAction! maps to Analyze and Monitor as they reveal information about the computer network state. GetDetectorAction! maps to a greedy deterministic policy that selects a Deploy Decoy action conditioned on a host node. Furthermore, GetMitigateAction! maps to Remove and Restore because the actions prevent further damage caused by the attacker. To coordinate which of the above behaviors are selected, a cyber-agent controller policy is used that is executed by GetMetaAction! bahavior. Finally, the type of controller policy used is determined using the SelectStrategy! behavior. A strategy switching policy captured by NotSelect- Strategy? behavior determines if the controller policy needs to be switched. B. Optimizing Behaviors Components in the EBT with complex decision-making benefit from optimization techniques for generalizability and adaptation to novel scenarios. Both the cyber-agent controller and the strategy switching will need to be optimized to mitigate complex red agent behavior. The cyber-agent controller contains a set of N learnable policies, [π1, π2, ...πN], for N red agent strategies. Each policy πi has an associated reward ri that maximizes with standard Fig. 4: Learned GPBT Architecture for Strategy Switching RL to mitigate red agent strategy i. πi inputs an observation from the network and outputs a specific cyber-agent. In this paper, r1 = r2 = . . . = rN, as we use the reward provided by CAGE Challenge Scenario 2 [1] which quantifies the disruption to the network caused by red agents. A larger negative value indicates higher disruption. During execution, the cyber-agent controller chooses an action from πi that is associated with the current red agent strategy. We predict red agent strategy i using a strategy switching policy πstrat. πstrat is a temporal policy that inputs a window of observations and outputs a red agent strategy labelled i. The policy is trained using supervised learning from a collection of (observation, strategy label) pairs. Collecting pairs is achieved through using a fully observable strategy switching policy. C. Deployment The optimized LECs can be connected back to their re- spective behaviors for deployment. Behaviors with LECs such as GetMetaAction! are implemented to allow for appropriate querying and functionality the LECs required to construct its policy. This involves identifying key inputs, such as the state of the system, and outputs, such as the meta-action. A behavior can have input and output dependencies on another behavior due to this setup. Behaviors with standard components are implemented in a similar manner for their full intended func- tionality. The fully implemented EBT can then be deployed and executed on the intended realistic cyber-environment. This cyber-environment can either be a simulation with abstractions or an emulation with realistic hardware and computer software components. VI. EVALUATION We perform an evaluation of optimizing the BT structure and LECs of the EBT. We determine if the optimal structure can appropriately mitigate an adversary and gain visibility of the network. For the EBT we determine if it is robust to multiple cyber-attacks compared to a state-of-the-art solution. We track the performance of simulations of Cyber-Firefighter and CybORG CAGE Challenge Scenario 2. We also discuss the explainable nature of the EBT and its role in monitoring the program flow. A. Software Architecture For evaluation, we developed a software architecture, shown in Fig. 5, that constructs and executes an EBT with a computer network simulator. A communication mechanism known as a blackboard [20] is instantiated between the the EBT and the simulator to maintain and update shared data. Similar to a publish-subscribe framework, the EBT and simulator can read or write particular data values based on access permissions. Behaviors in the EBT are restricted to access only variables on the blackboard specific to their functionality to prevent data leaking. To develop the EBT that are used for high- level decision making, we use the PyTrees library [20]. The tree that we construct contains actions for both the switching mechanism and the functionality to take specific actions. For training our autonomous agents, we use the PyTorch framework. We use this to decide which action should be taken based on the observation. The Cyber-Firefighter environment is set up as an Erdos- Renyi (ER) graph [21] with 10 nodes and a probability of 0.2 for edge connection with a constraint that the graph must be connected, otherwise the “fire” could not spread to all nodes. For each simulation a new ER graph is constructed with a fixed fire source, vf = 0, and a fixed maximum burning time, T max burn = 4 timesteps. The simulator we use is CybORG [7], the CAGE Challenge Scenario 2 [1]. The simulator records an observation state, stored as a vector of length 52, containing 4 bits for each of the 13 hosts. Two of these bits encode the type of program being executed on a host and the other two represent the degree to which the host has been compromised. The full action space of cyber actions contains 145 potential actions, however this was reduced to only 52 meaningful actions in our approach. These actions only focus on the operational server, the defender, the enterprise servers 0 −2, and user hosts 1 −4. For each of these servers and hosts, an analyze, remove, or restore action can be performed, in addition to a limited selection of decoys specific to each host. The EBT chooses a cyber action, records it in the blackboard, and the action is then received in CybORG for execution. The blackboard variables are initially assigned from the CybORG setup and then manipulated throughout the execution of the behavior tree. These variables contain information of the state, action, and reward. During execution, the EBT also relies on action functions to perform the low-level actions. Upon completion of behavior tree execution during a single step, the blackboard variables are then written back to the CybORG variables to update the environment. PyTrees CybORG Blackboard Blue EBT Agent Red Attacker Agent Network Topology Fig. 5: EBT and CybORG Software Architecture. A blackboard is used as a communication mechanism between the EBT and CybORG simulation. B. Genetic Programming Setup For evaluation of learning the structure of the EBT, we consider a comparison of multiple BT structures and their respective performance in the abstract environment. The GP algorithm is initialized with 16 BTs and a baseline BT in Fig. 7. The baseline BT is boosted through the inclusion in each generation and preference towards crossover and mutation. In each generation, a BT is represented using a hierarchical list of strings representation, which can then be directly converted to an executable BT for evaluation. The string representation allows for simple operations to update and add new behaviors before execution. The algorithm is executed for 5 trials of 200 generations with approximately 5000 simulation runs (episodes) recorded with the best fitness for each generation stored. We compare the fitness of the GP BTs, the baseline BT, and an expert manually crafted BT. C. GP Performance Fig. 6a shows the GP fitness over 5 training runs of 4000 episodes for the Expert, Baseline, and GPBT. The GPBT fitness (plot in blue) improves from the baseline (plot in green) up to the Expert BT (plot in black). The learned BT structure, computed from the trials, achieves the best fitness of −19.45 (shown in Fig. 4). This is the same fitness as the Expert BT. Therefore, we can conclude that we can learn the structure of a BT to achieve an abstract cyber-security task with performance similar to a BT constructed with expert knowledge. D. Optimizing Behaviors Setup Opposed to the standard scenario, in which the red agent selects one strategy for the entirety of an episode, our scenario sees the RedSwitch agent that changes from a Meander to a BLine strategy. This strategy switching is set to randomly occur between timesteps 10 −30, simulating how an attacker may change their strategy based on information of the network structure that they can quickly obtain. The defender (EBT) interacts with the environment in the architecture in Fig. 5 through the blackboard to determine and adapt from a red strategy change. Two cyber-agent controller policies were trained based on the Meander and BLine strategies using a PPO policy. The policies consist of actor-critic neural networks with the actor and critic networks containing 1 layer of size 64. Both of these policies were trained for 100K observations with 1000 episodes each in a non-deterministic manner. This was done using a multi-layered perceptron developed by the CardiffUni team. Training was done in order to minimize the negative reward received by the blue agent from allowing the red agent to take certain actions and reach various hosts. The blue agent receives a reward of −0.1 per turn for each user and operational host (excluding user host 0) the red agent has access to. A reward of −1 per turn is received for each enterprise and operational server the red agent has access to, or if a blue agent performs a restore operation. If the red agent performs an impact action on the operational server, the blue agent receives a reward of −10 per turn. Since these rewards are negative, an optimal solution should be closer to 0, indicating better success at mitigating a red agent attack. Training rewards over 100K timesteps for the BLine and Meander cyber-agent controller policies are shown in Fig. 6b. The reward is maximized, asymptotically approaching 0, indi- cating the training was successful. Therefore, the policies are prepared for deployment evaluation. An LSTM model was trained in order to carry out the strategy switching. This model was trained successfully with a window size of 5 over 5 epochs (3000 optimization iterations) with 100k observations (1000 episodes) using supervised Binary Cross Entropy (BCE) loss. Observations are collected using an oracle that correctly labels each strategy, Meander or BLine. The supervised loss is used to optimize the predicted label of the strategy from the LSTM with the correct labelling from the oracle using a window of observations. The LSTM model consists of a neural network with 2 layers of size 100. The learning rate used was 1e −3. BCE training loss for 3000 iterations over 5 training runs is shown in Fig. 6c. A supervised approach was taken in which the model was aware when the red agent switched strategies in order to maximize the efficacy of the training. Similar to training the PPO models, these agents were trained to minimize the reward received. We first evaluate that our cyber-agent controller policies for Meander and BLine integrated into the GPBT structure. The original CardiffUni solution is compared against our EBT without considering strategy switching. We evaluate using the cumulative reward over episodes for 1000 episodes for both Meander and BLine. Then, we evaluate multiple EBT 0 500 1000 1500 2000 2500 3000 3500 4000 Episodes −100 −90 −80 −70 −60 −50 −40 −30 −20 Fitness Expert Baseline GP (a) GP Fitness 0.0 0.2 0.4 0.6 0.8 1.0 Timestep ×105 −300 −250 −200 −150 −100 −50 0 Reward Bline Meander (b) Cyber-Agent Controller Rewards 0 500 1000 1500 2000 2500 3000 Iteration 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 BCE Loss (c) Strategy-Switch Policy Loss Fig. 6: Training evaluation for learning structure and optimizing LECs. The shaded region in (a) and (c) represents the standard deviation over 5 training runs. Fig. 7: Baseline BT considering only strategy switching solutions in our BT structure (GPBT) in order to compare their performance in our scenario. Every solution begins by scanning the network over the first three timesteps to determine the initial red agent strategy. • CardiffUni: The base solution with no strategy switching mechanism; once it determines the initial strategy (Me- ander), it retains it for the duration of the episode. • OracleSwitch: Operates as an oracle that gets informa- tion directly from the simulation on the exact timestep the RedSwitch agent changes strategies during an episode. • LearnedSwitch: (Ours) Uses the trained LSTM model to determine if the red agent has switched strategies based on its current behavior. LearnedSwitch uses OracleSwitch during training. E. Attacker Strategy Robustness In Figs. 8a and 8b, the cumulative reward over is plotted for CardiffUni and the EBT, recorded for 1000 episodes. The EBT (blue) achieves consistent results with the original CardiffUni approach (red). The EBT (blue) performs slightly below CardiffUni (red) on average over 1000 episodes. This is likely attributed to minimal overhead by BT execution. Therefore, integrating LECs into the GPBT structure to develop an EBT still achieves consistent performance with the state-of-the-art CardiffUni approach. Fig. 8c, displays the cumulative reward over time recorded for 1000 episodes, comparing CardiffUni, OracleSwitch, and LearnedSwitch GPBTs. We found that our LearnedSwitch solution (green) shows a significant improvement of around 0 20 40 60 80 100 Timestep −20.0 −17.5 −15.0 −12.5 −10.0 −7.5 −5.0 −2.5 0.0 Cumulative Reward CardiﬀUni (Original) EBT (GPBT) (a) Meander 0 20 40 60 80 100 Timestep −20.0 −17.5 −15.0 −12.5 −10.0 −7.5 −5.0 −2.5 0.0 Cumulative Reward CardiﬀUni (Original) EBT (GPBT) (b) BLine 0 20 40 60 80 100 Timestep −35 −30 −25 −20 −15 −10 −5 0 Cumulative Reward CardiﬀUni GPBT LearnedSwitch GPBT OracleSwitch GPBT (c) Strategy Switching Fig. 8: Cumulative Reward over time, recorded for 1000 episodes. In (a) and (b), the EBT with LECs is compared to original approach with LECs. In (c) varying degrees of strategy switching are compared. Mean and Standard Deviation are plotted. 39% over the CardiffUni solution (red). The CardiffUni solu- tion managed an average cumulative reward of −32.03, while our LearnedSwitch solution achieved an average cumulative reward of −19.56. This indicates that a strategy switch for the cyber agent is imperative to properly defend against a red agent that can switch strategies; solely detecting its strategy at the beginning of an episode is not sufficient. It should be noted that this solution did not see a noticeable drop in performance in the range of timesteps in which the red agent could change strategies. This highlights our solution as being resilient to a sudden change in strategy, as it can rapidly detect and adapt to it. Additionally, the OracleSwitch solution (blue) resulted in an average cumulative reward of −17.84. This slightly outperforms the LearnedSwitch solution, which is expected due to its immediate knowledge of a strategy switch. However, the reward between the two is comparable. F. Explainability The CardiffUni solution used a simple sequential approach in carrying out the actions during each episode. However, we employed an EBT in our solutions, as we believe that it im- proves evaluation. The EBT precisely explains the operations of the simulation during each episode and acts as a runtime monitor for high-level behaviors. In particular, it allows us to visually determine when certain events are occurring, such as adapting to a new strategy or deploying a decoy. Visualization is achieved through the built-in functionality of PyTrees to render BT execution [20]. This provides us with an interface for following the execution of the components throughout the runtime of the EBT. Also, the EBT simplifies the task of performing the strategy switch during an episode. It includes designated nodes to evaluate the current red agent strategy and then switch its own strategy, if needed. These actions are explicitly independent as illustrated in the EBT, both from each other and from other high-level actions. This overall assists in improving control flow of the program. VII. CONCLUSIONS & FUTURE WORK In this work, we describe an approach for designing long- term autonomous cyber-defense agents enabled by Evolving Behavior Trees (EBTs). We utilize a GP algorithm and develop a novel abstract cyber environment to learn the high-level structure of the EBT. Then, we optimize the EBT in a realistic cyber environment with LECs for reactive strategy switching to mitigate complex and dynamic cyber-attacks. The structure of the EBT is modular and generalizable to autonomous cyber- defense in a network. The performance of learning the struc- ture is evaluated in the abstract cyber environment where we demonstrate how the structure learned will promote visibility and mitigate an attack. We then evaluate the integration of LECs in the EBT in the CybORG simulation environment using CAGE Challenge Scenario 2. Furthermore, we develop a software architecture to integrate the EBT with CybORG. Our results demonstrate that our proposed approach against the attacker strategy switching shows a 39% improvement in average reward compared to a state-of-the-art approach, and provides explainability for use in runtime monitoring of key events. Our future works will focus on scalability, emulation, and expansion to new attack scenarios. We aim to expand our set of available defense strategies to cover a multitude of attacks. We will also research methods to train individual policies that can generalize to a subset of red agent strategies. Additionally, we seek to evaluate our approach in an emulation environment to demonstrate the ability to deploy our solution in real network systems. REFERENCES [1] “Cyber autonomy gym for experimentation challenge 2,” https:// github.com/cage-challenge/cage-challenge-2, 2022, created by Maxwell Standen, David Bowman, Son Hoang, Toby Richer, Martin Lucas, Richard Van Tassel, Phillip Vu, Mitchell Kiely. [2] M. Colledanchise and P. ¨Ogren, Behavior Trees in Robotics and AI. CRC Press, jul 2018. [3] M. Iovino, J. Styrud, P. Falco, and C. Smith, “Learning behavior trees with genetic programming in unpredictable environments,” in IEEE International Conference on Robotics and Automation (ICRA), 05 2021, pp. 4591–4597. [4] J. Styrud, M. Iovino, M. Norrl¨of, M. Bj¨orkman, and C. Smith, “Com- bining planning and learning of behavior trees for robotic assembly,” in International Conference on Robotics and Automation (ICRA), 05 2022, pp. 11 511–11 517. [5] A. Lykov and D. Tsetserukou, “Llm-brain: Ai-driven fast generation of robot behaviour tree based on large language model,” arXiv preprint arXiv:2305.19352, 2023. [6] Y. Cao and C. S. G. Lee, “Robot behavior-tree-based task generation with large language models,” ArXiv, vol. abs/2302.12927, 2023. [7] CybORG: A Gym for the Development of Autonomous Cyber Agents, 2021. [8] M. Kiely, D. Bowman, M. Standen, and C. Moir, “On autonomous agents in a cyber defence environment,” arXiv preprint arXiv:2309.07388, 2023. [9] M. Foley, C. Hicks, K. Highnam, and V. Mavroudis, “Autonomous network defence using reinforcement learning,” in Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security, ser. ASIA CCS ’22. New York, NY, USA: Association for Computing Machinery, 2022, p. 1252–1254. [10] M. Wolk, A. Applebaum, C. Dennler, P. Dwyer, M. Moskowitz, H. Nguyen, N. Nichols, N. Park, P. Rachwalski, F. Rau, and A. Web- ster, “Beyond cage: Investigating generalization of learned autonomous network defense policies,” 2022. [11] A. Molina-Markham, C. Miniter, B. Powell, and A. Ridley, “Network environment design for autonomous cyberdefense,” 2021. [12] E. Bates, V. Mavroudis, and C. Hicks, “Reward shaping for happier autonomous cyber security agents,” in Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security, ser. AISec ’23. New York, NY, USA: Association for Computing Machinery, 2023, p. 221–232. [13] B. Jalaian and N. D. Bastian, “Neurosymbolic ai in cybersecurity: Bridging pattern recognition and symbolic reasoning,” in MILCOM 2023 - 2023 IEEE Military Communications Conference (MILCOM), 2023, pp. 268–273. [14] B. Banerjee, “Autonomous acquisition of behavior trees for robot con- trol,” in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018, pp. 3460–3467. [15] F. Lundberg, “Evaluating behaviour tree integration in the option critic framework in starcraft 2 mini-games with training restricted by consumer level hardware,” 2022. [16] L. Li, L. Wang, Y. Li, and J. Sheng, “Mixed deep reinforcement learning- behavior tree for intelligent agents design,” in International Conference on Agents and Artificial Intelligence, 2021. [17] P.-L. Bacon, J. Harb, and D. Precup, “The option-critic architecture,” in Proceedings of the AAAI conference on artificial intelligence, vol. 31, no. 1, 2017. [18] N. Potteiger and X. Koutsoukos, “Safe explainable agents for au- tonomous navigation using evolving behavior trees,” in 2023 IEEE International Conference on Assured Autonomy (ICAA), 2023, pp. 44– 52. [19] A. Bonato, An invitation to pursuit-evasion games and graph theory. American Mathematical Society, 2022, vol. 97. [20] S. Reality, “Pytrees,” https://github.com/splintered-reality/py trees, 2023. [21] P. Erd˝os, A. R´enyi et al., “On the evolution of random graphs,” Publ. math. inst. hung. acad. sci, vol. 5, no. 1, pp. 17–60, 1960.