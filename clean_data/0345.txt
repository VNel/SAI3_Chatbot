Mathematical Modeling of Cyber Resilience Alexander Kott, Michael J. Weisman U.S. Army Combat Capabilities Development Command Army Research Laboratory Adelphi, MD {alexander.kott1, michael.j.weisman2}.civ@army.mil Joachim Vandekerckhove University of California, Irvine Department of Cognitive Sciences Irvine, CA joachim@uci.edu Abstract— We identify quantitative characteristics of responses to cyber compromises that can be learned from repeatable, systematic experiments. We model a vehicle equipped with an autonomous cyber-defense system and which also has some inherent physical resilience features. When attacked by malware, this ensemble of cyber-physical features (i.e., “bonware”) strives to resist and recover from the performance degradation caused by the malware’s attack. We propose parsimonious continuous models, and develop stochastic models to aid in quantifying systems’ resilience to cyber attacks. I. INTRODUCTION Resilience continues to gain attention as a key prop- erty of cyber and cyber-physical systems, for the pur- poses of cyber defense. Although deﬁnitions vary, it is generally agreed that cyber resilience refers to the ability of a system to resist and recover from a cyber compro- mise that degrades the mission-relevant performance of the system [1]. Resilience should not be conﬂated with risk or security [2]. To make the discussion more concrete, consider the example of a military ground logistics vehicle, possibly unmanned, which performs a mission of delivering heavy supplies along a difﬁcult route. The adversary’s malware successfully gains access to the Controller Area Net- work (CAN bus) of the vehicle [3]. Then, the malware executes cyber attacks by sending a combination of messages intended to degrade the vehicle’s performance and diminish its ability to complete its delivery mission. We assume that the malware is at least partly successful, and the vehicle indeed begins to experience a degradation of its mission-relevant performance. At this point, we expect the vehicle’s resilience- relevant elements to resist the degradation and then to The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofﬁcial policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to repro- duce and distribute reprints for Government purposes notwithstanding any copyright notation herein. This work was partially funded by Cyber Technologies, Deputy CTO for Critical Technologies/Applied Technology, Ofﬁce of the Under Secretary of Defense Research and Engineering. Dr. Vandekerckhove’s research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-21-2-0284. recover its performance to a satisfactory level, within an acceptably short time period. These “resilience-relevant elements” might be of several kinds. First, because the vehicle is a cyber-physical system, certain physical char- acteristics of the vehicles mechanisms will provide a de- gree of resilience. For example, the cooling system of the vehicle will exhibit a signiﬁcant resistance to overheating even if the malware succeeds in misrepresenting the temperature sensors data. Second, appropriate defensive software residing on the vehicle continually monitors and analyzes the information passing through the CAN bus [4]. When the situation appears suspicious, it may take actions such as blocking or correcting potentially malicious messages. Third, it is possible that a remote monitoring center, staffed with experienced human cyber defenders, will detect a cyber compromise and will provide corrective actions remotely [5]. For the purposes of this paper, we assume that the remote monitoring and resilience via external interven- tion is impossible [6]. This may be the case if the vehicle must maintain radio silence for survivability purposes, or if the malware spoofs or blocks communication channels of the vehicle. Therefore, in this paper we assume that re- silience is provided by the ﬁrst two classes of resilience- relevant elements. Here, by analogy with malware, we call these “bonware” – a combination of physical and cyber features of the vehicle that serve to resist and recover from a cyber compromise. A key challenge in the ﬁeld of cyber resilience is quan- tifying or measuring resilience. Indeed, no engineering discipline achieved signiﬁcant maturity without being able to measure the properties of phenomena relevant to the discipline [5]. Developers of systems like the notional vehicle in our example must be able to quantify the resilience of the vehicle under development in order to know whether the features they introduce in the vehicle improve its cyber resilience, or make it worse. Similarly, buyers of the vehicle need to know how to quantitatively specify and test resilience in order to determine whether the product meets their speciﬁcations. In this paper, we report some of the results of a project called Quantitative Measurement of Cyber Resilience arXiv:2302.04413v3 [cs.CR] 28 Feb 2023 (QMoCR) in which our research team seeks to identify quantitative characteristics of systems’ responses to cy- ber compromises that can be derived from repeatable, systematic experiments. Brieﬂy, we have constructed a test-bed in which a surrogate vehicle is subjected to controlled cyber attacks produced by malware. The vehicle is equipped with an autonomous cyber-defense system [6, 4] and also has some inherent physical resilience features. This ensemble of cyber-physical fea- tures (i.e., “bonware”) strives to resist and recover from the performance degradation caused by the malware’s attack. The test bed is instrumented in such a way that we can measure observable manifestations of this battle between the malware and bonware, especially the mission-relevant performance parameters of the vehicle. The details of the test bed and the experiment are given in a companion paper [7]. The focus of this paper is different -– here we concentrate on construct- ing mathematical models that can be used to describe the dynamics of the malware-bonware battle. We seek models that are parsimonious in the number of empirical parameters and allow us to easily derive parameters of the model from experimental data. The remainder of the paper is organized as follows. In the next section, we brieﬂy describe prior work related to quantiﬁcation of cyber resilience. We provide formal deﬁnitions of accomplishment and functionality. We propose a class of parsimonious models in which effects of both malware and bonware are approximated as deterministic, continuous differentiable variables, and we explore several variations of such models. In the following section we propose a different class of models – stochastic models, and we show how this class is related to the previously proposed class of determin- istic models. Then we show how these models are used to approximate experimental data obtained with our surrogate vehicle. We show how to determine the parameters of the models from experimental data. We discuss whether these parameters might be considered quantitative characteristics (i.e., measurements) of the bonware’s cyber resilience. II. PRIOR WORK A growing body of literature explores quantiﬁcation of resilience in general and cyber resilience in particular. Very approximately, the literature can be divided into two categories: (1) qualitative assessments of a system (actually existing or its design) by subject matter experts (SMEs) [8], [9] and (2) quantitative measurements based on empirical or experimental observations of how a system (or its high-ﬁdelity model; [10]) responds to a cyber compromise [1, 11]. In the ﬁrst category, a well- cited example is the approach called the cyber-resilience matrix [12]. In this approach, a system is considered as spanning four domains: (1) physical (i.e., the physical resources of the system, and the design, capabilities, features and characteristics of those resources); (2) in- formational (i.e., the system’s availability, storage, and use of information); (3) cognitive (i.e., the ways in which informational and physical resources are used to compre- hend the situation and make pertinent decisions); and (4) social (i.e., structure, relations, and communications of social nature within and around the system). For each of these domains of the system, SMEs are asked to assess, and to express in metrics, the extent to which the system exhibits the ability to (1) plan and prepare for an adverse cyber incident; (2) absorb the impact of the adverse cyber incident; (3) recover from the effects of the adverse cyber incident; and (4) adapt to the ramiﬁcations of the adverse cyber incident. In this way, the approach deﬁnes a 4- by-4 matrix that serves as a framework for structured assessments by SMEs. Another example within the same category (i.e., qual- itative assessments of a system by SMEs) is a recent, elaborate approach proposed by [13]. The approach is called Framework for Operational Resilience in Engi- neering and System Test (FOREST), and a key method- ology within FOREST is called Testable Resilience Efﬁ- cacy Elements (TREE). For a given system or subsystem, the methodology requires SMEs to assess, among others, how well the resilience solution is able to (1) sense or discover a successful cyber-attack; (2) identify the part of the system that has been successfully attacked; (3) reconﬁgure the system in order to mitigate and contain the consequences of the attack. Assessment may include tests of the system, although the methodology does not prescribe the tests. Undoubtedly, such methodologies can be very valu- able in ﬁnding opportunities in improvements of cyber- resilience in a system that is either at the design stage or is already constructed. Still, these are essentially qualitative assessments, not quantitative measurements derived from an experiment. In the second category (i.e., quantitative measurements based on empirical or experimental observations of how a system, or its high-ﬁdelity model, responds to a cyber compromise), most approaches tend to revolve around a common idea we call here the area under the curve (AUC) method [14, 15]. In an experiment/test, a system is engaged into a performance of a representative mis- sion, and then is subjected to an ensemble or sequence of representative cyber attacks. A mission-relevant quantita- tive functionality of the system is observed and recorded. The resulting average functionality, divided by normal functionality, can be used as a measure of resilience. However, AUC-based resilience measures are inher- ently cumulative, aggregate measures, and do not tell us much about the underlying processes. For example, is it possible to quantify the resilience impact of the bonware of the given system? Similarly, is it possible to quantify the impact of malware? In addition, is it possible to gain insights into how these values of impactfulness vary over time during an incident? We offer steps toward answering such questions. III. QUANTITATIVE MEASURES OF CYBER RESILIENCE Every mission has a goal, and we postulate that for a given mission, there exists a function A(t) that represents accomplishment and is cumulative from the mission start time up until the present time t. We deﬁne functionality, F(t), to be the time derivative of mission accomplishment. Thus, F(t) = dA dt , A(t) = Z t t0 F(τ) dτ. (1) The normal functionality, when the system performs normally and does not experience effects of a cyber attack, may, in general, vary with time. For simplicity, throughout this paper, we assume the normal function- ality to be constant in time, FN(t) = FN. Thus, the functionality of our system prior to an attack or other malfunction is normal at the start time: F(t0) = FN. In the following sections we develop models for the behavior of a system’s functionality over the course of a mission during which it is being attacked by malware and defended by bonware. In the ﬁrst set of models, we assume that there is an observable, sufﬁciently smooth function representing mission accomplishment, and we deﬁne functionality to be its time derivative. Then, we motivate a parsimonious model for the differential equation governing functional- ity, give the general solution, and discuss a few speciﬁc cases. We then develop a stochastic model and show its relationship to the continuous model. Finally, we show, both analytically and empirically, that with sufﬁciently many instantiations, the average of an ensemble of stochastic curves will approximate the solution to the continuous model differential equation. IV. CONTINUOUS MODEL For the ﬁrst set of models, we make the assumption that mission accomplishment is twice continuously dif- ferentiable: A ∈C2, and thus F ∈C1. As a ﬁrst approximation, we let the impact of malware on the derivative of functionality be linear. The impact of bonware is similarly deﬁned and proportional to the level of functionality below normal. Malware degrades the system while bonware aims to increase functionality over time. Malware impact and bonware impact are assumed to be continuous functions of time, M, B ∈C0. The impact on functionality is the sum of the impacts of malware and bonware, and dF dt + Q(t)F(t) = FNB(t), (2) where Q(t) = M(t)+B(t). Since we expect bonware to help (or at least not harm) and malware to not help, we assume B(t) ≥0 and M(t) ≥0. We also assume normal functionality is positive, F0 > 0, and functionality is always nonnegative and less than or equal to normal functionality, 0 ≤F(t) ≤FN. This ﬁrst-order linear differential equation has the following solution: F(t) = e− R t 0 Q(p) dp  F(0) + FN Z t 0 e R τ 0 Q(p) dpB(τ) dτ  . (3) To help us understand how the model works, we ﬁnd explicit solutions for a number of examples. A. Constant model Assuming M, B, and Q are constant, we have dF dt + QF(t) = FNB. (4) 1) No bonware: If B = 0, then Equation 4 reduces to dF dt + MF(t) = 0 and F(t) = F(0)e−Mt. If also M = 0 (no bonware and no malware), then dF dt = 0 and F(t) = F(0). 2) Bonware: With bonware present, the solution is F(t) =  F(0) −FNB Q  e−Qt + FNB Q . (5) 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0 Time [seconds] F(t)/FN Functionality: F(t)/FN ∈{0.0,0.5,1.0} M =0, B=1 M =.5, B=1 M =1, B=1 M =1, B=.5 M =1, B=0 Figure 1. Normalized functionality, F(t)/FN, is shown for various values of M (malware attacking) and B (bonware defending) and at initial conditions F(0)/FN ∈{0.0, 0.5, 1.0}. The functionality over time depends on the relative strengths of bonware and malware and on the intial condition. When the system initially is at normal functionality and when malware overpowers bonware, functionality exhibits exponential decay. When functionality initially is low, and when bonware overpowers malware, the system recovers (via Eq. 5) to FNB M+B . If F(0) > FNB/Q, then F(t) will initially, at time t = 0, be at F(0) and decrease to FNB/Q. If F(0) > FNB/Q, then the function F(t) = F(0) will be constant. If F(0) < FNB/Q, the function will start at F = F(0) and increase to FNB/Q. Examples of these situations are shown in Figure 1. The plot of (M > 0) in Figure 1 shows that even in the presence of bonware, malware will still have an impact on the system. The steady-state Notional Data F [t ] 0 20 40 60 80 100 120 0.0 0.2 0.4 0.6 0.8 1.0 Time [seconds] Functionality: Notional Data and Fit F(t) Figure 2. The smooth line is an example functionality curve with piecewise constant malware and bonware impacts. The notional data and piecewise constant model ﬁt are described below in Section VI-A. of the system is obtained either by setting dF dt = 0 in Equation 4 or letting t →∞: F∞= lim t→∞F(t) = FN B M + B (6) so that the antidote to malware is to overwhelm it with bonware. The exponent, −Qt = (−M −B)t in the solution given by Equation 5 indicates that increasing the impact of either malware or bonware will cause the system to more quickly approach steady-state. At steady- state, FN −F∞ F∞ = M M + B . (7) Equation 7 gives us further insight into the trade-off between impacts of both malware and bonware. The rel- ative decrease of the function from normal functionality is equal to the ratio of malware impact to the sum of malware and bonware impacts. B. Piecewise constant model If either malware’s or bonware’s impact diminishes at some point in the incident, the model may switch from one set of constants deﬁning malware and bonware to another set of constants. The differential equation (Eq. 2) may now be expressed as dF dt = N−1 X j=0 (FN −F(t))Bj(t) −F(t)Mj(t), (8) where the vectors M = (M0, M1, · · · MN−1) and B = (B0, B1, · · · , BN−1) contain the malware impact and bonware impacts within time windows whose end points are deﬁned by {t0, t1, · · · , tN}. The solution will be a function which, in each time interval, is the solution found in Equation 5. The purple curve in Figure 2 is a realization of this model. C. Linear model The impacts of malware and bonware may also be linear functions of t, so that M(t) = ν −µt, B(t) = α −βt, and Q(t) = λ −ωt, where λ = α + ν and ω = β+µ. Under this linear model, Equation 2 becomes dF dt + (λ −ωt)F(t) = FN(α −βt) (9) B = 0.+0.04t B = 0.2+0.04t B = 0.4+0.04t B = 0.6 +0.04t B = 0.8+0.04t 0 5 10 15 20 0.0 0.2 0.4 0.6 0.8 1.0 Time [seconds] F(t)/FN M = max(0.5-0.1t ,0) Figure 3. Normalized functionality, F(t)/FN, for piecewise linear models. Impacts of malware and bonware are linear functions of time. The solution can be expressed in terms of the error function erf(z) = 2 √π R z 0 e−τ 2 dτ: F(t) FN = 1 Ω(t) F(0) FN −β ω (1 −Ω(t)) + (αω −βλ) × p π 2 eΛ2 ω3/2  erf (Λ) + erf  ωt √ 2ω −Λ ) (10) where Ω(t) = eλt−1 2 ωt2, and Λ = λ/ √ 2ω. D. Piecewise linear model Both malware and bonware impacts may initially be linear, but if the situation changes and a different linear model holds after a time, the model should be able to account for it. In particular, if malware impact is decreasing over time, at some point we will reach M = 0 and the model switches to a new linear model. Equation 9 can be written dF dt = N−1 X j=0 [(λj −ωjt)F(t) −FN(αj −βjt)] . The solution follows from Equation 10. Example real- izations of the piecewise linear models are shown in Figure 3. The shapes of the curves resemble experimental data discussed in [7]. V. STOCHASTIC DIFFERENTIAL EQUATION MODEL In this section, we extend the previously deﬁned dif- ferential equation (DE) model to a stochastic differential equation (SDE) model. The extension is motivated by the discontinuous nature of the notional data in Figure 2. Whereas the DE model assumed a smooth functionality curve, our stochastic version allows for a more punctu- ated attack-and-restoration pattern. In the SDE model, both malware and bonware may be active at random (or a priori unknown) times with random (or a priori unknown) effectiveness. Let malware activity Am(t) ∈{0, 1} indicate whether malware was successful at time t, bonware activity Ab(t) ∈{0, 1} indicate whether bonware was successful at time t, mal- ware effectiveness Em(t) ∈[0, 1) express the proportion of functionality reduced by the malware’s success at time t, and bonware effectiveness Eb(t) ∈[0, 1) express the proportion of damage undone by the bonware’s success at time t. The SDE analog to Equation 2 is then dF dt = (FN −F(t)) Ab(t)Eb(t) −F(t)Am(t)Em(t). (11) Rather than changing deterministically over time, these model parameters are assumed to vary stochasti- cally according to these distributions: Am(t) ∼ Bern(θm(t)) , (12) Ab(t) ∼ Bern  θb(t)  , (13) Em(t) ∼ Unif(0, γm(t)) , (14) Eb(t) ∼ Unif  0, γb(t)  , (15) where Bern(θ) indicates the Bernoulli distribution with rate θ and Unif(0, γ) indicates a uniform distribution with lower bound 0 and upper bound γ. Hence, θm(t) ∈ [0, 1] is the probability that malware is successful at time t, θb(t) ∈[0, 1] is the probability that bonware is suc- cessful at time t, γm(t) ∈(0, 1] is the maximum fraction of damage inﬂicted by malware, and γb(t) ∈(0, 1] is the maximum fraction of damage undone by bonware. Like the ordinary differential equation (ODE) model, the SDE model allows for a number of interesting variants. In the remainder of this section, we introduce some useful simpliﬁcations and extensions. A. Constant parameters In the simplest version of the SDE model, we assume that the rate of malware attacks and their maximum efﬁciency are constant in time, and that the rate of bonware restoration and its maximum efﬁciency are both constant in time. Speciﬁcally, under this model it is assumed that θm(t) = θm, θb(t) = θb, γm(t) = γm, and γb(t) = γb (cf. Eqs. 12–15). This version of the model is parsimonious, with only four free parameters, each with a useful interpretation. B. Piecewise constant parameters In a ﬁrst extension of the SDE model, we assume that malware and bonware have an activity rate of 0 until they activate at time points tm and tb, respectively. After activation, both activity rates are constant. Additionally, both efﬁciency parameters are assumed to be constant in time. Speciﬁcally, θm(t) = θmu(t −tm), θb(t) = θbu  t −tb , γm(t) = γm, and γb(t) = γb. Here u(·) is the unit step function. The piecewise constant formulation here is particu- larly apposite for our experiments, in which the timing of malware attacks is known. The analyst can then choose between considering tm and tb as known, and retain the parsimony of a four-parameter model, or considering them unknown and estimate them from data in a six- parameter model. The difference between the known onset time of an attack and the estimated tm may then be interpreted as the time it takes for an attack to take effect. Similarly, the difference between that onset time and the estimated tb may be interpreted as the delay until bonware begins to restore functionality after an attack. C. Parameter expansion of the SDE model The SDE model can be conveniently stated as a hidden Markov model and implemented as a directed acyclic graph [16] for efﬁcient parameter estimation with a general-purpose Bayesian inference engine (e.g., JAGS; [17]). Our implementation relied on a sequential deﬁni- tion for the likelihood function: (F(t + 1) | F(t), . . .) ∼ Unif(L(t), U(t)), with L(t) = F(t)−Am(t)Em(t)F(t) and U(t) = F(t) + Ab(t)Eb(t) (1 −F(t)). Since this likelihood function depends on the unknown stochastic parameters Am(t), Ab(t), Em(t), and Eb(t), we applied a parameter expansion approach [18, 19] using Equations 12–15. D. Relationship between continuous and SDE model With the parameters of the stochastic model selected appropriately, we show that as the number of stochastic realizations increases, the expectation of the solution to the stochastic differential equation model approaches that of the ODE model. We show this for the simple constant parameter case. The general result follows by extension. Theorem. Let ym k ∼Bern(2M), yb k ∼Bern(2B), zm k ∼Unif(0, Fk), zb k ∼Unif(0, FN −Fk), and Fk+1 = Fk −ym k zm k + yb kzb k, (k = 1, . . . , K). (16) Let Fkn = Fkj n , (j = 1, . . . , n), then Fk = E(Fk) = limn→∞Fkn and Fk ≈F(k), for large k, where F(t) is the solution to the initial value problem given by Equation 4 with F(0) = F0. Proof. Take the expectation of Equation 16. Then Fk −Fk−1 + (M + B)Fk−1 = FNB. With F0n = F0, the solution is Fk =  F0 −FNB Q  (1−Q)k + FNB Q , which approximates Equation 5 for large k. VI. AN APPLICATION OF THE MODEL A. Obtaining model parameters Given notional data that represents a typical curve of functionality over the course of an incident where malware and bonware are active, we develop a fast method to estimate the continuous model parameters for a curve that approximates the data, and use these parameters to generate further realizations based on this model. In Figure 2 an example of such notional data is plotted (in light blue). In this section, we illustrate our method to extract the model parameters from this curve. The set P = {t0, . . . , tK} partitions the mission timeline and malware and bonware are constant in each interval (ti−1, ti), i = 1, . . . , K. In each interval, Qi = Mi + Bi and the differential equation governing Continuous Model I is dF (t) dt +QiF(t) = FN(t)Bi. Thus, in each interval (ti−1, ti), the solution is F(t) =  F(ti−1) −FNBi Qi  e−Qi(t−ti−1) + FNBi Qi . We compute the effectiveness, Em, and activity, Am, of malware and of bonware (Eb, Ab), in each interval: Mi = Em i Am i , Bi = Eb i Ab i. We observe that there is a unique switching time t⋆ where the functionality’s trend reverses, and thus we take K = 2. Before the switch, the impact of malware is greater than that of bonware. From the time of the switch until the end of the mission, bonware is stronger. To estimate the switching time t⋆, we ﬁnd the minimum of the data to occur over the interval from 64 s to 75 s. There, the minimum value of the data curve is m = 0.27. Taking the midpoint, our estimate for t⋆ is 69.5 s. We estimate the activity of malware before switching to be the number of times the data curve decreases divided by the switching time. Similarly, our estimate of bonware activity is the number of times the data curve increases prior to the switching time. We thus have Am 1 ≈ 7/69.5 ≈0.101 and Ab 1 ≈2/69.5 ≈0.014. To determine the remaining parameters, we numeri- cally solve this system of equations: αm = FN B1 Q1 , m = F(0) −FN B1 Q1 e−Q1t⋆+ FN B1 Q1 . The ﬁrst equation says that where the curve meets the minimum of the data, it has experienced exponential decay of α toward the asymptotic minimum. We take α to be α = 1 −1/e. The second equation says that the minimum occurs at the switching time (the time when the model switches from malware dominating bonware, to bonware dominating malware. Solving this system of equations yields (with M1 = Q1 −B1), M1 ≈0.025 and B1 ≈0.005, so that Em 1 = M1/Am 1 ≈0.503 and Eb 1 = B1/Ab 1 ≈0.362. To the right of t⋆, we ﬁt an exponentially increas- ing function. Similar to before the switching time, we compute the activities of the malware and bonware: Am 2 ≈ 1 100−69.5 ≈0.033 and Ab 2 ≈ 4 100−69.5 ≈0.131. To determine the remaining parameters, we numeri- cally solve this system of equations: ζ = F(0)B2 Q2 , ˜αζ =  m −FNB2 Q2  e−Q2(125−t⋆) + FNB2 Q2  . 0 20 40 60 80 100 120 0.0 0.2 0.4 0.6 0.8 1.0 Time [seconds] Fk Stochastic Approximation: 5 Realizations n = 5 n = 50 n = 500 n = 5,000 0 20 40 60 80 100 120 0.0 0.2 0.4 0.6 0.8 1.0 Time [seconds] Fkn Stochastic Approximation Averaged Over n Runs Figure 4. (Top) Five realizations of the stochastic model generated with the parameters obtained by ﬁtting the notional data shown in Figure 2. Each realization is different but each roughly follows an exponential decay when t < t⋆= 69.5 s and an exponential recovery for t ≥t⋆. (Bottom) Averages of n stochastic runs for n ∈{5, 50, 500, 5000}. As n increases, the average of the ensemble approaches the ﬁtted curve as predicted in the theorem of Section V-D. We have found that ˜α = 1 −e−4 and ζ = 0.95 are satisfactory values to use for these hyperparameters. We compute M2 ≈0.005 and B2 ≈0.088, so that Em 1 = M1/Am 1 ≈0.201 and Eb 1 = B1/Ab 1 ≈0.957. B. Generating stochastic realizations Using the parameters found in Section VI-A, we can now generate stochastic realizations. In Section V-D, we showed that for large sample sizes, the average of our ensemble will approach the solution to the continuous ODE model. We show empirically that this is indeed the case. To illustrate, we generated ﬁve realizations (Figure 4) of the stochastic model with parameters found from the notional data of Section VI-A. By averaging n curves when n ∈{5, 50, 500, 5000} we see how the ensemble average approaches the solution of the corresponding differential equation as predicted in the Theorem of Section V-D. VII. DISCUSSION AND CONCLUSION We have presented a broadly applicable framework for the analysis of the cyber resilience of military artifacts. Our framework relies on the construction of a custom differential equation time series model that shows good qualitative correspondence to the functionality of ve- hicles performing missions. Seeking to move beyond the use of area-under-the-curve quantiﬁcations of cyber resilience, our proposed models have the advantage that their parameters have domain-relevant interpretations such as the activity of malware and the effectiveness of bonware. Such interpretable parameters can provide a more nuanced interpretation of cyber resilience data being experimentally obtained in our lab. Our formal models come in two families with com- plementary advantages. A series of continuous models is parsimonious, mathematically convenient, and easy to ﬁt. A series of discrete, stochastic models shows greater verisimilitude but is slightly less parsimonious and requires more computationally onerous parameter estimation techniques. Both types of models can be extended to a large variety of custom circumstances, including the case where model parameters change gradually, abruptly, or predictably as a result of experimental manipulation. Future work will include an extension to the cases of multiple simultaneous objectives and to the case of multiple vehicles to be analyzed jointly. REFERENCES [1] A. Kott and I. Linkov. Cyber resilience of systems and networks. Springer International Publishing, New York, NY, 2019. [2] I. Linkov, B. D. Trump, and J. Keisler. Risk and resilience must be independently managed. Nature, 555:7694, 2018. [3] M. Bozdal, M. Samie, and I. Jennions. A survey on CAN bus protocol: Attacks, challenges, and po- tential solutions. In 2018 International Conference on Computing, Electronics & Communications En- gineering, pages 201–205. IEEE, August 2018. [4] A. Kott, P. Théron, M. Drašar, E. Dushku, B. LeBlanc, P. Losiewicz, ..., and K Rzadca. Au- tonomous intelligent cyber-defense agent (aica) ref- erence architecture, 2018. arXiv:1803.10664. [5] A. Kott, M. S. Golan, B. D. Trump, and I. Linkov. Cyber resilience: by design or by intervention? Computer, 54(8):112–117, 2021. [6] A. Kott and P. Théron. Doers, not watchers: Intelligent autonomous agents are a path to cyber resilience. IEEE Security & Privacy, 18(3):62–66, 2020. [7] J. Ellis, T. Parker, J. Vandekerckhove, B. Murphy, S. Smith, A. Kott, and M. Weisman. Experimental infrastructure for study of measurements of re- silience. Proceedings of IEEE Military Commu- nications Conference, pages 841–846, Dec. 2022. [8] A. Alexeev, D. Henshel, K. Levitt, P. McDaniel, Rivera B., S. Templeton, and M. Weisman. Con- structing a science of cyber-resilience for military systems. NATO IST-153 Workshop on Cyber Re- silience, pages 23—25, 2017. [9] D. S. Henshel, K. Levitt, S. Templeton, M. G. Cains, A. Alexeev, B. Blakely, P. McDaniel, G. Wehner, J. Rowell, and M. Weisman. The sci- ence of cyber resilience: Characteristics and initial system taxonomy. Fifth World Conference on Risk, 2019. [10] A. Kott, J. Ludwig, and M. Lange. Assessing mission impact of cyberattacks: Toward a model- driven paradigm. IEEE Secur Privacy, 15(5):65– 74, January 2017. [11] A. K. Ligo, A. Kott, and I. Linkov. How to mea- sure cyber-resilience of a system with autonomous agents: Approaches and challenges. IEEE Engi- neering Management Review, 49(2):89–97, 2021. [12] I. Linkov, D. A. Eisenberg, K. Plourde, T. P. Seager, J. Allen, and A. Kott. Resilience metrics for cyber systems. Environ. Syst. Decis., 33(4):471–476, 2013. [13] Peter Beling, Barry Horowitz, and Tom McDer- mott. Developmental test and evaluation (dte&a) and cyberattack resilient systems. TR SERC-2021- TR-015 (V2), September 2021. [14] S. Hosseini, K. Barker, and J. E. Ramirez-Marquez. A review of deﬁnitions and measures of system resilience. Reliability Engineering & System Safety, 145:47–61, 2016. [15] A. Kott and I. Linkov. To improve cyber resilience, measure it. Computer, 54(2):80–85, Feb. 2021. [16] T. L. Grifﬁths, C. Kemp, and J. B. Tenenbaum. Bayesian models of cognition, pages 59–100. Cam- bridge University Press, Cambridge, MA, 2008. [17] M. Plummer. JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling. In K. Hornik, F. Leisch, and A. Zeileis, editors, Proceedings of the 3rd International Workshop on Distributed Statistical Computing. DSC 2003, Vienna, Austria, 2003. [18] C. Liu, D. B. Rubin, and Y. N. Wu. Parameter expansion to accelerate em: the px-em algorithm. Biometrika, 85(4):755–770, 1998. [19] A. Gelman. Parameterization and Bayesian mod- eling. Journal of the American Statistical Associa- tion, 99:537–545, 2004.