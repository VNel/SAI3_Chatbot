1 Mind the Gap: Securely modeling cyber risk based on security devia:ons from a peer group Authors: Taylor Reynolds, Sarah Scheﬄer, Daniel J. Weitzner, Angelina Wu1 Abstract There are two strategic and longstanding ques@ons about cyber risk that organiza@ons largely have been unable to answer: What is an organiza@on's es@mated risk exposure and how does its security compare with peers? Answering both requires industry-wide data on security posture, incidents, and losses that, un@l recently, have been too sensi@ve for organiza@ons to share. Now, privacy enhancing technologies (PETs) such as cryptographic compu@ng can enable the secure computa@on of aggregate cyber risk metrics from a peer group of organiza@ons while leaving sensi@ve input data undisclosed. As these new aggregate data become available, analysts need ways to integrate them into cyber risk models that can produce more reliable risk assessments and allow comparison to a peer group. This paper proposes a new framework for benchmarking cyber posture against peers and es@ma@ng cyber risk within speciﬁc economic sectors using the new variables emerging from secure computa@ons. We introduce a new top- line variable called the “Defense Gap Index” represen@ng the weighted security gap between an organiza@on and its peers that can be used to forecast an organiza@on’s own security risk based on historical industry data. We apply this approach in a speciﬁc sector using data collected from 25 large ﬁrms, in partnership with an industry ISAO2, to build an industry risk model and provide tools back to par@cipants to es@mate their own risk exposure and privately compare their security posture with their peers. 1 Authors listed in alphabe1cal order. Weitzner and Wu were supported, in part, by NSF grant Collabora1ve Research: DASS: Legally Accountable Cryptographic Compu1ng Systems (LAChS) Award Number: 21315415. Reynolds was supported by MIT’s Future of Data Ini1a1ve, MIT’s FinTech@CSAIL, and MIT’s Machine Learning Applica1ons @CSAIL. 2 The data was collected from 25 large ﬁrms in the United States with combined annual revenues of USD 23 billion. Due to the sensi1ve nature of the results, we are keeping the name of the ISAO undisclosed in this version of the paper. 2 Abstract ...................................................................................................................................... 1 Introduc@on ................................................................................................................................ 3 Related work ............................................................................................................................... 5 Cyber risk modeling approaches ............................................................................................ 5 Cryptographic compu@ng ....................................................................................................... 7 Data ............................................................................................................................................ 7 Models and results ..................................................................................................................... 9 Sectoral risk modeling approach 1: PLG = R ........................................................................... 9 Sectoral risk modeling approach 2: Monte Carlo simula@ons & loss exceedance curves .... 19 Limita@ons of the work ............................................................................................................. 22 Policy implica@ons .................................................................................................................... 22 Conclusions & future work ....................................................................................................... 23 Bibliography .............................................................................................................................. 25 Annex ........................................................................................................................................ 28 3 Introduc.on There are two strategic and longstanding ques@ons about cyber risk that organiza@ons largely have been unable to answer: What is an organiza@on's es@mated risk exposure and how does its security compare with peers? Answering both requires industry-wide data on security posture, incidents, and losses that, un@l recently, have been too sensi@ve for organiza@ons to share. Un@l now, ﬁrms have been unable to assess their own cyber risk posture with reference to larger risk paderns. This means that ﬁrms have been unable to forecast their own cyber risk because they lack the tools to learn about the frequency and magnitude of adacks in their own sector and the defensive posture of their peer group. Some of this data has been narrowly available to insurers, but even insurance providers and brokers lack data on core data such as actual economic losses (in contrast with insured losses) that are necessary to accurately forecast cyber risk. This lack of data leaves organiza@ons struggling to answer basic ques@ons about the magnitude of their own cyber risk and how they compare with other organiza@ons in their peer group. This comes at a @me when government regula@ons increasingly require organiza@ons to evaluate and monitor their cyber risk and the eﬀec@veness of security controls. For example, the newly revised FTC Safeguards Rule in the United States requires organiza@ons, now extending beyond just ﬁnancial services, to conduct security risk assessments that “must be wriden and must include criteria for evalua@ng those risks and threats.”(“FTC Safeguards Rule: What Your Business Needs to Know” 2022). The New York State Department of Financial Services recently issued a rule requiring covered en@@es to conﬁrm that they have devoted adequate resources to cover expected risk (NYDFS 2023). In Europe, Ar@cle 21 of the European Union’s Network and Informa@on Security Direc@ve (NIS 2) mandates that organiza@ons have “policies on risk analysis and informa@on system security” as well as “policies and procedures to assess the eﬀec@veness of cybersecurity risk-management measures”(EU 2022). It is not just governments pukng new demands on organiza@ons to produce cyber risk assessments and track the eﬀec@veness of controls. The US Na@onal Associa@on of Corporate Directors (NACD) produced a 2023 Director’s Handbook on Cyber-Risk Oversight that calls for management to “deliver reports that are benchmarked, so directors can see metrics in context to peer companies or the industry” (NACD 2023). In addi@on, the NACD says directors should obtain cyber risk assessments and informa@on about cyber-risk exposure in economic terms (NACD 2023). While there are clear policy requirements for organiza@ons to evaluate the eﬀec@veness of controls and compare themselves to peers, the ability to do so must be called into ques@on without access to the key external data about their peers that they would need to so eﬀec@vely. This could change though as new cryptographic techniques open access to aggregated cyber security data within a par@cular industry. 4 Cryptographic computa@on tools, a type of privacy enhancing technology or PETS, facilitate new data sources within an industry that can be used to benchmark and model risk. A subset of PETs known as “encrypted data processing tools” or “cryptographic compu@ng” allow aggregated results to be computed from encrypted cyber security posture, incident, and loss data without requiring organiza@ons to disclose the individual inputs. These secure computa@on approaches are used to develop cybersecurity benchmarks that can be used by individual ﬁrms for private comparisons (de Castro et al. 2020). The introduc@on of secure computa@on techniques for data analy@cs opens access to data sets that were never available before, par@cularly at the sector level among a group of peers. Organiza@ons can now share sensi@ve informa@on into a computa@on without the risk of revealing or disclosing sensi@ve, proprietary, or embarrassing data to anyone. This exci@ng development introduces a new set of modeling possibili@es using a richer data set but one that has smaller data coverage. In general, secure data aggrega@on techniques in the cybersecurity sector produce aggregated data on security posture, control failures, incident frequencies, and losses. The available mathema@cal analy@c tools include sums, averages, and high-level visibility into the overall data distribu@on of the variables. Individual inputs and more detailed data are not available as a feature of these techniques to protect the privacy and security of the underlying data. Given these new developments, there is a need for modiﬁed cyber risk frameworks that can ingest and use these smaller but richer data sets. One of the most exci@ng developments is the ability to aggregate data on security posture and incidents at the sector level. Focusing on the industry level allows a group of similar ﬁrms facing similar threats to essen@ally pool informa@on to understand and compare against the relevant peer group. From a modeling perspec@ve, focusing on a peer group with common threats, similar incident frequencies, and comparable loss amounts opens new analy@cal possibili@es for holding certain elements constant across the group and exploring the impact of control adop@on and security posture on risk es@mates. In this paper we propose a modiﬁed cyber risk modeling framework that incorporates newly available securely aggregated data. We introduce a new top-line variable in a standard cyber risk model called the “Defense Gap Index” that measures how a ﬁrm’s devia@on from the average security posture, based on historical industry data of the peer group, impacts an organiza@on’s own security risk. We show further how to construct this gap measurement from the outputs of secure data aggrega@ons done within a speciﬁc sector. Figure 1 introduces the proposed risk model that uses data collected from the sector to es@mate the probability of a signiﬁcant event in a given year (P), the average observed ﬁnancial losses in the peer group (L), and now the gap index that relates control devia@ons from the group average to changes in risk outcomes. 5 Figure 1: PLG = R We apply this approach in a speciﬁc sector, in partnership with an industry ISAO3, using actual data collected from 25 large ﬁrms with combined revenues of over USD 23 billion. The result is a general risk model for the industry and new private benchmarking tools for individual ﬁrms that allow them to answer the two outstanding ques@ons – 1) what is my organiza@on’s es@mated cyber risk and 2) how does it compare to the peer group? Related work Cyber risk modeling approaches Some of the earliest work on what we now call cyber risk modeling was focused on the risk of data processing. In the 1970s, Courtney posited that risk to electronic data processing systems can be summarized with two elements – a statement of impact from a “diﬃculty” and the probability of encountering that diﬃculty (Courtney 1977). Nearly 50 years later, the basic formula for calcula@ng risk is s@ll widely used, although with diﬀerent names. The widespread adop@on of computers throughout the business world in the 1990s and the growth of Internet connec@vity later in the decade and throughout the 2000s highlighted the need for new informa@on security protec@ons. Markets responded and new risk transfer op@ons in the form of cyber insurance appeared from companies such as Chubb, AIG, Lloyds, and Marsh (Gordon, Loeb, and Sohail 2003). In 2003, Gordon, Loeb, and Sohail published a framework for using insurance for cyber risk management that assesses risks, deploys security controls to mi@gate some of the risk, and transfers remaining ﬁnancial risk via insurance (Gordon, Loeb, and Sohail 2003). Researchers began exploring the decision-making process for businesses to transfer cyber risk via insurance (Mukhopadhyay et al. 2005). Around the same @me, researchers began ques@oning whether the insurance market for cyber insurance was actually sustainable given the correla@ons among losses, the lack of actuarial data, and the diﬃculty of substan@a@ng claims (Wang and Kim 2009b; 2009a; Böhme 2005; Baer and 3 The data was collected from 25 large ﬁrms in the United States with combined annual revenues of USD 23 billion. Due to the sensi1ve nature of the results, we are keeping the name of the ISAO undisclosed in this version of the paper. 6 Parkinson 2007). The ques@on of cyber insurability remains an important research area (Biener, Eling, and Wirfs 2015). A new line of cyber risk research emerged in the early 2010s focusing on data breaches and the number of lost records to compare and quan@fy cyber incidents (Ayyagari 2012; Edwards, Hofmeyr, and Forrest 2016). Since the ﬁnancial impact of data breaches were not available outside of insurance providers, researchers adempted to use the number of records exﬁltrated as part of incident as a way to compare and quan@fy cyber breach (OECD 2013; Wheatley, Maillart, and Sornede 2016). The approach proved diﬃcult because diﬀerent records have diﬀerent value and some of the quan@ﬁca@on methods such as looking for changes in market capitaliza@on tended to revert to the tradi@onal growth path over @me. Recent work is re- exploring the poten@al to es@mate the value of data by es@ma@ng the value individuals put on access to their computer ﬁles (Cartwright, Cartwright, and Xue 2021). Later in the 2010s, cyber risk modeling began splikng into two camps – those with access to large data sets such as insurance providers, and individual organiza@ons that needed to understand and manage their own risk. Insurance providers such as brokers and underwriters arguably have access to the most detailed data on frequencies, and losses, but lack informa@on on security posture within an organiza@on. Individual organiza@ons have a much beder understanding of their own security posture than their insurers do (asymmetric informa@on), but they lack vital informa@on about the broader cybersecurity landscape and informa@on on incidents in their own sector that are valuable for forecas@ng their own risk. Since quan@ta@ve data is largely unavailable to individual ﬁrms, they rely heavily on heat maps and other qualita@ve measures to evaluate and address their cyber risk (Fink et al. 2009; Staheli et al. 2014; Jiang et al. 2022). In the mid 2010s, two inﬂuen@al books appeared targe@ng individual organiza@ons looking to quan@fy their own risk. Freund and Jones developed a bodom-up cyber risk modeling framework called Factor Analysis of Informa@on Risk (FAIR) that has the same top-level structure as the model proposed by Courtney in 1977 (Courtney 1977; Freund and Jones 2014). The FAIR approach expands this into a taxonomy and ontology for building cyber risk models and quan@fying cyber risk within a ﬁrm based on its own internal data and informa@on that can be gleaned from other sources (Freund and Jones 2014). Around the same @me, Hubbard and Seiersen published a popular book en@tled, “How to Measure Anything in Cybersecurity Risk” (Hubbard and Seiersen 2016). Both approaches target risk analysts in individual ﬁrms and rely heavily on stochas@c methods such as Monte Carlo simula@ons to es@mate an organiza@on’s cyber risk. At present, industry is moving toward risk quan@ﬁca@on methods, and governments are making this a requirement in certain sectors, but the lack of external data sources remains a signiﬁcant challenge. 7 Cryptographic compu6ng Beginning in the 2010s and con@nuing into the 2020s, new privacy-enhancing technologies such as cryptographic compu@ng began emerging that permit the collec@on, processing, analysis and sharing of informa@on while protec@ng the conﬁden@ality of the underlying data (OECD 2023). Advances in cryptography and expanding computa@onal power unlocked the poten@al to do secure computa@ons using homomorphic encryp@on that can compute func@ons over encrypted data (Abbe, Khandani, and Lo 2012; Asharov et al. 2012). This has the poten@al to make new data sets available to researchers that were previously too sensi@ve to share into data aggrega@ons. The technology is s@ll developing but various use cases have emerged from double auc@ons in Denmark (Bogetou et al. 2009), linking private data sets in Estonia (Bogdanov et al. 2016), protec@ng privacy in genome studies (Kamm et al. 2013), simula@ng electricity trading markets (Abidin et al. 2016) to es@ma@ng the gender wage gap using private wage data (Lapets et al. 2019). Current applica@ons include privacy-preserving inventory matching systems for the banking sector (Polychroniadou et al. 2023) and distributed private adribu@on for adver@sing (Case et al. 2023). In 2020, a cryptographic compu@ng plavorm from MIT called SCRAM (Secure Cyber Risk Aggrega@on and Measurement) ran a secure mul@-party computa@on to collect security posture, losses, and incident frequencies from six ﬁrms to produce new cyber security metrics that could be used for modeling in the future (de Castro et al. 2020). This was the ﬁrst @me that cryptographic compu@ng was used to calculate previously unavailable cyber risk metrics. Now that the tools are available, the industry needs models that can use them. In the cybersecurity context, we recommend encryp@ng data in transit and at rest, but assume that data must be decrypted during use. Cryptographic compu@ng plavorms are exci@ng because they bridge this ﬁnal gap and allow the data to stay encrypted while in use. Data Mul@-party computa@on and encrypted data processing rely primarily on the ability to calculate sums over encrypted data (Abbe, Khandani, and Lo 2012). In the cyber risk context, sums are useful for coun@ng the total number of incidents, arriving at a sum of total monetary losses, and coun@ng the number of organiza@ons that adopt a speciﬁc security control at a speciﬁc maturity level. The secure computa@on ingests values from a speciﬁc loca@on (vector) within an encrypted spreadsheet that is contributed by a par@cipa@ng organiza@on. Each of the encrypted elements is summed across the peer group and the resul@ng output (a new matching vector) is decrypted and contains the sum of each item in the input vector. These sums can then be used to calculate averages for the group simply by dividing by the number of par@cipants contribu@ng data. Averages can be used for calcula@ng the frequency of incidents and the average losses associated with an incident. Averages are typically calculated in post processing of the results data. 8 Another important data output from the computa@ons are binary ﬂags that are used for coun@ng speciﬁc elements or crea@ng distribu@ons of variables across a set of data ranges. For example, binary ﬂags are used to count the number of incidents that have a total monetary loss that falls between a speciﬁc range of values. These counts can then be combined to build a rough histogram of loss quar@les or quin@les that are then used to build the new gap index variable. It is worth no@ng that individual records are not visible in computa@on results. Researchers cannot do tradi@onal data cleaning on submided data, but data checks are implemented with a veriﬁed checksum before data can be uploaded into the computa@on plavorm. The lack of visibility into individual inputs can lead to some imprecision in modeling the losses, for example, but this is the cost of increased privacy that is given to the input data. ISAO study: For this paper, we securely collect data from 25 members of a single ISAO using the MIT SCRAM plavorm. The 25 organiza@ons have combined annual revenue of over USD 23 billion. The collected data includes a ra@ng of the maturity level of 22 controls in an organiza@on, the number of incidents with losses larger than $5,000 between January 2021 and June 2023, informa@on on which control failures are responsible for reported losses, and the total ﬁnancial loss amount of security incidents during the relevant period. Speciﬁc details about the variables produced by the resul@ng computa@on are provided in the list below. - Maturity level (22 variables): Average maturity level for each of 22 controls across the peer group (self-reported). Based on the Ransomware Readiness Index where all controls are drawn from the White House Execu@ve Order on Improving the Na@on’s Cybersecurity, and the White House Memo to Corporate Execu@ve and Business Leaders on Ransomware from 2021 (Spiewak, Reynolds, and Weitzner 2021). - Quar:le ﬂags – Maturity levels (88 variables): Count of maturity ra@ngs for 22 controls over 4 poten@al responses (Not implemented, par@ally implemented, largely implemented, fully implemented). This provides a distribu@on of maturity levels across the par@cipants. - Incident count (1 variable): Sum of the number of incidents across the peer group during the relevant period - Control failures (22 variables): Count of the @mes individual controls failed leading to incidents with ﬁnancial losses. Par@cipants submikng an incident can implicate up to 5 failed controls as responsible for the reported ﬁnancial loss. - Financial costs – total (1 variable): Sum of the total ﬁnancial costs across all incidents in USD. - Financial costs – by control (22 variables): Data on the adributed costs of incident failures to for each of the 22 controls in USD. Data losses for a single reported incident are distributed evenly across all implicated controls in that incident. - Quin:le ﬂags – losses (5 variables): Count of the number of incidents in each of ﬁve ﬁnancial loss bands in USD. (1k-5k, 5k-50k, 50k-500k, 500k-5m, >5m) 9 These data are then used to build each of the components of the industry risk model and underpin the private tools that ﬁrms can use to compare their own security posture and risk to the peer group. Models and results This modeling sec@on details two modeling approaches that take advantage of the aggregated results for the sector. The ﬁrst develops the PLG=R model and builds a new defense gap index (G) that captures the rela@onship between weighted security control devia@ons from the peer group and risk exposure. The second modeling sec@on uses the same aggregated results to build an industry risk es@mate and loss exceedance curve using a Monte Carlo simula@on. Sectoral risk modeling approach 1: PLG = R The PLG = R model can be re-wriden as follows to represent an organiza@on’s own risk rela@ve to its peers. Equa:on 1 𝑷#𝑷𝒆𝒆𝒓𝒔∗𝑳&𝑷𝒆𝒆𝒓𝒔∗𝑮𝑶𝒘𝒏= 𝑨𝒏𝒏𝒖𝒂𝒍𝑹𝒊𝒔𝒌𝑶𝒘𝒏 Equa:on 2 𝑷#𝑷𝒆𝒆𝒓𝒔∗𝑳&𝑷𝒆𝒆𝒓𝒔∗𝟏= 𝑨𝒏𝒏𝒖𝒂𝒍𝑹𝒊𝒔𝒌𝑷𝒆𝒆𝒓𝒔 Where: P = Probability of an incident. Calculated as the average annual incident rate across the peer group. Once P is derived, it is held constant across the peer group under the assump@on that similar ﬁrms face similar threats and defend similar assets. L = Average ﬁnancial loss amount per incident across the peer group. Once L is derived, it is also held constant over the peer group. G = Defense Gap Index mul@plier. The gap index represents how weighted security posture devia@ons from the peer average aﬀect risk forecasts. The calcula@on of G is deﬁned in detail in the following sec@ons. Annual Risk = The forecasted annual ﬁnancial risk (expected value). Equa@on 1 and Equa@on 2 above include a measure of frequency (P) of incidents and their impact (L) but introduce a new top-line element called the Defense Gap Index (G). The key innova@on of the Defense Gap Index is that it uses actual loss data from the peer group, control failure adribu@ons, and the average security posture of the peer group to es@mate the rela@onship between weighted devia@ons from the average security control maturi@es of the peer group and changes in risk outcomes. This gives ﬁrms an empirically grounded means of predic@ng risk in the future to support investment decisions and can help enable regulators to set expecta@ons for reasonable security posture. In the well-known modeling approach Factor Analysis of Informa@on Risk (FAIR), Freund and Jones capture the strength of security controls as “Resistance Strength” under the “Loss Frequency” category (Freund and Jones 2014). Control strength has an indirect eﬀect on the 10 model via the level of vulnerability the ﬁrm faces that impacts the frequency of successful adacks. Since our core interest is understanding how changes in security posture aﬀect cyber risk forecasts, our proposed model elevates diﬀerences in security posture from the peer group to a top line element in the risk model alongside probability and loss. The func@onality of the Defense Gap Index is aligned with the goals of the variable “SecT“ in Mukhopadhyay et al’s CRAM model (Mukhopadhyay et al. 2019), but it is calculated diﬀerently and named as a “gap” index to capture the dynamic that higher scores of the variable relate to higher risk. P and L are both derived from the secure computa@on as averages and represent the average probability of a signiﬁcant incident for the peer group and the average monetary loss across all reported incidents. In each step of the model explana@on, we will use real-world data derived from the secure data collec@on done with 25 ﬁrms from a single ISAO. This allows us to illustrate the process while producing actual risk metrics and results for the sector. ISAO data results from the secure computa@on: - Average control maturity level: 78% (high level, between largely and fully implemented) - Number of incidents: 4 - P = 0.064 incidents per year per organiza@on - L = $145,000 average loss per incident - G = 1 since this represents the average baseline weighted security of the peer group. In other words, the average security posture has no devia@on from itself and is assigned a mul@plier of 1. - R = $9,280 average annual cyber risk per ﬁrm (computed from PLG) - Total losses: $580,000 - Implicated control failures: 5 controls implicated across the total $580,000 of losses Defense Gap Index (G) At a high level, the Defense Gap Index acts as a mul@plier that ampliﬁes or reduces forecasted risk levels based on an organiza@on’s weighted devia@ons from the security control maturity averages of the peer group. The weights for speciﬁc controls are allocated based on actual ﬁnancial losses adributed to control failures reported by members of the peer group.4 Once individual control weights are assigned, the next step takes actual loss magnitudes contributed by the group and maps them to net weighted devia@ons from the group average. Large observed losses are assigned to large nega@ve devia@ons (poorer security), while small observed losses are assigned to posi@ve devia@ons from the average (beder security). Next, we ﬁt a func@on to the observed data points (including the known group average). This func@on is 4 If an organiza1on reports an incident, they must assign responsibility for the incident to speciﬁc control failures. They can implicate up to 5 control failures per incident. The reported loss amount is divided equally across all implicated controls. 11 then used to calculate the Defense Gap Index mul@plier (G). Using the gap index mul@plier formula, organiza@ons can privately input their own security posture to obtain a personalized Defense Gap Index mul@plier (G) that goes into the PLG=R model to calculate their own risk. Once the computa@on is complete and the Defense Gap Index calcula@on is parameterized, par@cipants are sent the group values for P and L along with the Defense Gap Index formula. This allows them to privately do their own in-house risk modeling and answer the two outstanding ques@ons of what is an organiza@on's es@mated risk exposure and how does its security compare with peers? Figure 2 below provides a broad overview of the modeling approach where data from private computa@ons in the ﬁrst horizontal sec@on feed into calcula@ons of the Defense Gap Index in the second sec@on. Finally, individual organiza@ons can privately compute their own Defense Gap Index mul@plier using their own security posture and use it for internal risk modeling. The ﬁve steps for modeling the Defense Gap Index mul@plier are provided below and are populated with real-world data obtained from the computa@on with 25 members of an ISAO. Figure 2: Summary of the sectoral risk modeling approach The next ﬁve steps explain how to derive the Defense Gap Index formula using data from the secure computa@on. Step 1: Allocate overall category weights between controls groups with and without losses In this ﬁrst step, researchers building the industry model decide how much importance to place on control failures that lead to losses, the “loss group”, compared to controls that are not implicated in loss events, the “no-loss group”. Three possible weigh@ng op@ons are: - Op@on 1: Equal weigh@ng for all controls (unweighted) - Op@on 2: Weigh@ng based on the alloca@on of losses across implicated control failures 12 - Op@on 3: Weigh@ng based on correla@ons of actual losses (or lack of losses) and security control maturity Ideally the alloca@on of weights should be done using correla@ons between losses (and the lack of losses) and security control maturity while controlling for endogeneity. At present, the tools for allowing Op@on 3 are s@ll under development, so this sec@on describes how Op@on 2 is produced. This second op@on requires that some of the overall weight is assigned among controls implicated in failures, and the remainder is allocated across controls that have no associated losses. We start by considering an 85% (implicated) / 15% (non-implicated) split of the weights but then adjust to make sure that the smallest implicated weight is larger than any non-implicated weight. ISAO result: We use a slightly modiﬁed data split of 75%/25% because of the wide loss range between the largest and smallest implicated control. We want to ensure the smallest implicated weight is larger than the non-implicated weights. Also, the rela@vely small number of implicated control failures (5) means addi@onal weight should be added to the non-implicated controls. Step 2: Allocate individual control weights within loss and no-loss groups The second step allocates weights across individual controls in the loss and no-loss groups ( Figure 3). We assign controls in the “loss group” a high propor@on of the total weight (e.g. 75%) and then the sub-weights of individual controls within the group are pro-rated based on the magnitude of losses assigned to each by the peer group. Sub-weights in the no-loss group are assigned as an equal distribu@on of the remaining weight (e.g. 25%). ISAO result: There were 5 implicated controls with loss amounts in the ISAO data collec@on. The 5 implicated controls are assigned a combined 75% of the weight, while 17 non-implicated controls receive an equal share of the remaining 25%. The weights in the implicated group vary widely from 42% of the total weight assigned to “Evaluate employee skills” down to 1.9% of the weight assigned to “Deploy Mul@factor Authen@ca@on”. The full breakdown is available in (Table 2 in the Annex. These are all based on observed losses which ranged from largest amount of $325,000 adributed to employee skills to the smallest amount of $15,000 on MFA. Employee skills and training were the two largest loss areas followed by controls related to backup and then MFA. The remaining 17 controls each received an equal weight of 1.5%. Figure 3 provides a breakdown of the ISAO weigh@ng. Figure 3: ISAO weights applied to controls with and without a\ributed losses 13 75% of the total weight is assigned propor@onately to the 5 controls with adributed losses based on the size of the observed ﬁnancial impact. Example: n = 5, weights vary from 42.0% to 1.9%. 25% of the total weight is assigned equally across all 17 non-implicated controls. Example: n = 17, weights are all 1.5% Step 3: Net weighted security control devia:on and boundaries The third step uses the weights produced in Step 2 to create a net weighted devia@on formula that individual organiza@ons will use to calculate their own weighted security control devia@ons from the peer group. The group average and control weights are calculated in Step 2 above. The general equa@on is given in below: Equa:on 3 𝑁𝑒𝑡𝑊𝑒𝑖𝑔ℎ𝑡𝑒𝑑𝐷𝑒𝑣𝑖𝑎𝑡𝑒= ∑ ((𝑂𝑤𝑛𝑀𝑎𝑡𝑢𝑟𝑖𝑡𝑦(/𝐺𝑟𝑜𝑢𝑝𝐴𝑣𝑒𝑟𝑎𝑔𝑒() ) (*+ ∗𝐶𝑜𝑛𝑡𝑟𝑜𝑙𝑊𝑒𝑖𝑔ℎ𝑡() Where: OwnMaturity = The organiza@on’s own control maturity for control n GroupAverage = The average maturity level for control n across the peer group ControlWeight = The control weight assigned to control n in Step 2 In this step, the industry model developers determine a set of devia@on boundaries that will be used to model the high and low ranges of observed losses. For example, the relevant ranges to consider could be 30% above and 30% below the group average, where having a net weighted security control devia@on that is 30% below the average would correspond to the highest range of losses reported by the peers. The lowest range of losses are then assigned to net weighted security control devia@ons that are beder than the peer average. Clearly there is some art involved in determining these ranges, but we have found +/- 30% to be a good set of modeling ranges in mul@ple sectors. In the future it should be possible to calculate the correla@ons between losses (and the lack of losses) and net weighted security control devia@on to produce beder es@mates. 14 ISAO result: We select a maximum range of +/- 30% for the net weighted security devia@on range to represent our best es@mate of the weighted security varia@ons across the sector.5 Step 4: Create a model ﬁ_ng observed loss data to the net weighted devia:on for controls The fourth step evaluates and models the distribu@on of actual observed losses over the net weighted security control devia@on boundaries deﬁned in Step 3. We assume a non-linear, exponen@al model. We also assume that higher security (posi@ve net weighted varia@on) corresponds to lower losses and vice versa (Eling and Wirfs 2019). The largest observed losses map to the lowest security levels (e.g. 30% below average) and the smallest losses to the higher security levels (e.g. up to 30% above average). The average loss and the average security level, which represent the averages of the peer group, correspond to a Defense Gap Index mul@plier (G) of 1 and are used as one of the observa@ons. Since individual losses are not visible, loss ranges in quar@les or quin@les provide the relevant data points for ﬁkng the loss func@on. There is no precise way to place individual loss points from a range, but op@ons include using the maximum, average, midpoint, or minimum as the representa@ve point in the quar@le. The average loss amount and average security level (corresponding to a Defense Gap Index of 1) are both known and serve as the grounding point for the model es@ma@on. ISAO result: The ISAO computa@on reveals four incidents reported by three ﬁrms spread over two quin@les. We calculate the loss model in this step by using the three observed loss amounts plus the computed average loss to build the loss model. The two incidents from the same ﬁrm are only visible to us as a single loss amount range which complicates interpre@ng the bands. The total losses across all incidents amount to $580,000. Two ﬁrms report losses between $50,000 and $500,000 and a third ﬁrm reports losses between $5,000 and $50,000. For the higher loss quin@le (50k-500k), we use $450,000 as the top end loss and assign it to a net weighted devia@on of -30%. We arrive at the $450,000 number by subtrac@ng away the bodom quar@le’s single highest loss ($50,000) from all reported losses ($580,000). We also know that the second ﬁrm’s loss in the high quar@le is larger than the upper limit of the smaller quar@le ($50,000) so that can be subtracted as well – leaving us with $480,000. We used a slightly smaller $450,000 value to reﬂect the ambiguity around the actual loss amounts. We know the average loss ($145,000) at the average level of security (net weighted security control devia@on of 0). Finally, we assume the single loss in the lower quin@le is close to the top of the range at $50,000 for an organiza@on that has 15% beder security than the average. These three available points provide us with enough data to es@mate a curve that traverses through the average for the peer group in Figure 4. 5 The maximum range is allowed to surpass 100% because weights on individual controls can vary considerably. In the ISAO case here, 42% of the total weight is assigned to one control (evalua1ng employee skills) so signiﬁcant devia1ons of this single control can have large eﬀects. 15 Figure 4: Producing the Gap index scalar The resul@ng gap index model is: Equa:on 4 𝑫𝒆𝒇𝒆𝒏𝒔𝒆𝑮𝒂𝒑𝑰𝒏𝒅𝒆𝒙= 𝑮= 𝒆,𝟒.𝟕𝟗𝟔∗𝑵𝒆𝒕𝑾𝒆𝒊𝒈𝒉𝒕𝒆𝒅𝑫𝒆𝒗𝒊𝒂𝒕𝒆 Where: DefenseGapIndex = the Defense Gap Index mul@plier (G) to be used in the equa@on P*L*G = R NetWeightedDeviate = the net weighted security control devia@on from the peer group’s average control maturity. Compu3ng private results Once the Defense Gap Index formula in Equa@on 4 is established and values for P and L are known from the computa@on results, organiza@ons can now use the model to privately calculate their risk and compare their security posture and risk to their peers.6 The equa@ons and process for each are described in this sec@on. Q1: What is our organiza6on’s es6mated risk exposure? The process a par@cipa@ng organiza@on uses to forecast their own risk exposure is seamless and automated via a spreadsheet once the secure computa@on results are available, but we step through the process in detail here. 6 Par1cipa1ng organiza1ons receive a results spreadsheet with detailed dashboards that only requires them to insert their own private values that were originally contributed into the secure computa1on that then populates all the dashboards. y = e^(-4.796x) 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 Net weighted security control deviation, percent deviation from the sector average (x-axis) Gap index scalar (y-axis) by Net weighted security control deviation (x-axis) Gap index scalar (y-axis) $450,000 loss, -30% $146,000 loss (peer average) $50,000 loss, +15% 16 The ﬁrst step is calcula@ng the organiza@on’s own risk exposure and comparing it to the peer group. This is done using Equa@on 3 to calculate its own net weighted security devia@on (x). The results Equa@on 3 are then used in Equa@on 5 to calculate the organiza@on’s own Defense Gap Index value (Gown). Equa@on 5 should already include the derived constant value for the peer group that was calculated earlier for the en@re group in Equa@on 4. Finally, the organiza@on inserts Gown from Equa@on 5 into the two following risk equa@ons, holding P and L constant, to obtain a forecast of its own annual cyber risk in monetary terms (Equa@on 6) and a forecasted incident size (Equa@on 7) in the case of an event. P and L are derived in the original secure computa@on and provided for the par@cipants along with the DerivedConstant from the gap index modeling. Equa:on 3 𝑁𝑒𝑡𝑊𝑒𝑖𝑔ℎ𝑡𝑒𝑑𝐷𝑒𝑣𝑖𝑎𝑡𝑒= ∑ ((𝑂𝑤𝑛𝑀𝑎𝑡𝑢𝑟𝑖𝑡𝑦(/𝐺𝑟𝑜𝑢𝑝𝐴𝑣𝑒𝑟𝑎𝑔𝑒() ) (*+ ∗𝐶𝑜𝑛𝑡𝑟𝑜𝑙𝑊𝑒𝑖𝑔ℎ𝑡() Equa:on 5 𝑮𝑶𝒘𝒏= 𝑮𝒂𝒑𝑰𝒏𝒅𝒆𝒙𝑫𝒆𝒇𝒆𝒏𝒔𝒆= 𝒆𝑫𝒆𝒓𝒊𝒗𝒆𝒅𝑪𝒐𝒏𝒔𝒕𝒂𝒏𝒕∗𝑵𝒆𝒕𝑾𝒆𝒊𝒈𝒉𝒕𝒆𝒅𝑫𝒆𝒗𝒊𝒂𝒕𝒊𝒐𝒏 Equa:on 6 𝑨𝒏𝒏𝒖𝒂𝒍𝑹𝒊𝒔𝒌𝑶𝒘𝒏= 𝑷#𝑷𝒆𝒆𝒓𝒔∗𝑳&𝑷𝒆𝒆𝒓𝒔∗𝑮𝑶𝒘𝒏 Equa:on 7 𝑭𝒐𝒓𝒆𝒄𝒂𝒔𝒕𝒆𝒅𝑰𝒏𝒄𝒊𝒅𝒆𝒏𝒕𝑺𝒊𝒛𝒆𝑶𝒘𝒏= 𝑳&𝑷𝒆𝒆𝒓𝒔∗𝑮𝑶𝒘𝒏 Q2a: How does our risk compare with our peers? Once the organiza@on knows its own forecasted annual risk and incident size, analysts can compare these results with the average results from the peer group. Equa@on 8 and Equa@on 9 compare the annual risk of the own ﬁrm with its peers, while Equa@on 10 and Equa@on 11 with the peer group on annual risk and forecasted incident sizes. Equa:on 8 𝑨𝒏𝒏𝒖𝒂𝒍𝑹𝒊𝒔𝒌𝑶𝒘𝒏= 𝑷#𝑷𝒆𝒆𝒓𝒔∗𝑳&𝑷𝒆𝒆𝒓𝒔∗𝑮𝑶𝒘𝒏 Equa:on 9 𝑨𝒏𝒏𝒖𝒂𝒍𝑹𝒊𝒔𝒌𝑷𝒆𝒆𝒓𝒔= 𝑷#𝑷𝒆𝒆𝒓𝒔∗𝑳&𝑷𝒆𝒆𝒓𝒔 Equa:on 10 𝑭𝒐𝒓𝒆𝒄𝒂𝒔𝒕𝒆𝒅𝑰𝒏𝒄𝒊𝒅𝒆𝒏𝒕𝑺𝒊𝒛𝒆𝑶𝒘𝒏= 𝑳&𝑷𝒆𝒆𝒓𝒔∗𝑮𝑶𝒘𝒏 Equa:on 11 𝑭𝒐𝒓𝒆𝒄𝒂𝒔𝒕𝒆𝒅𝑰𝒏𝒄𝒊𝒅𝒆𝒏𝒕𝑺𝒊𝒛𝒆𝑷𝒆𝒆𝒓𝒔= 𝑳&𝑷𝒆𝒆𝒓𝒔 Q2b: How does our security posture compare with our peers? The next ques@on that can be answered with the data is how the organiza@on’s own security posture compares with its peers. There are two ways analysts can compare their organiza@on’s own security posture with peers in the sector (Figure 5). The ﬁrst is using standard benchmarking tables outputs from the secure 17 computa@on which provide the average maturity across the peer group for each control and a distribu@on of responses (not / par@ally / largely / fully implemented). Analysts can also use the weighted controls lists that have been informed by actual losses across the group to accommodate control priori@za@on. The net weighted security control devia@on measure provides a weighted comparison against the average (value of 1) of the peer group. For example, a net weighted devia@on score of 0.75 implies that the organiza@on’s security posture is 25% lower than the sector’s peer average auer weigh@ng each control by observed losses. Figure 5: Security posture comparison (unweighted and weighted controls) ISAO results for the industry Equa:on 12 𝑨𝒏𝒏𝒖𝒂𝒍𝑹𝒊𝒔𝒌𝑷𝒆𝒆𝒓𝒔= 𝑷#𝑷𝒆𝒆𝒓𝒔∗𝑳&𝑷𝒆𝒆𝒓𝒔= 𝟎. 𝟎𝟔𝟒∗$𝟏𝟒𝟓, 𝟎𝟎𝟎= $𝟗, 𝟐𝟖𝟎 Equa:on 13 𝑭𝒐𝒓𝒆𝒄𝒂𝒔𝒕𝒆𝒅𝑰𝒏𝒄𝒊𝒅𝒆𝒏𝒕𝑺𝒊𝒛𝒆𝑷𝒆𝒆𝒓𝒔= 𝑳&𝑷𝒆𝒆𝒓𝒔= $𝟏𝟒𝟓, 𝟎𝟎𝟎 ISAO results for a par@cular ﬁrm Equa:on 14 𝑮𝑶𝒘𝒏= 𝑮𝒂𝒑𝑰𝒏𝒅𝒆𝒙𝑫𝒆𝒇𝒆𝒏𝒔𝒆= 𝒆,𝟒.𝟕𝟗𝟔∗𝑵𝒆𝒕𝑾𝒆𝒊𝒈𝒉𝒕𝒆𝒅𝑫𝒆𝒗𝒊𝒂𝒕𝒊𝒐𝒏 Equa:on 15 𝑨𝒏𝒏𝒖𝒂𝒍𝑹𝒊𝒔𝒌𝑶𝒘𝒏= 𝟎. 𝟎𝟔𝟒∗$𝟏𝟒𝟓, 𝟎𝟎𝟎∗ 𝑮𝑶𝒘𝒏 Equa:on 16 𝑭𝒐𝒓𝒆𝒄𝒂𝒔𝒕𝒆𝒅𝑰𝒏𝒄𝒊𝒅𝒆𝒏𝒕𝑺𝒊𝒛𝒆𝑶𝒘𝒏= $𝟏𝟒𝟓, 𝟎𝟎𝟎∗𝑮𝑶𝒘𝒏 Using the ISAO results from the equa@ons above, we illustrate how forecasted risk increases or decreases with changes in the net weighted security control devia@on through the Defense Gap Index mul@plier (G). Figure 6 shows annual expected risk based on varia@ons in an ISAO member’s defense posture. The average risk derived in Equa@on 12 of $9,280 per year for the average level of protec@on reﬂects the “fair price” for an insurance premium based on the incidents reported by the 25 ﬁrms over 2.5 years. However, if a member organiza@on has substan@ally lower levels of control implementa@on, its forecasted annual loss could be over ﬁve @mes the average, or $49,723 as shown in Figure 6. At the other end of the control maturity spectrum, an organiza@on with 35% higher weighted maturity will only suﬀer a forecasted average annual loss of $1,732, which is roughly one ﬁuh lower than the average. 18 In insurance parlance, this fair price is the equivalent of the expected loss for the pool, but does not include other internal costs, external costs, economic proﬁt needs, and capital costs that the insurance provider incurs to run its business. This means that the actual premium would need to be somewhat higher than the calculated expected loss for the insurance company to operate. The “fair price” calcula@on also assumes that all costs would be covered in the case of an incident, but that is typically not the case as there are exclusions and deduc@bles that lead to less than full coverage. The “fair price” calcula@ons are imprecise, but they s@ll provide a good star@ng point for organiza@ons in the peer group to evaluate insurance oﬀers. Figure 6: Annual cyber risk forecasts by net weighted security control devia:on from group Figure 7 shows the same trend but forecasts the ﬁnancial impact of an individual security incident based on the net weighted security devia@on rela@ve to the ISAO industry average. An organiza@on with the average security posture could expect an incident size of $145,000 when there is a successful adack. However, organiza@ons with a weighted net security gap that puts it 30% below average would expect an incident to cost $778,917 – nearly 5 @mes the average. Figure 7: Forecasted incident sizes by net weighted security control devia:on from group 19 Sectoral risk modeling approach 2: Monte Carlo simula6ons & loss exceedance curves Monte Carlo simula3ons The peer data on losses can also be used in a Monte Carlo simula@on at the peer group level to forecast the probability that the loss from a single cyber incident will be above a certain threshold. This requires an understanding of the distribu@on of ﬁnancial losses across the group that can be gleaned from the secure computa@on loss quin@les. Eling and Wirfs use insurance data to study the costs of cyber events and ﬁnd two categories of losses – the ﬁrst they call the “cyber risks of daily life” with frequent but low ﬁnancial losses, and the second that they call “extreme cyber risks” that are infrequent but have high associated losses (Eling and Wirfs 2019). One of their key ﬁndings is that the two categories of cyber events have diﬀerent distribu@ons and should be modeled separately. Following this approach, we set up a Monte Carlo simula@on based on the observed loss categories across the peer group. A mean, distribu@on, and probability are assigned to the large but infrequent loss category, and a diﬀerent mean, distribu@on, and probability are assigned to the small but frequent loss category. Our ISAO data show a poten@al cluster of one or two incidents in the $50,000 low end range and another poten@al cluster of incidents in the higher quin@le in the $450,000 range. The computa@on results were ambiguous about the number of incidents in each quin@le, so we will assume a 75% low end and 25% high end distribu@on that we have seen in other sectors. We model them separately within the same Monte Carlo simula@on.7 The Monte Carlo that selects losses distributed around $50,000 for 75% of the @me and around $450,000 for the remaining 25% of the @me (Table 1). We ﬂaden the distribu@ons by increasing the standard devia@ons for each of the categories to roughly correspond with the +/- 30% net weighted security devia@on scores discussed earlier. The high distribu@on has a larger rela@ve standard devia@on indica@ng that losses at the high end will vary more than losses at the low end. Table 1: Monte Carlo inputs based on observed data Variable Low distribu@on High distribu@on Mean $50,000 $450,000 Standard devia@on $25,000 $300,000 Probability 75% 25% The Monte Carlo simula@on selects a random value 10,000 @mes that follows the distribu@ons shown in Table 1 and represents the average security level for the group. For 75% of the @me, that random value comes from the low distribu@on, and for 25% of the @me from the high distribu@on. The results of the 10,000 itera@ons are then classiﬁed by their loss amount to provide a distribu@on of possible losses. In Figure 8, the Y axis shows the count of results in a speciﬁc range, and the X -axis shows the corresponding monetary loss. 7 Spreadsheet equa1on: IF(RAND()<0.75,NORMINV(RAND(),50000,25000),NORMINV(RAND(),450000,300000)) 20 The mean and spread of each distribu@on are determined by the observed data in the loss categories, but standard devia@ons are typically large and ﬂat to cover the broad range of poten@al losses. The probability of a loss falling in either of the distribu@ons should largely be set by the observed data but can be augmented with other known industry loss data if available. It is easier to introduce external data for this approach because no measure of the aﬀected organiza@on’s security posture is required to place the loss in context. Using the seeds from the peer group data collec@on, the next stop is running a Monte Carlo simula@on with 10,000 or more instances. Random loss values cannot be nega@ve, so any nega@ve values are bodom censored at zero. This simula@on represents expected losses based on the average security level for the peer group and is not tailored to a speciﬁc ﬁrm. ISAO Results: The distribu@on emerging from Monte Carlo simula@on using ISAO data is shown in Figure 8. clear peak is visible around $75,000 at the low end, while the distribu@on of high losses is thin and rela@vely ﬂat around $500,000. Figure 8: ISAO Monte Carlo simula:on of random loss values Note: Random values in the distribu@on cannot be nega@ve and are bodom censored at zero. Loss exceedance curves The results of the Monte Carlo simula@on can be used to build loss exceedance curves (also known as complementary cumula@ve distribu@on func@ons) that are commonly used in catastrophic risk modeling to describe the probability that a certain loss value will be exceeded in a predeﬁned future @me period (Grossi, Kunreuther, and Windeler 2005). Loss exceedance curves have also been adopted in cyber risk modeling to convey the probability that the losses from large cyber incident will exceed a given amount (Hubbard and Seiersen 2016), (Sokri 2019), (Humphreys 2021). They are useful for risk managers and governance boards charged with managing the organiza@on’s overall risk. 21 In the context of cyber risk governance, an organiza@on’s leadership may want to know whether the organiza@on can handle the ﬁnancial losses of a large incident and the probability that a single signiﬁcant loss event will exceed a certain amount. The loss data derived from secure computa@ons is put into a Monte Carlo simula@on whose outputs are use to create loss exceedance curves. We use a model based on (Hubbard and Seiersen 2016) and (Humphreys 2021) which shows the probability that a large incident will be above a certain loss threshold (Equa@on 17). Equa:on 17 𝑳𝒐𝒔𝒔𝑬𝒙𝒄𝒆𝒆𝒅𝒂𝒏𝒄𝒆𝑪𝒖𝒓𝒗𝒆= 𝟏−𝑭𝑳(𝒍) = 𝒑(𝑳𝑴𝑪> 𝒍) Where: FL is the cumula@ve distribu@on func@on of losses LMC is the random variable of the loss from the Monte Carlo (real numbers). l (lowercase) is the poten@al loss amount In this implementa@on, the loss variable LMC represents the size of a single incident. Figure 9 shows the ISAO peer group’s loss exceedance curve based on the Monte Carlo simula@on above. The results show that in 97% of cases, the cost of a signiﬁcant incident will be over $10,000. The probabili@es fall as the losses increase so that the probability of having a loss over $500,000 falls to 12%, and the probability of having a loss over $1,000,000 is only 1%. It is important to note that this simula@on may only be representa@ve for the peer group and not the en@re sector due to selec@on bias issues. Figure 9: ISAO imputed loss exceedance curve 22 Limita.ons of the work These cyber risk models making use of secure computa@on results will improve our understand on risk and produce beder es@mates. Yet, there are several limita@ons to this research approach. First, the data that is used in the secure computa@ons is self-reported by the organiza@ons themselves. Although every eﬀort is made to educate the par@cipants about evalua@ng control maturi@es and es@ma@ng loss amounts, the self-reported data is likely to have variability that limits the precision of the results. In the future, automated data collec@ons of speciﬁc variables could help minimize this challenge. The risk modeling process, and the produc@on of the Defense Gap Index in par@cular, require a bit of art mixed with science to locate and map reported loss ranges to net weighted devia@ons from the peer average. We understand that the process is imprecise, but we believe that perfec@on should not be the enemy of the good and having a small amount of actual data to model cyber risk for a peer group is beder than having no data at all. The number of organiza@ons that can poten@ally par@cipate in a computa@on is limited to a single sector, so there will be fewer incidents that are available for modeling then would be possible using ﬁrms from a variety of sectors. We also need a large representa@ve sample from the sector to get results that reﬂect the state of the sector as a whole. Our strategy of limi@ng the research to one sector at a @me allows us to hold P and L constant and evaluate changes in the Defense Gap Index (G) and how they aﬀect risk. Broadening to the en@re economy would certainly increase the number of incidents that could be used to model, but the assump@on that P and L remain constant would be much more diﬃcult to make. The secure data collec@ons likely suﬀer from some selec@on bias. Any organiza@ons that is a member of an ISAO and is willing to invest @me par@cipa@ng in a secure data collec@on for understanding cyber risk beder is also likely to be among the most proac@ve in defending their data and networks. The 25 ﬁrms from the ISAO that par@cipated in the study were somewhat surprised by the loss results. They expected much larger losses than were reported by the group, and several par@cipants suggested that the issue may be due to selec@on bias. As a result, inferences related to the ﬁndings of the ISAO risk modeling should be limited to the proﬁle of leading ﬁrms in the sector with regards to their security. Policy implica.ons The crea@on of the new defense gap index has important implica@ons for policy making. First, it provides a valuable tool for organiza@ons to calculate their cyber risk and compare it against their peer group in a way that has never been possible before. Second, it introduces a quan@ﬁca@on methodology for priori@zing security controls based on actual losses and control failures reported by the peer group – providing clear guidance to policymakers on areas of par@cular need and targets for policy aden@on. Third, the defense gap index provides a holis@c view of an organiza@on’s cyber security posture rela@ve to its peers in the sector. Fourth, the gap index provides a baseline security posture for an industry that can be tracked over @me to understand the sector’s evolving security landscape. 23 This research shows that there are methods for calcula@ng cyber risk metrics and models for speciﬁc sectors that can take advantage of new data coming from secure aggrega@ons. These new secure computa@onal techniques have opened a rich set of metrics that can be used to gauge the risk proﬁle of a speciﬁc economic sector and allow organiza@ons within that sector to compare themselves to their peers. Government eﬀorts to bring together peer groups to jointly and securely aggregate cyber risk data could help policy makers and the organiza@ons themselves obtain a much beder understanding of cyber risk throughout the sector. One of the key challenges in cyber risk modeling is a lack of standardized deﬁni@ons and terms that are used across the industry. Un@l now, there has been limited eﬀort to standardize the terminology since the data was previously too sensi@ve to share. But this is changing, and governments working with industry groups and academic researchers can play a role in helping standardize the deﬁni@ons and terms we use for cyber risk modeling. One of the key ﬁndings emerging from the ISAO data and backed up by other literature is that improving security for organiza@ons that are signiﬁcantly below the peer average can have an outsized eﬀect rela@ve to the investment. These ﬁrms with the lowest security levels oﬀer the largest return on security investment because of the observed non-linearity of security losses. Another related ﬁnding is that focusing interven@ons ﬁrst on security control failures associated with the largest losses will likely have a larger return on investment and aden@on. Governments should priori@ze research into uncovering beder informa@on about the eﬀec@veness of controls to guide their own security investments and priori@es. Conclusions & future work The goal of this research is providing new models and data to answer two key ques@ons that organiza@ons have struggled to answer. What is an organiza@on's es@mated risk exposure? How does the security of an organiza@on compare with its peers in the sector? We provide the tools to answer each of these ques@ons through the key innova@on in this paper - a new variable called the Defense Gap Index in the top line of the risk model. The Gap Index works as a mul@plier to increase or decrease forecasted risk for an individual ﬁrm based on the net weighted distance of its own security posture from the average security posture of the peer group. These comparisons are made possible using cryptographic computa@on tools. This modeling approach provides new tools to individual organiza@ons to forecast and benchmark their risk, but also allows policymakers to compare aggregate security levels across sectors. In the paper we apply the model to a data collec@on across 25 large ﬁrms in single sector to produce a benchmark for the industry and create powerful new tools for the par@cipants to privately compute their own results. Using data derived from a secure mul@-party computa@on, we can develop a risk model for an ISAO sector and provide modeling tools to the par@cipa@ng ﬁrms to forecast their own risk 24 based on their unique security posture, and then compare themselves to their peers. The model proposed in this paper is used for a secure data collec@on with an ISAO to build benchmarks of security posture, and risk models for the industry and individual ﬁrms. Future research in this area should expand to addi@onal sectors using similar methods so that the results could be compared to one another. Another area for future research would be developing new methods for introducing external data from outside the peer group into the modeling process for the Defense Gap Index. 25 Bibliography Abbe, Emmanuel A, Amir E Khandani, and Andrew W Lo. 2012. “Privacy-Preserving Methods for Sharing Financial Risk Exposures.” American Economic Review 102 (3): 65–70. hdps://doi.org/10.1257/aer.102.3.65. Abidin, Aysajan, Abdelrahaman Aly, Sara Cleemput, and Mustafa A. Mustafa. 2016. “An MPC- Based Privacy-Preserving Protocol for a Local Electricity Trading Market.” In Cryptology and Network Security, edited by Sara Fores@ and Giuseppe Persiano, 10052:615–25. Lecture Notes in Computer Science. Cham: Springer Interna@onal Publishing. hdps://doi.org/10.1007/978-3-319-48965-0_40. Asharov, Gilad, Abhishek Jain, Adriana López-Alt, Eran Tromer, Vinod Vaikuntanathan, and Daniel Wichs. 2012. “Mul@party Computa@on with Low Communica@on, Computa@on and Interac@on via Threshold FHE.” In Advances in Cryptology – EUROCRYPT 2012, edited by David Pointcheval and Thomas Johansson, 7237:483–501. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer Berlin Heidelberg. hdps://doi.org/10.1007/978-3-642-29011-4_29. Ayyagari, Ramakrishna. 2012. “An Exploratory Analysis of Data Breaches from 2005-2011: Trends and Insights.” Journal of InformaGon Privacy and Security 8 (2): 33–56. hdps://doi.org/10.1080/15536548.2012.10845654. Baer, Walter S., and Andrew Parkinson. 2007. “Cyberinsurance in It Security Management.” IEEE Security & Privacy 5 (3): 50–56. Biener, Chris@an, Mar@n Eling, and Jan Hendrik Wirfs. 2015. “Insurability of Cyber Risk: An Empirical Analysis.” The Geneva Papers on Risk and Insurance - Issues and PracGce 40 (1): 131–58. hdps://doi.org/10.1057/gpp.2014.19. Bogdanov, Dan, Liina Kamm, Baldur Kubo, Reimo Rebane, Ville Sokk, and Riivo Talviste. 2016. “Students and Taxes: A Privacy-Preserving Study Using Secure Computa@on.” Proceedings on Privacy Enhancing Technologies 2016 (3): 117–35. hdps://doi.org/10.1515/popets-2016-0019. Bogetou, Peter, Dan Lund Christensen, Ivan Damgård, Mar@n Geisler, Thomas Jakobsen, Mikkel Krøigaard, Janus Dam Nielsen, et al. 2009. “Secure Mul@party Computa@on Goes Live.” In Financial Cryptography and Data Security, edited by Roger Dingledine and Philippe Golle, 325–43. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer. hdps://doi.org/10.1007/978-3-642-03549-4_20. Böhme, Rainer. 2005. “Cyber-Insurance Revisited.” In Weis. hdps://infosecon.net/workshop/slides/weis_5_1.pdf. Cartwright, Anna, Edward Cartwright, and Lian Xue. 2021. “The Value of Data: Es@ma@ng the Value Individuals Put on Access to Their Computer FIles,” June. Case, Benjamin, Richa Jain, Alex Koshelev, Andy Leiserson, Daniel Masny, Thurston Sandberg, Ben Savage, Erik Taubeneck, Mar@n Thomson, and Taiki Yamaguchi. 2023. “Interoperable Private Adribu@on: A Distributed Adribu@on and Aggrega@on Protocol.” Cryptology ePrint Archive. hdps://eprint.iacr.org/2023/437. Castro, Leo de, Andrew W. Lo, Taylor Reynolds, Fransisca Susan, Vinod Vaikuntanathan, Daniel Weitzner, and Nicolas Zhang. 2020. “SCRAM: A Plavorm for Securely Measuring Cyber Risk.” Harvard Data Science Review 2 (3). hdps://doi.org/10.1162/99608f92.b4bb506a. 26 Courtney, Robert H. 1977. “Security Risk Assessment in Electronic Data Processing Systems.” In Proceedings of the June 13-16, 1977, NaGonal Computer Conference on - AFIPS ’77, 97. Dallas, Texas: ACM Press. hdps://doi.org/10.1145/1499402.1499424. Edwards, Benjamin, Steven Hofmeyr, and Stephanie Forrest. 2016. “Hype and Heavy Tails: A Closer Look at Data Breaches.” Journal of Cybersecurity 2 (1): 3–14. hdps://doi.org/10.1093/cybsec/tyw003. Eling, Mar@n, and Jan Wirfs. 2019. “What Are the Actual Costs of Cyber Risk Events?” European Journal of OperaGonal Research 272 (3): 1109–19. hdps://doi.org/10.1016/j.ejor.2018.07.021. EU. 2022. “Direc@ve (EU) 2022/2555 of the European Parliament and of the Council of 14 December 2022 on Measures for a High Common Level of Cybersecurity across the Union, Amending Regula@on (EU) No 910/2014 and Direc@ve (EU) 2018/1972, and Repealing Direc@ve (EU) 2016/1148 (NIS 2 Direc@ve) (Text with EEA Relevance).” December 14, 2022. hdps://eur-lex.europa.eu/eli/dir/2022/2555. Fink, Glenn A., Christopher L. North, Alex Endert, and Stuart Rose. 2009. “Visualizing Cyber Security: Usable Workspaces.” In 2009 6th InternaGonal Workshop on VisualizaGon for Cyber Security, 45–56. hdps://doi.org/10.1109/VIZSEC.2009.5375542. Freund, Jack, and Jack Jones. 2014. Measuring and Managing InformaGon Risk: A FAIR Approach. Buderworth-Heinemann. “FTC Safeguards Rule: What Your Business Needs to Know.” 2022. Federal Trade Commission. April 27, 2022. hdps://www.uc.gov/business-guidance/resources/uc-safeguards-rule- what-your-business-needs-know. Gordon, Lawrence A., Mar@n P. Loeb, and Tashfeen Sohail. 2003. “A Framework for Using Insurance for Cyber-Risk Management.” CommunicaGons of the ACM 46 (3): 81–85. hdps://doi.org/10.1145/636772.636774. Grossi, Patricia, Howard Kunreuther, and Don Windeler. 2005. “An Introduc@on to Catastrophe Models and Insurance.” In Catastrophe Modeling: A New Approach to Managing Risk, edited by Partricia Grossi and Howard Kunreuther, 23–42. Catastrophe Modeling. Boston, MA: Springer US. hdps://doi.org/10.1007/0-387-23129-3_2. Hubbard, Douglas W., and Richard Seiersen. 2016. How to Measure Anything in Cybersecurity Risk. Hoboken, NJ, USA: John Wiley & Sons, Inc. hdps://doi.org/10.1002/9781119162315. Humphreys, Natalia A. 2021. “Exceedance Probability in Catastrophe Modeling,.” In Casualty Actuarial Society E-Forum, 1–61. hdps://books.google.com/books?hl=en&lr=&id=b8- hDwAAQBAJ&oi=fnd&pg=PA466&dq=exceedance+probability+curve+cybersecurity&ots =KQM_zzNjDr&sig=DquooFaJZQ_27bv- GGPkLiKluzU#v=onepage&q=exceedance%20probability%20curve%20cybersecurity&f=f alse. Jiang, Liuyue, Asangi Jaya@laka, Mehwish Nasim, Marthie Grobler, Mansooreh Zahedi, and M. Ali Babar. 2022. “Systema@c Literature Review on Cyber Situa@onal Awareness Visualiza@ons.” IEEE Access 10: 57525–54. Kamm, Liina, Dan Bogdanov, Sven Laur, and Jaak Vilo. 2013. “A New Way to Protect Privacy in Large-Scale Genome-Wide Associa@on Studies.” BioinformaGcs 29 (7): 886–93. 27 Lapets, Andrei, Kinan Dak Albab, Rawane Issa, Lucy Qin, Mayank Varia, Azer Bestavros, and Frederick Jansen. 2019. “Role-Based Ecosystem for the Design, Development, and Deployment of Secure Mul@-Party Data Analy@cs Applica@ons.” In 2019 IEEE Cybersecurity Development (SecDev), 129–40. hdps://doi.org/10.1109/SecDev.2019.00023. Mukhopadhyay, Arunabha, Samir Chaderjee, Kallol K. Bagchi, Peteer J. Kirs, and Girja K. Shukla. 2019. “Cyber Risk Assessment and Mi@ga@on (CRAM) Framework Using Logit and Probit Models for Cyber Insurance.” InformaGon Systems FronGers 21 (5): 997–1018. hdps://doi.org/10.1007/s10796-017-9808-5. Mukhopadhyay, Arunabha, Debashis Saha, Ambuj Mahan@, and Asok Podder. 2005. “Insurance for Cyber-Risk: A U@lity Model” 32. NACD. 2023. “NACD 2023 Director’s Handbook on Cyber-Risk Oversight.” hdps://isalliance.org/wp-content/uploads/2023/03/Cyber-Risk-Oversight- Handbook_WEB.pdf. NYDFS. 2023. “Cybersecurity Requirements for Financial Service Companies.” New York State Department of Financial Services, no. 23 NYCRR 500 (November). OECD. 2013. “Exploring the Economics of Personal Data: A Survey of Methodologies for Measuring Monetary Value.” OECD Digital Economy Papers 220. Vol. 220. OECD Digital Economy Papers. hdps://doi.org/10.1787/5k486qtxldmq-en. ———. 2023. “Emerging Privacy-Enhancing Technologies: Current Regulatory and Policy Approaches.” Paris: OECD. hdps://doi.org/10.1787/bf121be4-en. Polychroniadou, An@goni, Gilad Asharov, Benjamin Diamond, Tucker Balch, Hans Buehler, Richard Hua, Suwen Gu, Greg Gimler, Manuela Veloso, and J P Morgan. 2023. “Prime Match: A Privacy-Preserving Inventory Matching System,” August. Sokri, Abderrahmane. 2019. “Cyber Security Risk Modelling and Assessment: A Quan@ta@ve Approach.” In Proc. 18th Eur. Conf. Cyber Warfare Secur.(ECCWS), 466. hdps://books.google.com/books?hl=en&lr=&id=b8- hDwAAQBAJ&oi=fnd&pg=PA466&dq=Cyber+Security+Risk+Modeling+and+Assessment: +A+Quan@ta@ve+Approach&ots=KQM_zzNovp&sig=BSusWuDEHPaJISiQYArKIyPme64. Spiewak, Rebecca L., Taylor W. Reynolds, and Daniel J. Weitzner. 2021. “Ransomware Readiness Index: A Proposal to Measure Current Preparedness and Progress Over Time.” Working Paper. hdps://dspace.mit.edu/handle/1721.1/132615. Staheli, Diane, Tamara Yu, R. Jordan Crouser, Suresh Damodaran, Kevin Nam, David O’Gwynn, Sean McKenna, and Lane Harrison. 2014. “Visualiza@on Evalua@on for Cyber Security: Trends and Future Direc@ons.” In Proceedings of the Eleventh Workshop on VisualizaGon for Cyber Security, 49–56. VizSec ’14. New York, NY, USA: Associa@on for Compu@ng Machinery. hdps://doi.org/10.1145/2671491.2671492. Wang, Qiu-Hong, and Seung Hyun Kim. 2009a. “Cyber Adacks: Cross-Country Interdependence and Enforcement.” In . WEIS. hdps://ink.library.smu.edu.sg/sis_research/3301/. ———. 2009b. “Cyberadacks: Does Physical Boundry Mader?” ICIS 2009 Proceedings, 48. Wheatley, Spencer, Thomas Maillart, and Didier Sornede. 2016. “The Extreme Risk of Personal Data Breaches & The Erosion of Privacy.” The European Physical Journal B 89 (1): 7. hdps://doi.org/10.1140/epjb/e2015-60754-4. 28 Annex Table 2: Observed losses and prorated control weights for the Gap Index (defense) Control Observed losses Equal control weights Prorated control weights by losses: 75% prorated across losses 25% equally across non- losses 5a. Eval employee skills $325,000 4.5% 42.0% 5b. Deliver regular training $90,000 4.5% 11.6% 6b. Test backups $75,000 4.5% 9.7% 6d. Store backups offline $75,000 4.5% 9.7% 1a. Deploy MFA $15,000 4.5% 1.9% 2a. Deploy EDR $0 4.5% 1.5% 2b. Hunt malicious activity $0 4.5% 1.5% 3a. Encrypt in transit $0 4.5% 1.5% 3b. Encrypt at rest $0 4.5% 1.5% 4a. Remove sharing barriers $0 4.5% 1.5% 4b. Threat intelligence $0 4.5% 1.5% 6a. Regular backups $0 4.5% 1.5% 6c. Protect backups $0 4.5% 1.5% 7a. Timely updates & patching $0 4.5% 1.5% 7b. Centralized patch system $0 4.5% 1.5% 7c. Risk-based patching $0 4.5% 1.5% 8a. Codify incident response plan $0 4.5% 1.5% 8b. Test incident response plan $0 4.5% 1.5% 8c. Maintain incident response plan $0 4.5% 1.5% 9a. External pen testing $0 4.5% 1.5% 9b. Red team exercises $0 4.5% 1.5% 10a. Network segmentation $0 4.5% 1.5%