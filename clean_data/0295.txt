Assessment of Cyber-Physical Intrusion Detection and Classiﬁcation for Industrial Control Systems Nils M¨uller, Charalampos Ziras, Kai Heussen Wind and Energy Systems Department Technical University of Denmark Lyngby, Denmark {nilmu; chazi; kh}@dtu.dk Abstract—The increasing interaction of industrial control sys- tems (ICSs) with public networks and digital devices introduces new cyber threats to power systems and other critical infrastruc- ture. Recent cyber-physical attacks such as Stuxnet and Irongate revealed unexpected ICS vulnerabilities and a need for improved security measures. Intrusion detection systems constitute a key security technology, which typically monitors cyber network data for detecting malicious activities. However, a central characteris- tic of modern ICSs is the increasing interdependency of physical and cyber network processes. Thus, the integration of network and physical process data is seen as a promising approach to improve predictability in real-time intrusion detection for ICSs by accounting for physical constraints and underlying process patterns. This work systematically assesses machine learning- based cyber-physical intrusion detection and multi-class classiﬁ- cation through a comparison to its purely network data-based counterpart and evaluation of misclassiﬁcations and detection delay. Multiple supervised detection and classiﬁcation pipelines are applied on a recent cyber-physical dataset, which describes various cyber attacks and physical faults on a generic ICS. A key ﬁnding is that the integration of physical process data improves detection and classiﬁcation of all considered attack types. In addition, it enables simultaneous processing of attacks and faults, paving the way for holistic cross-domain root cause identiﬁcation. Index Terms—cyber-physical, intrusion detection, industrial control systems, machine learning, power systems I. INTRODUCTION In recent years, industrial control systems (ICSs) face an ongoing opening to the internet [1]. Previously isolated sys- tems relying on private networks and speciﬁcally designed protocols increasingly use public networks and digital devices to achieve several business beneﬁts. However, along with advantages such as cost-efﬁciency and process ﬂexibility, new cyber vulnerabilities emerge, as evidenced by a growing number of cyber attacks on ICSs [2]. Besides the introduction of new cyber threats, the ongoing integration of cyber and physical components transforms modern ICSs to complex cyber-physical systems. In such cyber-physical ICSs, failures can originate from a variety of hard- or software faults, human errors or malicious activities. Distinguishing attacks from physical faults or human errors is a particularly challenging task in such highly integrated and complex systems, which complicates the identiﬁcation of root causes. This work is funded by Innovation Fund Denmark (File No. 91363) and Horizon 2020 project INTERPRETER under grant agreement No. 864360. Intrusion detection systems (IDSs) are responsible for de- tecting malicious activities by monitoring and analyzing either ICSs end-device (host-based IDS) or network data (network- based IDS). In recent years, the use of machine learning (ML) for IDSs has attracted increasing interest for reasons such as the ability to capture complex properties of ICS operation and attacks, lower central processing unit (CPU) load compared to conventional IDSs, higher detection speed, reduced need for expert knowledge due to generalizability, and the exploitation of steadily increasing amounts of data in ICSs [3]. In this context, supervised multi-class classiﬁcation comes with the advantage of enabling automated distinction between different types of attacks and other anomalies such as physical faults, facilitating the identiﬁcation of root causes. A promising approach to improve supervised intrusion detection and classiﬁcation is the integration of cyber network with physical process data [4]. In this way, the underlying physical constraints and patterns of an ICS are included into the IDS, potentially improving predictability. A few studies investigate such supervised cyber-physical detection and clas- siﬁcation of cyber attacks. In [5] the authors propose a multi- layer cyber attack detection system, which combines a super- vised and exclusively network data-based classiﬁcation step with an empirical model for detecting abnormal operation in physical process data. However, the authors consider a binary classiﬁcation problem (normal vs. attack) which weakens the determination of causational factors. Moreover, as the dataset was shufﬂed and randomly divided into training and test data, samples of a speciﬁc attack event can be found in both sets, which entails data leakage, weakening the validity of the results. In [6]–[8] ML-based intrusion detection and classiﬁca- tion in ICSs is investigated considering multiple attack types as well as cyber network and physical process features. However, none of these works compares cyber-physical with network in- trusion detection, or evaluates misclassiﬁcations and detection delay. In [9]–[11] the beneﬁt of integrating physical process data into a supervised classiﬁcation-based IDS is demonstrated for robotic vehicles. These works neither consider a multi-class classiﬁcation problem nor a scenario including both cyber attacks and physical faults. The thematically and methodologi- cally closest works are [12] and [13]. In [12] the authors intro- duce the dataset which also forms foundation for the present arXiv:2202.09352v2 [cs.CR] 3 Jan 2023 work. To demonstrate a use case of the dataset, they compare several supervised classiﬁers for intrusion detection. Although the work compares the use of cyber network and physical process data, the integration of both information sources is not considered. Moreover, only a binary classiﬁcation problem is examined. Finally, models are evaluated using K-fold cross- validation (CV), which leads to data leakage as samples from the same attack or fault event are placed in the training and test datasets. In [13] the authors compare several unsupervised and supervised models for cyber-physical intrusion detection in power systems. The study explicitly compares network to cyber-physical intrusion detection. However, in total only four man-in-the-middle (MITM) attack events are considered and investigated individually, again leading to data leakage. The review reveals that existing works on supervised cyber- physical intrusion detection and classiﬁcation either i) lack systematic assessment through comparison to a purely network data-based approach and evaluation of misclassiﬁcations and detection delay, or ii) do not consider multi-class classiﬁca- tion, weakening root cause identiﬁcation, or iii) suffer from methodological issues, limiting the validity of results. This work systematically assesses real-time cyber-physical intrusion detection and multi-class classiﬁcation based on a comparison to its exclusively network data-based counter- part and evaluation of misclassiﬁcations and detection delay. Various supervised detection and classiﬁcation pipelines are implemented and evaluated on a recent dataset of a generic ICS [12], describing several cyber attack and physical fault types based on physical process and cyber network data. A. Contribution and paper structure The main contributions of this work are as follows: • Systematic comparison of ML-based network and cyber- physical intrusion detection and multi-class classiﬁcation for ICSs, evaluating multiple classiﬁcation pipelines. • Attack and fault class-wise analysis of misclassiﬁcations and detection delay for cyber-physical intrusion detection. • Proposal and assessment of prediction ﬁltering for reduc- tion of misclassiﬁcations. • Transferability evaluation of the investigated generic ICS to control systems in the power sector. The remainder of the paper is structured as follows. In Section II the investigated dataset is introduced, and the transferability of the underlying ICS to power sector control systems evaluated. Section III provides a description of the data preparation as well as applied models and techniques. In Section IV results are presented and discussed, followed by a conclusion and a view on future work in Section V. II. DATASET DESCRIPTION AND TRANSFERABILITY This section ﬁrst describes the dataset under investigation (Subsection II-A). Thereafter, Subsection II-B sheds light on the transferability of this work’s results by comparing the investigated generic ICS to control systems in the power sector. A. Dataset The dataset used in this study was acquired from a hardware-in-the-loop water distribution testbed and is intro- duced in [12]. The system distributes water across eight tanks, where one process cycle is deﬁned by a full ﬁlling/emptying process of each tank. This procedure is steadily repeated, rendering it a cyclical process. Water ﬂow between the tanks is realized by valves, pumps, pressure sensors and ﬂow sensors. The process is controlled by a typical supervisory control and data acquisition (SCADA) architecture consisting of multiple sensors and actuators (ﬁeld instrumentation control layer), four programmable logic controllers (PLCs) (process control layer), and a SCADA workstation, including an human-machine interface (HMI) and data historian (supervisory control layer). Communication is conducted via the MODBUS TCP/IP pro- tocol. An additional Kali Linux machine is included for launching cyber attacks. The process consists of four stages, each of which is controlled by one of the four PLCs. The dataset describes the normal operation of the sys- tem as well as several types of cyber attacks and physical faults against different components and communication links. Among the cyber attacks are eight different MITM attacks, ﬁve denial-of-service (DoS), and seven scanning attacks. More- over, three different water leaks and six sensor and pump breakdowns are included as physical events, which partly ap- pear simultaneously and are interchangeably called attacks or faults by the authors. In the present work, they will be referred to as physical faults. The dataset consists of two sub-datasets, namely a cyber network and physical process dataset. While the physical dataset has a constant one-second resolution, the network dataset on average has 2633 observations per second. The raw features are listed in Table I. For a more detailed explanation of the dataset the reader is referred to [12]. TABLE I RAW CYBER NETWORK AND PHYSICAL PROCESS FEATURES. No. Physical features No. Network features 1 Timestamp 1 Timestamp 2-9 Pressure sensor value of tank 1-8 2-3 IP address (src. & dst.)a 4-5 MAC address (src. & dst.) 10-15 State of pump 1-6 6-7 Port (src. & dst.) 16-19 Flow sensor value of ﬂow sensor 1-4 8 Protocol 9 TCP ﬂags 20-41 State of valve 1-22 10 Payload size 11 MODBUS function code 12 MODBUS response value 13-14 No. of packetsb (src. & dst.) aSrc. and dst. refer to source and destination, respectively. bRefers to packets of the same device during the last two seconds. B. Transferability to control systems in the power sector The considered ICS constitutes a generic test bed with characteristics potentially differing from its real-world coun- terparts. To shed light on the transferability of the results of this study, a contextualization of the investigated ICS is required. Prominent representatives of real-world ICSs are control systems in the power sector, such as distribution system operator’s SCADA or substation automation systems (SASs), upon which the following comparison is based. An overview is given in Table II. An important commonality is the SCADA architecture and its components. Thus, the investigated attack scenarios, target- ing various SCADA components, provide realistic scenarios for both system types. Another similarity is the well-deﬁned and steady conﬁguration of the physical process and cyber network. Moreover, both systems show continuous and repet- itive patterns of the physical process either due to process cycles (generic ICS) or seasonality (power systems). Such deterministic conﬁgurations and patterns allow to model both systems with a set of physical and network features. Differences are mainly related to the physical process. In contrast to the investigated ICS, power systems are subject to external inﬂuences such as weather and customer behavior, increasing process volatility. Moreover, compared to a distribu- tion system operator’s SCADA system, the number of physical and network components is relatively small. Thus, a central difference is system complexity due to higher volatility and number of components in power systems. To conclude, the present generic ICS can be considered a valuable test case for smaller control systems in the power sector such as SASs. However, due to a lack of system com- plexity, investigation of intrusion detection for larger SCADA systems will require more extensive test beds. TABLE II COMPARISON OF THE INVESTIGATED ICS TO CONTROL SYSTEMS IN THE POWER SECTOR. No. Similarities No. Differences 1 SCADA architecture & components 1 Volatility & trend pattern 2 Attack types & target components 2 Physical fault types 3 Steady network & process conﬁgu- ration (e.g., IP addresses, protocols & number of physical devices) 3 External impacts (e.g., weather & customer behavior) 4 Continuous, repetitive and thus deter- ministic network & process patterns 4 Type of communication protocol 5 Continuous, discrete & categorical features 5 Number of physical & network components III. METHODOLOGY This section ﬁrst describes the preparation of the dataset. Thereafter, the data pipelines applied to the intrusion detection and classiﬁcation problem are introduced. A. Data preparation 1) Data partitioning: A good practice to test performance and generalization of a fully speciﬁed ML model is the application to a holdout test dataset which stems from the same target distribution as the training set but was not previously seen [14]. The present dataset describes attack or fault events in time series format, where a speciﬁc event corresponds to a sequence of observations. As a speciﬁc event can only occur once, its entire sequence must either be placed in the training or test dataset. Placing observations from a single event in both the training and test set will assume information from future events during model training. Thus, data shufﬂing or CV-based performance evaluation will result in an overly optimistic model performance assessment. In this work, the time series format of the investigated dataset is considered to perform a fair evaluation of the model performance. The entire sequences of the last two events of each attack or fault1 class are reserved for the test dataset and not considered during model training. As a result, the training set consists of the ﬁrst 80 % of all normal operation observa- tions, 73.07 % of the DoS observations, 80.91 % of the MITM observations, 77.75 % of the physical fault observations, and 71.42 % of the scanning observations. In this way, the risk for an overly optimistic performance assessment through data leakage is minimized, while a typical 75/25 ratio between the training and test set can be maintained. Thus, future works examining the present dataset are encouraged to use the same partitioning in order to improve validity and comparison of results. 2) Feature extraction and fusion: Cyber-physical intrusion detection and classiﬁcation simultaneously processes physical process and network trafﬁc data. As these originate from different domains, they usually exhibit unequal characteristics such as observation rates and noise levels. Thus, feature ex- traction from raw data typically requires different approaches for these two data sources. In this work, the extraction of features from raw network trafﬁc is realized by several sample statistics. An overview of the considered statistics and resulting set of network features is given in Table III. The selected statistics evaluate the trafﬁc for each second. The objective is to retain existing and extract additional information compared to consideration of individual data packets, while network and physical process features are aligned and the number of model executions reduced. As discussed in Subsection II-B, ICSs usually exhibit well- deﬁned and static network conﬁgurations, which include, for example, ﬁxed sets of IP addresses or ports. This characteristic can be exploited by statistics which indicate occurrence of instances or instance combinations not present during normal operation. Related features include MAC/IP address mismatch occurrence (Feature no. 2-3) and abnormal instance occur- rence (Feature no. 4-14). The set of normal instances of a raw network feature is extracted from the normal operation observations of the training dataset (see Subsection III-A1). To retain the information detail of individual data packets, an abnormal instance (combination) occurrence is already indicated if a single packet is affected during the considered second. Additional information is extracted by contextualizing packets within each second based on several counts and mean values (see Table III). If, for example, only normal packets are received during a DoS attack, but at an unusual rate, it can only be detected based on the additional information provided by the context of multiple packets. 1Since there are only three water leak events in the dataset, some of which also occur simultaneously with sensor or pump breakdowns, all events of physical faults are combined into a physical fault event class. TABLE III EXTRACTED NETWORK TRAFFIC AND PHYSICAL PROCESS FEATURES. Feature no. Extracted network features Description Underlying raw featuresa 1 Number of data transfers Count of data packets transferred during the last second. Raw network trafﬁc data index 2-3 MAC/IP mismatch occurrence Mismatch indication between the IP and MAC address of at least one network device within the last second. IP and MAC address 4-14 Abnormal instance occurrence Indication of an abnormal instance occurrence within the respective raw network feature during the last second. All raw network features except timestamp and number of packets 15-25 Number of abnormal instance occurrences Count of occurrences of abnormal instances within a sp- eciﬁc raw network feature during the last second. All raw network features except timestamp and number of packets 26-36 Number of normal instance occurrences Count of occurrences of normal instances within a speci- ﬁc raw network feature during the last second. All raw network features except timestamp and number of packets 37-106 Number of occurrences for each instance Individual occurrences count for all instances of the res- pective raw network feature during the last second. All raw network features except timestamp, port and number of packets 107-117 Number of different instances Count of distinct instances of a speciﬁc raw network fea- ture within the last second. All raw network features except timestamp and number of packets 118-128 Number of NaN occurrences Count of NaN occurrences within a raw network feature during the last second. All raw network features except timestamp and number of packets 129-131 Mean value Mean value of the respective raw network feature during the last second. Payload size and number of packets 132-177 Number of different class- speciﬁc instancesb Count of distinct event class-speciﬁc instances of the res- spective raw network feature during the last second. All raw network features except timestamp, IP address and MAC address Feature no. Extracted physical features Description Underlying raw features 178-185 Pressure value of tank 1-8 Raw pressure value of the respective tank. Pressure sensor value of tank 1-8 186-191 State of pump 1-6 Raw state of the respective pump. State of pump 1-6 192-195 Value of ﬂow sensor 1-4 Raw value of the respective ﬂow sensor. Value of ﬂow sensor 1-4 196-217 State of valve 1-22 Raw state of the respective valve. State of valve 1-22 218 Normal progress of a process cycle Normal state of the current process cycle, deﬁned on the range between zero and one. Pressure value of tank 1 219 Sine transformed normal pro- gress of a process cycle Sine transformation of the normal progress of a process cycle. Pressure value of tank 1 220 Cosine transformed normal progress of a process cycle Cosine transformation of the normal progress of a process cycle. Pressure value of tank 1 aSource and destination considered in case of IP address, MAC address, port and number of packets. bEvent class-speciﬁc instances are deﬁned only based on the training dataset. As discussed in Subsection II-B, the volatility, and hence noise level, of raw physical process features is comparatively small in the present case. For that reason, no processing, such as data smoothing, is required and raw features can directly be used (see Table III). In addition, the normal progress of a process cycle is extracted with the associated feature vector P = {p1, p2, ..., pN | pi ∈R∀i} of length N. Values of P are deﬁned on the range pi ∈[0, d], where d corresponds to the usual duration of a process cycle, which is derived from pressure sensor values of tank 1 within the training dataset. While P can represent an expected progress of, for example, 10 percentage points between p = 0.85d and p = 0.95d, the same progress from p = 0.95d to p = 0.05d in the next process cycle is not described properly due to the jump discontinuity. To eliminate the discontinuity, and hence account for the cyclical nature of the process, the additional feature vectors Psin and Pcos are extracted by applying sine and cosine transformation [15] on P according to psin,i = sin 2πpi d  , and pcos,i = cos 2πpi d  , (1) ∀i ∈[1, N]. Note that cosine transformation is required as the sine function alone is not bijective, which would lead to ambiguity in the process cycle progress. In total, 220 features are extracted (see Table III), some of which exhibit constant values and thus are non-informative. After removing the non-informative features, the ﬁnal dataset comprises 161 features and 9185 observations. B. Intrusion detection and classiﬁcation data pipelines Supervised detection and classiﬁcation of attacks and faults constitutes a highly imbalanced multi-class classiﬁcation prob- lem. To improve the classiﬁcation performance, several up- and downstream data transformation steps are considered. Typical steps comprise scaling, dimensionality reduction, un- dersampling and oversampling [16]. Together with classi- ﬁcation and prediction ﬁltering, these deﬁne the intrusion detection and classiﬁcation data pipeline considered in this work (see Fig. 1). Note that prediction ﬁltering stems from result evaluation in Section IV and is not considered during model selection. As depicted in Fig. 1, the data pipeline maps observations of a cyber-physical dataset to the considered event classes. For each of the transformation steps, several candidate methods are considered, which represent the most widely used techniques. Scaling candidates include feature standardization by removing the mean and scaling to unit variance, normalization to values between zero and one, and scaling to the maximum absolute value. Principal component analysis (PCA) with Bayesian selection of the number of prin- ciple components is considered for dimensionality reduction [17]. Methods for undersampling include instance hardness threshold (IHT) and removal of Tomek Links, while synthetic Network features • Transfers: 5328 • Anom. IP: True • ... Physical features • Pump state: On • Cycle stage: 0.6 • ... Network features • Transfers: 5328 • Anom. IP: True • ... Physical features • Pump state: On • Cycle stage: 0.6 • ... Network features • Transfers: 5328 • Anom. IP: True • ... Physical features • Pump state: On • Cycle stage: 0.6 • ... Prediction • Normal • DoS • MITM • Phy. fault • Scan Prediction • Normal • DoS • MITM • Phy. fault • Scan Prediction • Normal • DoS • MITM • Phy. fault • Scan Scaling Dimensionality reduction Undersampling Oversampling Classiﬁcation Prediction ﬁltering • Standardizing • Normalizing [0,1] • Max. value scaling • None • PCA • None • IHT • Tomek Links • None • SMOTE • Borderline SMOTE • None • RF • SVM • ANN • KNN • Majority ﬁlter • None Only applied on training data Only applied on training data Fig. 1. Data pipeline for cyber-physical intrusion detection and multi-class classiﬁcation, which maps observations of a dataset (left side) to event classes (right side). Underscores indicate an exemplary selection of methods for all data transformation steps. minority oversampling technique (SMOTE) and Borderline SMOTE constitute the oversampling methods. Note that under- sampling and oversampling is only applied on training data. The classiﬁcation models include a random forest (RF), k- nearest neighbors (KNN), a support vector machine (SVM) and an artiﬁcial neural network (ANN). Prediction ﬁltering is realized by a moving majority ﬁlter which outputs the most frequent label of the past six predictions. As scanning attacks do not necessarily appear in sequences, they are excluded from the ﬁltering process. All data transformation steps, except for classiﬁcation, can also be bypassed. For some classiﬁers speciﬁc transformation steps, such as bypassing scaling for SVM, are excluded due to numerical issues. The pipeline and most of the embedded data transfor- mation methods and classiﬁcation models are implemented in Python using the scikit-learn library [18]. An exception is the ANN which is implemented using the deep learning library Keras [19]. Due to the multitude of models and techniques considered in this work, theoretical descriptions are omitted for brevity. Thus, for detailed backgrounds, the reader is referred to the respective library documentation as well as [14] and [16]. The selection of transformation methods and hyperparameters is conducted based on the training set TABLE IV METHOD AND HYPERPARAMETER SELECTION RESULTS. Cyber-physical intrusion detection & classiﬁcation pipelines Step RF KNN SVM ANN Scaling Standard. Standard. Max. val. sc. Max. val. sc. Dim. red. None PCA None PCA Unders. None None None None Overs. SMOTE None Bor. SMOTE Bor. SMOTE Classif. nestimators: 100, nmax-features: 17 nneighbors: 5, Dist. func.: manhattan, Weight func.: distance Kernel: radial- basis function, Penalty para.: 10000, Kernel coeff.: 0.0175 nhid.-layers: 2, nunits: 150, Act. func.: Re- Lu, Dropout rate: 0.5, nepochs: 500, Batch size: 512 Network intrusion detection & classiﬁcation pipelines Step RF KNN SVM ANN Scaling Max. val. sc. Standard. Max. val. sc. Max. val. sc. Dim. red. None PCA None PCA Unders. IHT Tomek Links None None Overs. None None None Bor. SMOTE Classif. nestimators: 100, nmax-features: 17 nneighbors: 5, Dist. func.: manhattan, Weight func.: uniform Kernel: radial- basis function, Penalty para.: 10000, Kernel coeff.: 0.0175 nhid.-layers: 2, nunits: 100, Act. func.: Re- Lu, Dropout rate: 0.5, nepochs: 500, Batch size: 256 (see Section III-A1) applying a shufﬂed and stratiﬁed 5-fold CV grid search. Although shufﬂing introduces data leakage during model selection, it is required to ensure observations of each attack type in all folds. While this may result in selection of non-optimal hyperparameters, the evaluation of the selected models is unaffected. The method and hyperparameter selection is summarized in Table IV, where selected pipelines are referred to as the respective classiﬁer. For hyperparameters not deﬁned in Table IV, the library’s default values are used. Before being applied to test data, the selected detection and classiﬁcation pipelines are retrained on the full training set. IV. PERFORMANCE EVALUATION This section assesses the performance of cyber-physical intrusion detection and multi-class classiﬁcation. Subsection IV-A introduces the applied performance metrics. In Subsec- tion IV-B a comparison to network intrusion detection and classiﬁcation is conducted, while Subsection IV-C evaluates detection delay and misclassiﬁcations. A. Metrics To evaluate the class-wise detection and classiﬁcation per- formance, this study considers the F1 score according to F1,i = TPi TPi + 1 2(FPi + FNi), (2) where TPi, FPi and FNi are the number of true positives, false positives and false negatives of the i-th class, respectively. The overall performance is assessed based on a macro average of the class-wise F1 scores, given as F m 1 = PNclasses i=1 F1,i Nclasses , (3) with Nclasses being the number of classes. As seen from (3), the macro average F m 1 treats all classes evenly, which is important given the high cost of missing observations of the less-populated attack classes. Average detection delay of class i and average detection delay over all classes are given by τi = PNevents,i j=1 (tdet,j,i −tstart,j,i) Nevents,i , and τ = PNclasses i=1 τi Nclasses , (4) where Nevents,i is the number of events, tstart,j,i the start time of the j-th event and tdet,j,i the ﬁrst-detection time of the j-th event of the i-th class, respectively. B. Comparison of network and cyber-physical intrusion de- tection and multi-class classiﬁcation In Table V the F1 scores of all detection and classiﬁcation pipelines are listed. The highest scores are in bold, while the second best scores are underlined. For network intrusion de- tection, all models show a similar overall performance, despite the differences on a class level. As expected, physical faults are barely detected with pure network data. However, most models detect some physical fault observations, especially the ANN. It can be concluded that network data provide some information about the physical process, which for instance could result from altered payload sizes or higher NaN occurrences. The use of the cyber-physical feature set improves class- wise and overall performance for all models, with detection of normal observations by the RF being the only exception. This allows the conclusion that incorporating physical process data has the potential to improve supervised intrusion detection and classiﬁcation in ICSs. Interestingly, also the classiﬁcation of scanning attacks improves, although these do not affect the physical process. In fact, the absence of physical impact can be informative in the case where observations of different attack types exhibit similar impact on network trafﬁc. If only one of the attack types also impacts the physical process, classiﬁcation of both will be improved by incorporating phys- ical data, due to less confusion between these attacks. As a result, physical process features also improve detection and classiﬁcation of purely network trafﬁc-affecting attacks. The number of scanning attack observations is small com- pared to the other attack types. Nevertheless, most of the studied cyber-physical pipelines perfectly detect and classify scanning attacks, despite the very few training examples. It can be inferred that the extracted features in Table III very well capture the distinctive characteristics of scanning attacks. Although ANNs can capture highly non-linear and complex relationships, they achieve only second best performance. An explanation may be the insufﬁcient number of observations. However, as training data of cyber attacks usually is scarce, ANNs might not be appropriate for the given problem. The highest overall performance is achieved by the SVM, due to the superior exploitation of physical process information. Moreover, SVMs are known to be accurate also in high dimen- sional spaces, which may be an advantage given the relatively large feature-to-observation ratio. The good performance of the SVM under existence of physical faults demonstrates that TABLE V CLASS-WISE AND AVERAGE F1 SCORES FOR NETWORK AND CYBER-PHYSICAL INTRUSION DETECTION AND CLASSIFICATION. Network features Cyber-physical features Event class RF KNN SVM ANN RF KNN SVM ANN Normal 0.92 0.93 0.86 0.88 0.91 0.94 0.95 0.93 DoS 0.55 0.47 0.96 0.47 0.71 0.49 1.00 0.50 MITM 0.87 0.83 0.42 0.68 0.87 0.88 0.81 0.92 Phy. fault 0.07 0.04 0.00 0.14 0.26 0.06 0.62 0.46 Scanning 0.57 0.67 0.80 0.80 1.00 0.80 1.00 1.00 Avg. (F m 1 ) 0.60 0.59 0.61 0.60 0.75 0.63 0.88 0.76 integration of physical process data also allows simultane- ous detection and classiﬁcation of events of fundamentally different nature, paving the way for holistic cross-domain root cause analysis. Nevertheless, the comparatively low F1 score for physical faults requires further improvements such as comprehensive physical feature extraction. While the overall performance (F m 1 ) on average improves by 15.5 percentage points, the models show very different class-speciﬁc improvements. Although detection of physical faults clearly improves for most models, KNN sets an ex- ception. Moreover, only the SVM and ANN show strong improvements for MITM detection, while the RF shows a comparatively good improvement for DoS attacks. This complementarity suggests further investigation of ensemble modeling, e.g., combination of a SVM and ANN. C. Misclassiﬁcation and detection delay evaluation of cyber- physical intrusion detection and multi-class classiﬁcation Misclassiﬁcations and detection delay are investigated on the SVM considering cyber-physical features due to superior performance. The confusion matrix in Fig. 2 reveals that the SVM mainly confuses physical faults and MITM attacks with normal operation and vice versa. Moreover, it shows almost no confusion among the attack and fault classes. Normal 1460 94% 0 0% 45 3% 41 3% 0 0% DoS 0 0% 42 100% 0 0% 0 0% 0 0% MITM 14 10% 0 0% 128 90% 0 0% 0 0% P. fault 48 39% 0 0% 1 1% 74 60% 0 0% Scanning 0 0% Normal 0 0% DoS 0 0% MITM 0 0% P. fault 2 100% Scanning True event class Predicted event class Fig. 2. Confusion matrix of the SVM for the cyber-physical feature set. To further evaluate the location of misclassiﬁcations, the true and predicted labels of an excerpt of the test dataset are depicted in Fig. 3(a) in time series format. It can be noticed that misclassiﬁcations do not primarily appear during transition between event classes and are rather distributed. However, some increased emergence can be noticed at the transition from physical fault to normal operation (15:55:56) and at the beginning and end of the MITM attack between 16:08:26 and 16:10:01. High misclassiﬁcation densities ex- ist around 16:00:55 and 16:06:20 during normal operation, which might result from unlabeled irregular process behavior or noise. Finally, most misclassiﬁcations occur individually and not in sequences. This ﬁnding suggests ﬁltering of the classiﬁcation output (prediction ﬁltering), as indicated in Fig. 1. Fig. 3(b) depicts the predictions after applying the majority ﬁlter described in Subsection III-B. The implemented ﬁlter is 15:55:40 15:59:32 16:03:24 16:07:16 16:11:08 Normal DoS MITM Phy. fault Scanning (a) Unﬁltered Ground truth Prediction 15:55:42 15:59:34 16:03:26 16:07:18 16:11:10 Normal DoS MITM Phy. fault Scanning (b) Filtered Fig. 3. Unﬁltered (a) and ﬁltered (b) predicted and true test dataset labels. simple and not the result of a purely training data-based model selection process. Thus, results are of preliminary nature and further investigation is required. From a comparison of Fig. 3(a) and (b), it is noticeable that individual misclassiﬁcations are ﬁltered out and only sequences remain, which greatly reduces the number of false positives. A quantitative as- sessment of the additional prediction ﬁltering step in terms of F1 score and detection delay for the test set excerpt of Fig. 3 is given in Table VI. Prediction ﬁltering improves overall detection performance (F m 1 ) by 3 percentage points. While detection of physical faults and MITM attacks greatly improves, the performance of DoS detection decreases. This can be explained by the prediction ﬁltering-induced false negatives and positives at the beginning and end of the initially perfectly classiﬁed DoS events. From Table VI it can also be seen that the unﬁltered SVM immediately detects all event classes except for physical faults. Prediction ﬁltering increases detection delay, since several seconds of an event need to elapse to reach majority within the ﬁlter sequence. TABLE VI COMPARISON OF THE UNFILTERED AND FILTERED SVM BASED ON F1 SCORE AND DETECTION DELAY FOR THE TIME SERIES DEPICTED IN FIG. 3. Model Metric Normal DoS MITM P. fault Scan Average SVM unﬁlt. F1 [-] 0.94 1.00 0.85 0.62 1.00 0.88 τi [s] − 0.00 0.00 3.00 0.00 0.75 SVM ﬁltered F1 [-] 0.96 0.90 0.98 0.72 1.00 0.91 τi [s] − 3.00 3.00 6.00 0.00 3.00 V. CONCLUSION AND FUTURE WORK This work assesses ML-based cyber-physical intrusion de- tection and multi-class classiﬁcation for ICSs. For that pur- pose, a systematic comparison to a purely network data-based approach is conducted, followed by an evaluation of misclassi- ﬁcations and detection delay. An average F m 1 improvement of 15 percentage points across several supervised classiﬁcation pipelines demonstrates the beneﬁt of incorporating physical process data into intrusion detection and classiﬁcation. More- over, simultaneous processing of cyber attacks and physi- cal faults is demonstrated, which paves the way to holistic cross-domain root cause analysis. Based on the evaluation of misclassiﬁcations, ﬁltering of the classiﬁer output (prediction ﬁltering) is proposed to reduce false positives, which, however, comes at the cost of higher detection delays. A remaining problem of cyber-physical intrusion detection and classiﬁcation is the dependency on usually scarce attack samples. A potential solution is seen in applying attack sample-independent unsupervised methods. The often weaker performance of such methods in distinguishing attack types may be counteracted by considering cyber-physical input data. REFERENCES [1] M. R. Asghar, Q. Hu, and S. Zeadally, “Cybersecurity in industrial con- trol systems: Issues, technologies, and challenges,” Computer Networks, vol. 165, 2019. [2] ThoughtLab, “Cybersecurity solutions for a riskier world.” https://thou ghtlabgroup.com/wp-content/uploads/2022/05/Cybersecurity-Solut ions-for-a-Riskier-World-eBook FINAL-2-1.pdf, 2022. [Accessed: 01/09/2022]. [3] P. Mishra, V. Varadharajan, U. Tupakula, and E. S. Pilli, “A detailed investigation and analysis of using machine learning techniques for intrusion detection,” IEEE Communications Surveys Tutorials, vol. 21, no. 1, pp. 686–728, 2019. [4] A. Ayodeji, Y.-k. Liu, N. Chao, and L.-q. Yang, “A new perspective towards the development of robust data-driven intrusion detection for in- dustrial control systems,” Nuclear Engineering and Technology, vol. 52, no. 12, pp. 2687–2698, 2020. [5] F. Zhang, H. A. D. E. Kodituwakku, J. W. Hines, and J. Coble, “Mul- tilayer data-driven cyber-attack detection system for industrial control systems based on network, system, and process data,” IEEE Transactions on Industrial Informatics, vol. 15, no. 7, pp. 4362–4369, 2019. [6] J. Yeckle and S. Abdelwahed, “An evaluation of selection method in the classiﬁcation of scada datasets based on the characteristics of the data and priority of performance,” in Proceedings of the International Conference on Compute and Data Analysis, pp. 98–103, 2017. [7] M. Keshk, N. Moustafa, E. Sitnikova, and G. Creech, “Privacy preser- vation intrusion detection technique for scada systems,” in Military Communications and Information Systems Conference, IEEE, 2017. [8] R. C. B. Hink, J. M. Beaver, M. A. Buckner, T. Morris, U. Adhikari, and S. Pan, “Machine learning for power system disturbance and cyber- attack discrimination,” in 2014 7th International symposium on resilient control systems (ISRCS), pp. 1–8, IEEE, 2014. [9] G. Loukas, T. Vuong, R. Heartﬁeld, G. Sakellari, Y. Yoon, and D. Gan, “Cloud-based cyber-physical intrusion detection for vehicles using deep learning,” Ieee Access, vol. 6, pp. 3491–3508, 2017. [10] T. P. Vuong, G. Loukas, and D. Gan, “Performance evaluation of cyber-physical intrusion detection on a robotic vehicle,” in 2015 IEEE International Conference on Computer and Information Technology, pp. 2106–2113, IEEE, 2015. [11] T. P. Vuong, G. Loukas, D. Gan, and A. Bezemskij, “Decision tree- based detection of denial of service and command injection attacks on robotic vehicles,” in 2015 IEEE International Workshop on Information Forensics and Security (WIFS), pp. 1–6, IEEE, 2015. [12] L. Faramondi, F. Flammini, S. Guarino, and R. Setola, “A hardware- in-the-loop water distribution testbed dataset for cyber-physical security testing,” IEEE Access, vol. 9, 2021. [13] A. Sahu, Z. Mao, P. Wlazlo, H. Huang, K. Davis, A. Goulart, and S. Zonouz, “Multi-source multi-domain data fusion for cyberattack detection in power systems,” IEEE Access, vol. 9, 2021. [14] J. Friedman, T. Hastie, R. Tibshirani, et al., The elements of statistical learning, vol. 1. Springer series in statistics New York, 2009. [15] D. Chakraborty and H. Elzarka, “Advanced machine learning techniques for building performance simulation: a comparative analysis,” Journal of Building Performance Simulation, vol. 12, no. 2, pp. 193–207, 2019. [16] A. Fernndez, S. Garca, M. Galar, R. C. Prati, B. Krawczyk, and F. Herrera, Learning from Imbalanced Data Sets. Springer Publishing Company, Incorporated, 1st ed., 2018. [17] T. Minka, “Automatic choice of dimensionality for pca,” Advances in neural information processing systems, vol. 13, 2000. [18] F. Pedregosa et al., “Scikit-learn.” https://scikit-learn.org/, 2011. [Accessed: 25/01/2022]. [19] F. Chollet, “Keras.” https://keras.io, 2015. [Accessed: 25/01/2022].