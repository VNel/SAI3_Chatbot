LLMs for Cyber Security: New Opportunities Dinil Mon Divakaran, A*STAR Institute for Infocomm Research Sai Teja Peddinti, Google Abstract—Large language models (LLMs) are a class of powerful and versatile models that are beneficial to many industries. With the emergence of LLMs, we take a fresh look at cyber security, specifically exploring and summarizing the potential of LLMs in addressing challenging problems in the security and safety domains. Index Terms: LLM, Deep Learning, Security, Vulnerabilities, Safety Introduction Large Language Models (LLMs) are creating a trans- formational impact in the space of science and tech- nology, giving rise to a wide variety of new applications for various services across diverse industry verticals. Their capability to comprehend and, in particular, to generate contents, represents a paradigm shift that is reshaping the way we interact with computers, leading to the development of numerous innovative applica- tions. Today, LLMs are able to generate text, images, and videos; there are LLM applications that hold con- versations with humans, translate between languages, explain and write code, resolve programming bugs, and so forth. LLMs generally are based on a transformer archi- tecture that uses self-attention mechanism to efficiently learn long-range dependencies of tokens (words or sub-words) in a sequence of data (e.g., a sentence). This has allowed transformer models to not only im- prove upon previous sequence models such as RNNs (Recurrent Neural Networks), but also to train large models of billions and even trillions of parameters on datasets of massive sizes. Importantly, the pretraining of an LLM is unsupervised, removing the burden of labeling large datasets. Like other generative models, LLMs fundamentally aim to recreate data they are trained on. Using these properties, pretrained LLMs have been used to generalize across many tasks, often by fine-tuning on small amounts of labeled data. GPT-4, Gemini, Llama 2, Mistral, Falcon, OLMo (Open Language Model), etc., are some of the well-known LLMs today, while new ones are being built at a rapid pace. Examples of downstream tasks include language translation, sentiment analysis, domain-specific chat- bot conversation, text based image/video generation, assistive medical diagnosis, etc. Unsurprisingly though, such a compelling technol- ogy can be put to dual use. An LLM is fundamentally a probabilistic model, which learns to make predictions based on the massive datasets that it has been trained on; and thus, it is only reasonable that the model may not consistently generate factually accurate, benign, or positive outputs, even if trained to do so. This inher- ent characteristic can be exploited, e.g., via prompt injection attack (discussed later), by malicious actors for various purposes. We refer the reader to the ‘NIST Trustworthy and Responsible AI report (2023)’ [1], for a detailed taxonomy of adversarial machine learning (ML) in the context of both conventional ML as well as LLMs. There are ongoing efforts to mitigate the risks due to LLMs. Companies such as OpenAI (https://openai.com/safety), Google (https: //safety.google/cybersecurity-advancements/saif/), Meta (https://ai.meta.com/responsible-ai/), Microsoft (https://www.microsoft.com/en-us/ai/responsible-ai), etc. have frameworks for developing safe and responsible AI systems. In fact, many of the firms also focus on red teaming LLMs, to proactively investigate and identify vulnerabilities of LLMs, e.g., to detect adversarial prompts that can generate harmful or malicious responses. In 2023, Microsoft, Anthropic, Google, and OpenAI launched the Frontier Model Forum [2] to support best practices to mitigate risks, advance research on AI safety and security, as well as facilitate information sharing among companies and governments. Similarly, companies formed a C2PA coalition [3] to create an open technical standard that will aid in the ability to trace the origin of different types of generated media. Lastly, governments across the world are also working on regulatory frameworks for AI, to protect AI users and user privacy (among others). It is worth noting that, governments are encouraging global collaborative efforts to tackle AI vulnerabilities and security risks (e.g., refer the U.S Executive Order on AI [4], and the European Union’s AI Act [5]). arXiv:2404.11338v1 [cs.CR] 17 Apr 2024 New opportunity to address cyber security problems We now turn to the main focus of this article and dis- cuss the new opportunities LLMs present in addressing security and safety challenges that users today face in the digital world. The cyber security domain has already started to see the benefits of utilizing LLMs for addressing some of the important problems in the domain, and we summarize some of these recent ad- vancements. These efforts can be broadly categorized into five themes described below. Refer to Figure 1 for an overview. LLMs for Vulnerability Detection and Management Today, there are multiple LLM-based tools that are being built to help with code development. Generating code based on natural language description has the promise to transform the software development do- main. Devin AI, GitHub Copilot, IBM’s watsonx, Ama- zon CodeWhisperer and Codeium are some of the emerging AI code assistants. They perform advanced tasks such as code generation and completion, code repair, code refactoring, and code explanation. Besides lowering the entry barrier for software development, these code assistants help in reducing bugs in soft- ware development process. For instance, propagating changes in variable type automatically, although ap- pears simple, is a particularly useful feature that helps developers. The number of CVEs published has been in- creasing over the years and approached close to 29, 000 in 2023 [6]. A 2024 report from Syn- opsys states that the proportion of codebases that have high-risk vulnerabilities—including exploited vulnerabilities—increased from 48% in 2022 to 74% in 2023 [7]. Software vulnerabilities lead to system failures, and malicious actors target the vulnerabilities to launch cyber attacks. While AI-generated programs are not perfect and could also be vulnerable, they hold promise in comparison to human developers—an empirical study by Asare et al. demonstrates less vul- nerabilities introduced by AI code assistants than hu- mans [8]. Another user study assessing LLM-assisted coding of 58 students also indicates low security risk due to LLMs [9]. Besides, researchers are studying how LLMs could be utilized to not only detect vul- nerabilities [10], but also to automatically repair code vulnerabilities [11], [12]. Indeed, the results from [12] are promising: the proposed solution AutoCodeRover resolved 67 GitHub issues, each taking less than 12 minutes; this is much faster than the time taken by human developers (more than two days on average). Google shared that its Gemini model helped success- fully fix 15% of bugs discovered by their sanitizer tools, resulting in hundreds of bugs patched [13, Section 5]. Furthermore, given that LLMs are pretrained on vast amounts of online data including source code and RFCs of protocols, new research illustrates the potential of LLMs in guiding protocol and code fuzzing for vulnerability discovery. The protocol fuzzer ChatAFL [14] capitalizes on the understanding of the RFCs the LLMs have. Fuzz4All [15] is a universal code fuzzer that can target many different input languages and many different features of these languages, and it has been shown to discover bugs and vulnerabilities in software systems. Also, competitions such as the AI Cyber Challenge1, a two-year competition announced in late 2023, organized by DARPA in collaboration with others to design and develop AI-based solutions to secure code, have given momentum to this line of research. LLMs for Content Classification and Enforcement LLMs are being leveraged to augment or automate several general purpose security/safety classifiers, some of which are described below. Safety Classifiers for Policy Enforcement: Toxic contents are on the rise on online platforms. Hate speech, harassment, cyber-bullying, etc. adversely af- fect users of all communities, and in particular un- derrepresented groups. The complexity of this socio- technological problem is amplified by the multilingual nature of communications, the use of evolving lingo, emojis, styles, and so forth. One of the well-known classifiers for toxic content detection that is used by developers and publishers is Google Jigsaw’s Perspec- tive API2. The collaborative team has been publishing tools and data, besides improving the model capabili- ties. There are also a number of ML models proposed in the literature to address this issue. Despite the active research in toxic content detec- tion, the scarcity of large-scale, high-quality data im- pedes research. However, LLMs pretrained on massive data offer a promising direction. As noted previously, LLMs have the capability to solve downstream tasks with a small number of labeled samples, or even without fine-tuning. Indeed, He et al. show that, with prompt learning–giving a few examples at an LLM’s 1https://aicyberchallenge.com/ 2https://perspectiveapi.com/ 2 2024 Protocol Fuzzing Content Moderation Phishing Detection Opportunities due to LLMs Policy Violations Explainability & Prioritization Tackling Data Challenges Mitigating LLM Risks Content Classification & Enforcement Vulnerability Detection & Management Log Analysis Security Data Augmentation LLM Guardrails Safety Classifiers Code Fuzzing Adversarial Example Generation Augment/Automate Manual Reviews Foundational Security LLMs Deepfake Detection Code Explainability and Repair Network Traffic Modeling FIGURE 1: LLMs offer versatile solutions to address a wide range of cyber security challenges. prompt, pretrained LLMs are able to achieve better performance than models trained specifically for toxic content detection [16]. That said, the problem is far from being solved. We have to develop solutions that extend beyond text analysis to detect toxicity in various media formats, including images, audios, videos, and obfuscated messages. Continued research in the field of LLMs, aimed at enhancing their capability to perform on tasks across diverse content formats, holds the potential to offer new solutions for combating toxic content in online platforms. Another area where LLMs are useful is content moderation. Content safety policies often evolve too frequently to catch-up with the different types of threats emerging online. LLM’s zero-shot capabilities are im- mensely valuable in quick enforcement of these evolv- ing policies, or for reducing labeling costs when cre- ating annotated datasets for training down-stream ML models. Kumar et al. [17] show that LLMs (such as GPT-3.5) are effective at rule-based moderation for many Reddit communities, achieving performance close to human moderators for some communities. This early result motivates exploring LLM use for con- tent moderation in other settings. Phishing Detection: Phishing is one of the most common cyber attacks in recent times. Attackers craft and send phishing emails to victims, often including text, image (e.g., brand logo) and a URL to a phishing website. Phishing emails can be targeted to specific individuals (say, a person in the Finance department of a company), and links to phishing websites are also distributed via social media, chats, SMSes, etc. This also presents multiple options for solution development. For example, specific phish- ing detection solutions are integrated with email and SMS gateways. Also, threat intelligence services get URLs from various sources and analyze them using standalone services. A popular service is VirusTotal, which utilizes more than 70 URL-analyzing engines from cyber security vendors and provides aggregate results to users. Despite these protections in place, many (carefully crafted) phishing emails are evading these scanners and reaching users’ mail boxes. Phishing emails. Over the years, phishing email solutions have evolved from relying solely on rules and signatures to the use of ML models to automatically learn patterns of phishing emails. Recently, we also see the use of LLMs for addressing this threat. The phishing detection system D-Fence [18] uses the LLM BERT to generate the embeddings of texts in emails, and subsequently uses the embeddings along with other features to train a model for classifying emails as either phishing or benign. Koide et al. [19] created ChatSpamDetector, that utilizes LLMs to detect phish- ing emails and obtain detailed reasoning for the phish- ing determination. This system is shown to outperform existing baseline detection systems, does not require continuous updates to the detection models and block lists like in existing spam filters, and the generated rationales assist users in making informed decisions when handling suspicious emails. Phishing webpages. A well-known approach to detecting phishing webpages, called reference-based approach, is to compare the logos on a given webpage to a known reference set of logos of popular brands (e.g., Paypal, Amazon, etc.) [20], [21]. The basic idea in reference-based approach is that, if a webpage contains a well-known brand’s logo (e.g., Paypal’s) but has a different domain name, then it is a phishing page. The state-of-the-art solution, Phishpedia [20], trains an object-detection model to detect the logos on screen- shots of webpages and a Siamese model to identify the brand of a detected logo. Making advancement in this direction, in [22], the authors use an LLM to extract brand information from the text present in the HTML pages as well as to detect whether the webpage so- licits user credentials (login/password). This approach enables the detection of phishing pages with or without the presence of logos, expanding the capabilities of existing reference-based detection methods (such as 2024 3 Phishpedia). Another potential research direction for phishing webpage detection is, training or fine-tuning an LLM pretrained on large-scale website dataset for HTML un- derstanding and semantic information extraction [23]. Similar models, and even multi-modal LLMs that can take texts and images as input, could be utilized for building classifiers that detect phishing webpages. As a concrete example, a model pretrained on large-scale webpage dataset can be fine-tuned using benign and phishing pages to develop a phishing webpage clas- sifier. To reduce the high maintenance costs of such LLM-classifiers, distillation techniques can be used to transfer learnings to smaller sized models for wide- scale deployment. The use of Large Language Models (LLMs) in combating phishing attacks is gaining research trac- tion. However, significant challenges remain. Firstly, attackers have the ability to perturb logos of brands they use in their phishing attacks, and thereby evade logo-based or reference-based phishing detectors [24]. Secondly, legitimate logos from popular companies used in single sign-on (SSO) or advertisements can trigger false positives [20]. Finally, not all phishing attacks rely on logos [22]. LLMs for Explainability and Prioritization LLMs, with their natural language interface and the ability to work with data in multiple modalities (text, im- ages, videos, code, etc.), can help with understanding diverse data. Newer LLMs, such as Google’s Gemini Pro 1.5 and Anthropic’s Claude 3 Haiku, boast ex- tremely large context windows of more than 100,000 tokens, enabling them to digest and summarize large amounts of data. These capabilities have opened up new avenues of utilizing LLMs for data explainability, summarization, and for automating or augmenting hu- man reviews. Explainability: Enterprises deploy security solutions from one or more vendors to protect their endpoints. To gain high vis- ibility, modern security solution providers gather de- tailed data from processes, network connections, ap- plications, file/registry accesses, etc., thus resulting in humongous logs. SentinelOne Singularity, CrowdStrike Falcon and Trend Micro Apex are examples of commer- cial EDR (endpoint detection and response) solutions. Besides the logging capability, EDR solutions also come with a set of rules to detect malicious patterns of known malware. Similar problem also exists in the cloud and distributed computing systems. For example, the promising microservice architecture that helps to scale up resources as required for an application, also comes with threats due to insecure packages, misconfigured authentications, etc. The large attack surface exposed due to the distributed nature of the architecture makes it all the more relevant to log information and analyze them in real-time for timely detection of anomalies and attacks. As traditional approach of writing rules to match malicious patterns neither scales nor achieves high de- tection accuracy, security researchers are developing ML models that train on huge amounts of process/audit logs to detect suspicious behaviors. However, this creates another challenge—the detected patterns from the endpoints need to be investigated by security ana- lysts to take the appropriate mitigation steps. Besides, ML models also raise false positives; and a high num- ber of patterns that need to be investigated leads to alert fatigue, which in turn results in missing out high- risks threats and attacks. Cyber defenders’ burn-out is a known chronic problem [13, Section 3]. LLMs are currently being used to explain the detected patterns, to make it easier for an analyst to decide quickly. For example, HuntGPT [25] is a specialized intrusion detection dashboard that uses LLMs to discern pat- terns in network traffic and deliver detected threats in an understandable format. Powered by GPT-3.5-turbo, the system achieved more than 80% success rate at the CISM (Certified Information Security Manager) Practice exams, showing promise in guiding security decisions. Other examples from a recently published Google report [13] include the following. i) The Google Detection & Response teams have leveraged Gem- ini LLM for natural language querying and automatic summarization of alerts data, and have seen a 51% time savings and higher quality results in incident analysis. ii) Google Cloud’s SecLM, a security-specific LLM, facilitates analysts to conversationally search and interact with security events, provides explanations for complex attack graphs, and even recommends mitiga- tions. Similarly, VirusTotal Code Insight explains what a potentially malicious Powershell code is doing [26], and solutions such as CrowdStrike’s Charlotte AI, Google Cloud’s DuetAI, and Microsoft’s Security Copilot also aim to empower security analysts in their threat hunting process. Such assistive solutions can help even non- expert security analysts to detect, investigate, and respond to cyber threats with confidence. Performing content moderation across online plat- forms has very similar challenges, where human re- viewers have to investigate a multitude of (ML or user) flagged posts for policy violations. Kumar et al. [17] 4 2024 show that the reasoning capabilities of LLMs are im- mensely useful in providing explanations and in iden- tifying the specific rules being violated by the policy violating posts, making LLMs a valuable aid for humans performing content moderation. Prioritization: LLMs are also very useful in automating or augmenting manual reviews, and help reduce a reviewer’s fatigue when sifting through detected security incidents or flagged online content. They help evaluate the veracity of identified incidents or policy violations, automat- ing decisions in clear cases and triaging/escalating high risk, complex, or borderlines cases to help focus engineering/expert resources efficiently. For instance, Qiao et al. [27] employed LLMs to scale up content moderation in Google Ads. They were able to reduce the number of manual reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM implementation. Automated decision making of LLMs also helps re- duce exposure of human reviewers to harmful content, thereby enhancing their mental well-being. Puentes et al. [28] propose a Large Language Model (LLM) that analyzes and classifies the information received in reports on sextortion, sexting, grooming, and sexual cyberbullying. Their system even efficiently forwards the reports to competent authorities, and reduces the exposure of analysts to harmful contents. Despite LLM’s strengths in content summariza- tion, explanability, and automation, they are known to be prone to hallucinations—where they generate responses that are factually incorrect, nonsensical, or disconnected (from inputs). Research focusing on ‘grounding’ the LLMs to the provided data can alleviate these concerns. LLMs for Tackling Data Challenges Building highly accurate ML models for security and safety use cases requires large labeled datasets. In the domain on cyber security, there are two challenges in obtaining quality datasets for training models. • Labeling Cost: As in many domains, labeling is a costly task requiring human effort. To de- velop ML models for solving security problems (such as detection of network attacks, malware detection via static and dynamic analyses, etc.) requires large labeled datasets. While the re- search community publishes data once in a while, they are limited in size, may contain ar- tifacts (e.g., malicious datasets for network at- tacks and endpoint logs for malware analysis are often generated via emulation in a controlled environment), or may be obsolete. • Data Privacy and Retention: Another challenge in obtaining real-world dataset is the risk of leak- ing sensitive or confidential information. Con- sider email data (required for phishing detec- tion), social media data (required for content moderation), network traffic, etc., where there is risk of privacy leak. Even though privacy- preserving transformation of data can be per- formed before making the dataset available for research, the risk of leak is so high that, such real-world datasets are only available to re- searchers of the corresponding firm who ‘owns’ the data – which also affects the reproducibility of the research works in this domain. On the other hand, to provide privacy guarantees, com- panies often employ retention timelines when storing user data, that indicate how long the data can be stored and used. Often these retention requirements also get applied to the manually annotated training data, when it is derived from user data. For instance, consider the case of a toxicity model trained on social media data. Based on the policy that a user’s data would be deleted from the social media website within a week after they delete their account, the toxi- city model would start forgetting patterns seen across deleted users’ data. For model perfor- mance benchmarking over time and to avoid for- getting patterns observed in old data, permanent access to annotated training data is necessary. Given the above challenges, LLMs are being ex- plored for data augmentation needs. Data augmen- tation techniques help with diversifying training ex- amples without the need for additional data collec- tion or labeling. For instance, Lee et al. [29] have proposed LLM2LLM, an iterative data augmentation strategy to enhance a small-seed dataset, and have demonstrated that this reduces dependence on labor- intensive data curation while simultaneously achiev- ing improvements over regular fine-tuning in low-data regime tasks. Others are leveraging LLMs for augment- ing training datasets in new languages (to enhance cross-lingual performance of base models [30]), or are exploring synthetic data generation approaches for completely skipping training data annotation [31]. To avoid any privacy leaks, LLMs are also being fine- tuned on sensitive datasets in a ‘differentially private’ way [32]. 2024 5 Traffic Modeling for Network Security: However, data augmentation alone isn’t sufficient. Con- sider network traffic analysis for detection of various threats, anomalies and attacks. Years of research works have led to the development of numerous sta- tistical, ML and data mining algorithms for network security tasks, such as detection of bots, C&C chan- nels used for communication between attacker and compromised hosts, low-rate DDoS attacks, password- spraying attempts, generic anomalies, etc. [33], [34], [35]. Each of these tasks require large amounts of labeled data with minimum noise for training and eval- uating ML models; but large-scale, high-quality data is not available (e.g., see [35]). For example, to train a model for detecting bot traffic to an e-commerce website, the dataset has to have hundreds of thou- sands of labeled network requests that are made by both bots and legitimate users [34]. Yet, it is arguable whether such a dataset helps in building models that can generalize well, given data can come from different operating systems, browsers, locations, etc. Therefore, to generalize, and even to sustain model by retraining, such an e-commerce entity would have to label net- work traffic regularly. The advancements in LLM development present a new opportunity to train domain-specific foundational models in an unsupervised way. In the case of network traffic analysis for detecting threats and attacks, a network-specific foundational LLM that learns network ‘conversations’ (e.g., requests and responses) can be trained using openly available real-world network traffic datasets. CAIDA and MAWI, for example, regularly publish network traces for research purposes; while being massive in size, they are mostly unlabeled. But these unlabeled datasets can be utilized for training a network LLM. Such a foundational LLM can then be fine-tuned for multiple downstream tasks, such as botnet detection. Although fine-tuning is a supervised approach, it typically requires only small amounts of labeled data, thereby decreasing labeling costs signif- icantly. The network research community is witness- ing active discussions in this direction, of training an LLM that learns network communication language (see ACM HotNets 2022 [36] and 2023 [37] proceedings). LLMs for Mitigating LLM Risks With their generative capability, LLMs have lowered the entry barrier for cyber criminals. Phishing emails, tailored to specific roles or individuals, can be gener- ated using LLM applications such as ChatGPT [38]. Researchers at CyberArk outlined how to generate polymorphic malware; the malware runs with ChatGPT API generating new payloads and malicious modules as and when required to evade detection [39]. Secu- rity researchers have already discovered generative AI tools in the dark web marketplaces that help at- tackers with their cyber criminal activities; examples include FraudGPT [40] and WormGPT. And attackers are exploiting the capability of LLMs to generate highly realistic and convincing images, videos, and audio to create Deepfakes [41]. Deepfakes are already being used for unethical and malicious purposes such as spreading misinformation, generating fake news, and defaming individuals. Microsoft lists a number of threat actors that have adopted generative AI tools to launch recent attacks [42]. While the above attacks are not novel per se, their proliferation is enabled by LLMs, specifically due to a new attack vector of LLMs, namely prompt injection. In this attack, an attacker exploits the ability to query LLM models through well-defined APIs and interfaces to either extract sensitive information (such as application product keys), or enable scope for other threats such as remote code injection. For instance, prompt injection attacks enabled researchers from Juniper Networks to trick ChatGPT to generate malware code [43], and the security vendor Bitdefender to solve a CAPTCHA [44]. The attack surface increases when an LLM is extended with data sources to provide more up-to-date informa- tion via retrieval augmented generation (RAG), thereby blurring the line between instruction and data [45]. An example is of an attacker sending an email with malicious instructions that are automatically fed to an LLM application meant for detecting spam or phishing emails, but then inadvertently follows the attacker’s in- structions. Prompt injection attack is recognized as the top LLM related attack by OWASP [46]; and they are of particular concern when new applications interface with an LLM for automated responses [47]. To negate the above mentioned LLM risks and vulnerabilities, there is also research studying and de- ploying a multitude of security risk mitigation strategies, including defining and applying strict policies for mod- erating the input and filtering the output. One approach is to have safeguard checks and controls, also termed as guardrails, in place. For example, safety filters in text-to-image models, such as DALL-E 2 and Mid- journey, prevent generating not-safe-for-work (NSFW) content. Llama Guard [48] from Meta is an LLM trained to classify an LLM prompt or a response as safe. In a recent work [49], researchers have shown that fine- tuning a pretrained DistilBERT model on labeled safe and harmful prompts is more effective in detecting harmful prompts than safety filters of Llama-2, due to fine-tuning on the specific task. There are also 6 2024 independently developed guardrail solutions focusing on a specific data type and task, such as unsafe image detectors (see, e.g., [50]), and LLM firewalls that can block malicious prompts and sensitive/harmful out- puts [51]. Developing these guardrails is a challenging and ongoing effort, as they have to catch up to different models, applications and evolving policies. Besides, recent research shows that bypassing LLM defenses is possible today through prompt injection attacks even when the LLMs are safety-aligned [52], [53]. Countering the challenge of exploiting AI-generated contents (deepfakes) for fraudulent purposes is an active area of research within the AI domain, and one of the interesting research directions is to add water- marks to contents generated by LLMs. A noteworthy contribution in this area comes from Kirchenbauer et al. [54]. Their work proposes a watermarking frame- work that i) generates a watermarked text without re- quiring the LLM to be retrained, ii) enables subsequent identification of watermarked text with negligible false positives. ML-based defense solutions are susceptible to eva- sion attacks. For example, attackers could generate perturbed logos of reputed brands in their phishing campaigns to persuade human users into divulging their credentials. The perturbations are such that they are imperceptible to the human eye while being ef- fective in evading phishing defense solutions based on logo identification (using ML models) [24]. A well- studied approach to counter such evasions is adversar- ial training, where training with adversarial examples (e.g., perturbed logos) can enhance the robustness of defense models against evasion attacks (see [24, Section 7]). With their generative capabilities, LLMs are being leveraged to automate the generation of adver- sarial examples with little human effort [55], [56]. These adversarial examples can then be incorporated into adversarial training to build robust models to defend against threats and attacks. Key takeaways There is an inherent asymmetry between the attackers and defenders in the cyberspace, popularly referred to as “Defender’s Dilemma”, which states that it is suffi- cient for an attacker to succeed once but a defender must be successful in protecting at all times [13]. Machine Learning and Artificial Intelligence (AI), and specifically Large Language Models, have the potential to tilt the scales of cyberspace to give the defenders an advantage over the attackers. The emergence of LLMs presents an opportunity to reimagine how we ap- proach and solve cyber security challenges, enabling the development of innovative solutions by leveraging the capabilities of these powerful models. There are early works indicating that LLMs are helpful in this regard – in defending against software vulnerabilities, phishing attacks, network threats, moderating toxic content on social networks, etc. A recent MIT study has shown that inexperienced workers stand to gain the most from generative AI solutions, such as LLMs, while skilled workers gain incremental benefits [13, Section 5]. In other words, generative AI solutions are democratizing security expertise for everyone and are being termed as the “great equalizer”. Organiza- tions without much security expertise are leveraging AI assistive solutions for improving their security pos- tures. Similarly, experiments are being carried out to evaluate the effectiveness of LLMs in succeeding at security practitioner exams (e.g., CISM), CTF (Capture The Flag) challenges with and without human-in-the- loop [57], etc. The findings suggest these evolving models can narrow the divide between attackers and defenders. On the other hand, LLMs also introduce significant security and privacy challenges, potentially expand- ing the attack surface in organizations where LLMs or LLM-integrated applications are deployed. Factors such as the novelty, scale, efficiency, and effectiveness of potential attacks, coupled with the unprecedented growth of new LLM-powered applications, add to the concerns. However, cyber security stands out as a domain where the concept and practice of red teaming has long been established. Now, red teaming is also being performed on LLM models and applications, during the different phases of LLM training, fine-tuning and operation. This evolution encourages a new syn- ergy between ML and security researchers, architects, and engineers. It is also worth noting that the LLM security domain is witnessing multifaceted activities spanning industry, academia and government bodies, including the development of AI safety frameworks, the formation of alliances, the drafting of regulations, and the definition of processes. This comprehensive approach holds promise for mitigating LLM security risks and pave way for responsible development in this exciting field. REFERENCES 1. A. Vassilev, A. Oprea, A. Fordyce, and H. An- derson, “Adversarial machine learning: A taxonomy and terminology of attacks and mitigations,” tech. rep., National Institute of Standards and Technol- ogy, 2024. Published at https://doi.org/10.6028/ NIST.AI.100-2e2023. 2024 7 2. “Microsoft, Anthropic, Google, and OpenAI launch Frontier Model Forum.” https://blogs.microsoft.com/ on-the-issues/2023/07/26/anthropic-google- microsoft-openai-launch-frontier-model-forum/. Last accessed: Apr. 2024. 3. “Coalition for Content Provenance and Authenticity (C2PA).” https://c2pa.org/. Last accessed: Apr. 2024. 4. “Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.” https://www.whitehouse.gov/briefing- room/presidential-actions/2023/10/30/executive- order-on-the-safe-secure-and-trustworthy- development-and-use-of-artificial-intelligence/. Last accessed: Apr. 2024. 5. “EU AI Act: first regulation on artificial intelligence.” https://www.europarl.europa.eu/topics/en/article/ \20230601STO93804/eu-ai-act-first-regulation-on- artificial-intelligence. Last accessed: Apr. 2024. 6. “Published CVE Records.” https://www.cve.org/ About/Metrics. Last accessed: Apr. 2024. 7. Synopsys, “2024 Open Source Security and Risk Analysis Report.” https://www.synopsys.com/ blogs/software-security/open-source-trends-ossra- report.html. Last accessed: Apr. 2024. 8. O. Asare, M. Nagappan, and N. Asokan, “Is GitHub’s Copilot as Bad as Humans at Introducing Vulnerabil- ities in Code?,” Empirical Softw. Engg., vol. 28, Sep 2023. 9. G. Sandoval, H. Pearce, T. Nys, R. Karri, S. Garg, and B. Dolan-Gavitt, “Lost at c: A user study on the security implications of large language model code assistants,” in 32nd USENIX Security Symposium, 2023. 10. X. Zhou, T. Zhang, and D. Lo, “Large language model for vulnerability detection: Emerging results and future directions,” arXiv preprint arXiv:2401.15468, 2024. 11. H. Pearce, B. Tan, B. Ahmad, R. Karri, and B. Dolan- Gavitt, “Examining Zero-Shot Vulnerability Repair with Large Language Models,” in IEEE Symposium on Security and Privacy (SP), 2023. 12. Y. Zhang, H. Ruan, Z. Fan, and A. Roychoud- hury, “AutoCodeRover: Autonomous Program Im- provement,” arXiv preprint arXiv:2404.05427, 2024. 13. Google, “Secure, Empower, Advance: How AI Can Reverse the Defender’s Dilemma.” https://services.google.com/fh/files/misc/how-ai- can-reverse-defenders-dilemma.pdf, 2024. 14. R. Meng, M. Mirchev, M. Böhme, and A. Roy- choudhury, “Large Language Model guided Protocol Fuzzing,” in Proc. NDSS, 2024. 15. C. Xia, M. Paltenghi, J. Tian, M. Pradel, and L. Zhang, “Fuzz4all: Universal fuzzing with large lan- guage models,” in IEEE/ACM 46th International Con- ference on Software Engineering (ICSE), pp. 910– 910, Apr 2024. 16. X. He, S. Zannettou, Y. Shen, and Y. Zhang, “You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content,” in Proc. IEEE Symposium on Security and Privacy (SP), 2024. 17. D. Kumar, Y. AbuHashem, and Z. Durumeric, “Watch your language: Investigating content mod- eration with large language models,” arXiv preprint arXiv:2309.14517, 2024. 18. J. Lee, F. Tang, P. Ye, F. Abbasi, P. Hay, and D. M. Divakaran, “D-Fence: A Flexible, Efficient, and Com- prehensive Phishing Email Detection System,” in IEEE European Symposium on Security and Privacy (EuroS&P), 2021. 19. T. Koide, N. Fukushi, H. Nakano, and D. Chiba, “ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection,” 2024. 20. Y. Lin, R. Liu, D. M. Divakaran, J. Y. Ng, Q. Z. Chan, Y. Lu, Y. Si, F. Zhang, and J. S. Dong, “Phishpedia: A Hybrid Deep Learning Based Approach to Visually Identify Phishing Webpages,” in 30th USENIX Secu- rity Symposium, 2021. 21. R. Liu, Y. Lin, X. Yang, S. H. Ng, D. M. Divakaran, and J. S. Dong, “Inferring Phishing Intention via Webpage Appearance and Dynamics: A Deep Vision Based Approach,” in USENIX Security Symposium, 2022. 22. Y. Li, C. Huang, S. Deng, M. L. Lock, T. Cao, N. Oo, B. Hooi, and H. W. Lim, “KnowPhish: Large Lan- guage Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection,” arXiv preprint arXiv:2403.02253, 2024. 23. I. Gur, O. Nachum, Y. Miao, M. Safdari, A. Huang, A. Chowdhery, S. Narang, N. Fiedel, and A. Faust, “Understanding HTML with large language models,” arXiv preprint arXiv:2210.03945, 2023. 24. J. Lee, Z. Xin, M. N. P. See, K. Sabharwal, G. Apruzzese, and D. M. Divakaran, “Attacking logo- based phishing website detectors with adversarial perturbations,” in Proc. ESORICS, 2023. 25. T. Ali and P. Kostakos, “HuntGPT: Integrating Ma- chine Learning-Based Anomaly Detection and Ex- plainable AI with Large Language Models (LLMs),” arXiv preprint arXiv:2309.16021, 2023. 26. B. Quintero, “Introducing VirusTotal Code Insight: Empowering threat analysis with generative AI.” https://blog.virustotal.com/2023/04/introducing- virustotal-code-insight.html. Last accessed: Apr. 2024. 27. W. Qiao, T. Dogra, O. Stretcu, Y.-H. Lyu, T. Fang, D. Kwon, C.-T. Lu, E. Luo, Y. Wang, C.-C. Chia, 8 2024 A. Fuxman, F. Wang, R. Krishna, and M. Tek, “Scal- ing Up LLM Reviews for Google Ads Content Moder- ation,” in Proceedings of the 17th ACM International Conference on Web Search and Data Mining, WSDM ’24, Mar. 2024. 28. J. Puentes, A. Castillo, W. Osejo, Y. Calderón, V. Quintero, L. Saldarriaga, D. Agudelo, and P. Ar- beláez, “Guarding the guardians: Automated analy- sis of online child sexual abuse,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3728–3732, 2023. 29. N. Lee, T. Wattanawong, S. Kim, K. Mangalam, S. Shen, G. Anumanchipali, M. W. Mahoney, K. Keutzer, and A. Gholami, “LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement,” arXiv preprint arXiv:2403.15042, 2024. 30. C. Whitehouse, M. Choudhury, and A. F. Aji, “Llm- powered data augmentation for enhanced crosslin- gual performance,” arXiv preprint arXiv:2305.14288, 2023. 31. Y. Meng, M. Michalski, J. Huang, Y. Zhang, T. Ab- delzaher, and J. Han, “Tuning language models as training data generators for augmentation-enhanced few-shot learning,” in Proceedings of the 40th Inter- national Conference on Machine Learning, ICML’23, JMLR.org, 2023. 32. W. Shi, R. Shea, S. Chen, C. Zhang, R. Jia, and Z. Yu, “Just fine-tune twice: Selective differential privacy for large language models,” arXiv preprint arXiv:2204.07667, 2022. 33. Q. P. Nguyen, K. W. Lim, D. M. Divakaran, K. H. Low, and M. C. Chan, “GEE: A Gradient-based Explain- able Variational Autoencoder for Network Anomaly Detection,” in IEEE CNS, pp. 91–99, 2019. 34. S. T. Jan, Q. Hao, T. Hu, J. Pu, S. Oswal, G. Wang, and B. Viswanath, “Throwing Darts in the Dark? Detecting Bots with Limited Data using Neural Data Augmentation,” in IEEE Symposium on Security and Privacy (SP), pp. 1190–1206, 2020. 35. Y. Qing, Q. Yin, X. Deng, Y. Chen, Z. Liu, K. Sun, K. Xu, J. Zhang, and Q. Li, “Low-Quality Training Data Only? A Robust Framework for Detecting Encrypted Malicious Network Traffic,” in Proc. NDSS, 2024. 36. “ACM HotNets 2022.” https:// conferences.sigcomm.org/hotnets/2022/ program.html. Last accessed: Apr. 2024. 37. “ACM HotNets 2023.” https:// conferences.sigcomm.org/hotnets/2023/ program.html. Last accessed: Apr. 2024. 38. D. Kang, X. Li, I. Stoica, C. Guestrin, M. Zaharia, and T. Hashimoto, “Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks,” in Proc. ICML AdvML Frontiers Workshop, 2023. 39. “Chatting Our Way Into Creating a Polymorphic Malware.” https://www.cyberark.com/resources/ threat-research-blog/chatting-our-way-into-creating- a-polymorphic-malware. Last accessed: Apr. 2024. 40. “FraudGPT: The Villain Avatar of ChatGPT.” https://netenrich.com/blog/fraudgpt-the-villain- avatar-of-chatgpt. Last accessed: Apr. 2024. 41. Y. Mirsky and W. Lee, “The creation and detection of deepfakes: A survey,” ACM Computing Surveys (CSUR), vol. 54, no. 1, pp. 1–41, 2021. 42. “Staying ahead of threat actors in the age of AI.” https://www.microsoft.com/en-us/security/blog/2024/ 02/14/staying-ahead-of-threat-actors-in-the-age-of- ai/. Last accessed: Apr. 2024. 43. “Using ChatGPT to Generate Native Code Malware.” https://blogs.juniper.net/en-us/threat-research/using- chatgpt-to-generate-native-code-malware. Last accessed: Apr. 2024. 44. “Bing Chat successfully duped into solving CAPTCHA.” https://www.bitdefender.com/blog/ hotforsecurity/bing-chat-successfully-duped-into- solving-captcha/. Last accessed: Apr. 2024. 45. K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz, “Not What You’ve Signed Up For: Compromising Real-World LLM-Integrated Ap- plications with Indirect Prompt Injection,” in Proc. 16th ACM Workshop on Artificial Intelligence and Security, p. 79–90, 2023. 46. “OWASP Top 10 for Large Language Model Appli- cations.” https://owasp.org/www-project-top-10-for- large-language-model-applications/. Last accessed: Apr. 2024. 47. Y. Liu, G. Deng, Y. Li, K. Wang, T. Zhang, Y. Liu, H. Wang, Y. Zheng, and Y. Liu, “Prompt Injection attack against LLM-integrated Applications,” arXiv preprint arXiv:2306.05499, 2023. 48. H. Inan, K. Upasani, J. Chi, R. Rungta, K. Iyer, Y. Mao, M. Tontchev, Q. Hu, B. Fuller, D. Testuggine, et al., “Llama Guard: LLM-based Input-Output Safe- guard for Human-AI Conversations,” arXiv preprint arXiv:2312.06674, 2023. 49. A. Kumar, C. Agarwal, S. Srinivas, S. Feizi, and H. Lakkaraju, “Certifying LLM safety against adver- sarial prompting,” arXiv preprint arXiv:2309.02705, 2023. 50. “LAION-AI: CLIP-based-NSFW-detector.” https:// github.com/LAION-AI/CLIP-based-NSFW-Detector. Last accessed: Apr. 2024. 51. “Arthur Shield.” https://www.arthur.ai/blog/ announcing-arthur-shield-the-first-firewall-for-llms. Last accessed: Apr. 2024. 52. Z. Ba, J. Zhong, J. Lei, P. Cheng, Q. Wang, Z. Qin, Z. Wang, and K. Ren, “SurrogatePrompt: Bypassing 2024 9 the Safety Filter of Text-To-Image Models via Substi- tution,” arXiv preprint arXiv:2309.14122, 2023. 53. G. Deng, Y. Liu, Y. Li, K. Wang, Y. Zhang, Z. Li, H. Wang, T. Zhang, and Y. Liu, “MASTERKEY: Auto- mated Jailbreaking of Large Language Model Chat- bots,” in Proc. NDSS, 2024. 54. J. Kirchenbauer, J. Geiping, Y. Wen, J. Katz, I. Miers, and T. Goldstein, “A watermark for large language models,” in Proc. ICML (International Conference on Machine Learning), 2023. 55. P. Guo, F. Liu, X. Lin, Q. Zhao, and Q. Zhang, “L- AutoDA: Leveraging Large Language Models for Au- tomated Decision-based Adversarial Attacks,” arXiv preprint arXiv:2401.15335, 2024. 56. L. Struppek, M. H. Le, D. Hintersdorf, and K. Ker- sting, “Exploring the adversarial capabilities of large language models,” 2024. 57. M. Shao, B. Chen, S. Jancheska, B. Dolan-Gavitt, S. Garg, R. Karri, and M. Shafique, “An Empirical Evaluation of LLMs for Solving Offensive Security Challenges,” 2024. Dinil Mon Divakaran (Senior Member, IEEE; dinil_divakaran@i2r.a-star.edu.sg) is a Senior Principal Scientist at the A*STAR Institute for Infocomm Research in Singapore. He is also an Adjunct Assistant Professor of the School of Computing, at the National University of Singapore (NUS). His research experience and interests include topics such as AI for security, network security and privacy, phishing attacks, as well as endpoint protection. He carried out his doctoral studies at ENS Lyon, France, in the joint lab of INRIA and Bell Labs. Sai Teja Peddinti (psaiteja@google.com) is a Staff Research Scientist at Google. His research focuses on applying machine learning/AI and data analysis tech- niques to build novel privacy and security solutions. He has published papers in many top research venues. He completed his PhD in Computer Science from New York University (NYU), School of Engineering. 10 2024