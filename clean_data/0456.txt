Multi-Agent Actor-Critics in Autonomous Cyber Defense Mingjun Wang World Wide Technology Email: mingjunwang88@gmail.com Remington Dechene World Wide Technology Email: Remington.Dechene@wwt.com Abstract—The need for autonomous and adaptive defense mechanisms has become paramount in the rapidly evolving landscape of cyber threats. Multi-Agent Deep Reinforcement Learning (MADRL) presents a promising approach to enhancing the efficacy and resilience of autonomous cyber operations. This paper explores the application of Multi- Agent Actor-Critic algorithms which provides a general form in Multi-Agent learning to cyber defense, leveraging the collaborative interactions among multiple agents to detect, mitigate, and respond to cyber threats. We demonstrate each agent is able to learn quickly and counter act on the threats autonomously using MADRL in simulated cyber-attack scenarios. The results indicate that MADRL can significantly enhance the capability of autonomous cyber defense systems, paving the way for more intelligent cybersecurity strategies. This study contributes to the growing body of knowledge on leveraging artificial intelligence for cybersecurity and sheds light for future research and development in autonomous cyber operations. Keywords: Deep Learning, Multi-Agent Reinforcement Learning, Cybersecurity . 1. Introduction Network security is characterized by significant asym- metry, as defenders must ensure the continuous protection of the network’s components from various attackers, while adversaries need only exploit a single weak entry point at any given time. This entails a great demand for the con- tinuous automates cyber-defense systems. The majority of the current security techniques are categorized as intrusion detection system(IDS). Early generations of IDS were pre- dominately rule-based. However, recent advancements have seen application of the machine learning and deep learning in cyber-defense. These models utilize traffic features such as packet header information to predict the likelihood of a traffic being a threat. Unfortunately, such models often suffer from high false positive rate. Reinforcement Learning(RL) excels in interactive se- quential decision making [1] that cannot easily be solved using analytical solutions. RL algorithms with mindset of optimizing long term goals. This makes it different from the classical supervised learning based models. With the help of deep learning, Deep Reinforcement leaning(DRL) is able to tackle large observation and action spaces. DRL trained agents achieved human and even super-human levels of performance in a range of complex tasks including classic board games such as Go [2], video games ranging from classic Atari [3], autonomous driving [4], and robotics [5]. DRL has also been successfully applied to autonomous net- work defense [6], a task where the defend agent proactively monitors the state of the network, identifies abnormalities, and take actions to remediate. In a more realistic word, security engineers are facing large enterprise networking systems which often compose different segments. These segments may have different op- erational systems and objectives. A single agent would not be able to tackle these complicated networks. Multi- Agent system appears to be a good choice. In this paper, we investigate to utilize MADRL in a cooperative stetting. More specifically we assume each agent shares a common reward. Their common goal is to maximize the long term total rewards. We will explore several variants of Multi- Agent Actor-Critic algorithms in autonomous cyber defense system. 2. Background 2.1. Actor Critic Algorithms Actor-Critic (AC) algorithms are a class of reinforce- ment learning methods that combine the benefits of both policy-based and value-based approaches. They utilize two primary components: the actor and the critic. Actor maps states to a probability distribution over action space. Critic measures the expected return (future rewards) of being in a given state or state-action pair under policy π. The actor is updated under policy gradient methods maximizing the expected total return by repeatedly estimating the gradient g := ▽θ PT t=0 γtrt The general formalism for Actor-Critic algorithms is based on policy gradient from [1] gt := E[▽θ log πθ(at|st)At] (1) where At = Qπ(ss, at) −Vπ(st) At is advantage function measured by the difference be- tween state-action value and state value. Qπ(st, at) can arXiv:2410.09134v1 [cs.CR] 11 Oct 2024 be estimated as rt + Vπ(st, at) More advanced GAE was proposed by [7]. In this paper, we used empirical action value due to it’s simplicity and effectiveness. We represent actor and state value(critic) as the deep neural network. The hyper-parameter information is listed in section 5. There are numerous types of AC algorithms. In this paper, we focus on classical AC from [1], [8](A2C) and Proximal Policy Optimization(PPO) [9] which is a special version of TRPO [10] that can be easily implemented. The difference between the classical policy gradient and PPO is that PPO uses a proxy objective function as the replacement of log πθ(at|st) in 1. maximize E[ πθ(at|st) πθold(at|st)At] (2) subject to E[KL[πold(.|st), πθ(.|st)]] ≤δ (3) The classical policy gradient method improves the policy directly within the parameter space, whereas PPO refines the policy within the policy space itself. This distinction leads to PPO offering greater stability and a more consistent, monotonic improvement during training [10]. Value-based algorithms are highly developed and have demonstrated strong performance in various applications [11]. However, the challenge of decomposing the multi-agent utility func- tion in mixed-agent environments remains unresolved due to the complexity of agent interactions. On the other hand, Actor-Critic methods, which are capable of handling both continuous and discrete action spaces, are particularly effec- tive in environments with continuous action spaces where value-based approaches often encounter difficulties. Both A2C and PPO are on-policy algorithms, meaning they cannot leverage past experiences for learning, which limits their sample efficiency. However, on-policy algo- rithms offer the advantage of mitigating non-stationarity issues in multi-agent settings, as all agents act based on their most recent policies. Despite this benefit, training Actor- Critic methods can be challenging due to their sensitivity to hyperparameters, making the tuning process critical and potentially difficult. 2.2. Multi-Agent Actor Critics Multi-Agent Actor Critics is a direct extension of the actor-critic to multi-agent settings. One naive solution is to train each agent independently without considering the interaction nature among the agents namely independent learning (IL) [12], [13], [14]. One major drawback of IL is that it suffers non-stationarity due to each agent treats other agents as part of the training environment while ignoring the evolving of the other agent’s policy. As a result, independent learning approaches may produce unstable learning and may not converge to any solution. An enhancement to the approach is to train a centralized critic in conjunction with a decentralized policy. In this paper, we model this framework as a decewntralized par- tially observable Markov decision process (DEC-POMDP) with shared rewards. This structure allows for decentralized decision-making by the agents while benefiting from cen- tralized value assessment through the critic [15]. A DEC- POMDP is defined by ⟨S, A, O, R, P, n, γ⟩. S is the state space. A is the shared action space for each agent i. oi is the local observation for agent i. P(s ′|s, a) denotes the transition probability from s to s ′ given the joint action a = (a1,...an) for all n agents. R(s,a) denotes the shared reward function. γ is the discount factor. Agents use a local policy πθ((ai|oi) parameterized by θi to emit an action ai from the local observation oi, and jointly optimize the discounted accumulated reward. E[ T X t=0 γtR(st, at)] where at = (at 1, ..., at N) is the joint action at time step t. This paradigm tackles the non-stationarity during train- ing still maintaining the decentralised fashion namely cen- tralized training and decentralized execution(CTED). [16] provides a general actor-critic framework which is suited to both cooperative and mixed environments. It works with both homogeneous and heterogeneous agents. We can ex- tend policy gradient in equation 1 to centralized learning with gt i := E[▽θi log πi(at i|ot i)Qπ i (xt, at 1, ..., at N)] (4) where oi is the local observation of agent i. Qπ i (xt, at 1, ..., at N) is the centralized version of the action value function. It takes input xt which can be the concatenation of all the agent’s local observations and x = (xt 1, ..., xt N) as the inputs. Equation 4 usually can be replaced by the advantage version: gt i := E[▽θi log πi(at i|ot i)At] (5) where At = Qπ i (xt, at 1, ..., at N) −V π i (xt) At here becomes centralized advantage function as well as for MAPPO. In this paper, we explore both A2C and PPO in the CTED paradigm assuming partial observability for each agent in a cooperative environment in which all the agents share a common reward. Algorithm 1 provides a pseudocode for multi-agent actor-critic implementation. It first initializes the actor and critic for each agent. The actor is only conditional on agent’s local observation. The critic for each agent takes the state which could be the concatenation of each agent’s local observation as input. Given the initial state, each agent acts according to it’s own policy conditional on it’s local obser- vation. After this joined action is applied to the environment, new state is returned from environment. Each agent receives a common reward. When the buffer is full, actor is updated with policy gradient while critic is updated by minimizing the loss between value function and target value function. For PPO, algorithm follows the suit. The difference is that PPO does not use policy gradient, rather it utilizes proxy function 2 to update the policy in the policy space. Algorithm 1 On Policy Multi-Agent Actor-Critic Algorithm Initialize πi(θi) and Vi(ϕi) for each agent i Initialize buffer size D for iteration=1,2,. . . do Receive initial state s, observe each oi for t=1,2,. . . ,max-length do Select ai for each agent i according to πi(a|o; θi) Execute actions a =(a1, ..., aN), observe reward r and new state s′ Store (s, a, r, s′) in buffer D s ←s′ if D is full then for agent i = 1 to N do set yi = ri + γVi(s′; ϕi) set δi = yi −Vi(s; ϕi) Update critic by minimizing the loss: ζ(ϕi) = 1 |D| |D| X j=1 (yj i −Vj i(s; ϕi))2 Update actor using the policy gradient: ▽θiξ = 1 |D| |D| X j=1 ▽θi log πθi(aj i|oj i)δj i end for Clear buffer D end if end for end for 3. Related Work [17] explored both independent Deep Q-Learning(IDQ) and QMIX [18] in a cooperative setting. QMIX is a en- hanced version in the sense that it decomposes overall central action valie Q into multiple non-negative Local Q values respective to each agent. It compared IDQ and QMIX across three scenarios: Confidentiality, Integrity, Availabil- ity. The results showed QMIX outperformed IDQ due to it’s ability to consider more information when computing a central Q value while IDQ only consider each agent’s local observations. [19] utilized DIAL [20] to explore the effectiveness of the communication in the multi-agent CybORG backed environment. [21] went a step further to model the commu- nication among the agents. In this study, the author com- pared DIAL with and without communication and QMIX in several phases. It showed the DIAL with communication outperformed it’s counter-partner DIAL without communi- cation. It also performs comparable to QMIX due to which utilized the fully observed environment in the critic. In this paper, we explore using AC base algorithms since AC provides a more general RL learning framework. It fits the generalized policy iteration paradigm which is funda- mental to nearly all RL learning algorithms. Secondly it can be naturally applied to both continuous and discrete action spaces by only adjusting the actor outputs. With stochas- tic discrete policy, the actor needs to output a categorical distribution and takes a sample from it. With continuous actions space, the actor outputs a Gaussian distribution and takes a sample of it or through reparameteration trick [22]. Actor also can outputs a deterministic vector of real values with continues action space [23]. Lastly Actor-critic is more scalable during training by applying parameter sharing when facing the homologous agent environments. 4. Autonomous Cyber Defense Environment We leverage CybORG environment [24] known through a series of CAGE Challenges. Challenge4 [25] is the most recent one. It provides a multi-agent partially observable cooperative environment. The network structure is seen in Fig 1. This network provides a realistic environment that can be either emulated using Amazon Web Services (AWS) or simulated with Python. The network is divided into four segments: two deployed networks, a Headquarters (HQ) network, and a Contractor network. Each deployed network is structured into two security zones: a restricted zone and an operational zone. The Headquarters network is segmented into three security zones: a Public Access Zone, an Admin Zone, and an Office Network. These networks are interconnected via the internet. Three types of agents operate within the networks: red agents, green agents, and blue agents. Green agents function as regular users, while red agents act as attackers. Our primary focus is on developing blue agents, whose role is to defend the network against penetration by red agents. There are five blue agents in total. Each deployed network contains two blue agents, one for each security zone, and the Headquarters network is protected by a single blue agent responsible for all zones. The red team initiates operations with access to a random machine within the Contractor Network and attempts to move laterally throughout the network. With each turn, there is a small chance that a red agent will spawn if a green agent opens a phishing email or accesses a compromised service. Each zone can host a maximum of one red agent, though these agents can maintain a presence across multiple hosts. While the blue team may successfully eliminate all traces of red agents from a network, red agents will always retain a foothold within the Contractor Network. The primary objective of the agents is to ensure the continuity of routine operations while preventing malicious activities across the network. CAGE4 categorizes the oper- ations into three phases, each with distinct priorities. Our focus will be on the first phase: General Operations and Maintenance. 4.1. Observations CybORG returns raw observations in the form of a python dictionary which provides information related to the previous actions undertaken by both the red and blue agents. The dictionary include the details pertaining to each host Figure 1: Network Structure. such as network subnets, processes and system details. Each agent only be able to observe their local sebnet information and have no knowledge of other subnets’. CAGE4 challenge provides wrappers that support MARL algorithms by con- forming the raw observation to the PettingZoo Environment. PettingZoo is one popular multi-agent gym environments that can be utilized by Deep Reinforcement Learning al- gorithms. The observation space is MultiDiscrete. During training, they are converted into real value vectors by one- hot encoding which makes them suitable input to neural network. 4.2. Actions Blue agents can perform a series discrete actions in order to eliminate the threats from the red agents. The actions are listed in table 1. 4.3. Rewards Blue agents start with zero points. Negative re- wards/penalties are received when green agents are not able to perform their work, when they access a compromised service, and when red agents chooses the Impact action. All rewards for Phase 1 are shown in Table 2 5. Result and Discussion We conducted two multi-agent actor-critics in discrete action spaces. One is classical AC implementation A2C. An- other one is PPO. We explored both independent and central- ized experiments for both algorithms namely independent actor-critic(IAC), multi-agent actor-critic(MAAC), Indepen- dent PPO(IPPO) and multi-agent actor-critic(MAPPO). For both MAAC and MAPPO. We assume each agent only has it’s local observation. During training, we train centralized Actions Description of action Monitor Collection of information about flagged mali- cious activity on the network. Corresponds to action ID 1: Scan in the OpenC2 specification. This action occurs automatically as a default action. Analyze Collection of further information relating to malware, on a specific host to enable blue to better identify if red is present on the system. Corresponds to action ID 30: Investigate in the OpenC2 specification. DeployDecoy Setup of a decoy service (type randomly as- signed) on a specified host. Green agents do not access these services, so any access is an unambiguous indicator of red activity. Decoy services cannot replace legitimate services. Remove Attempting to remove red from a host by destroying malicious processes, files and ser- vices. Restore Restoring a system to a known good state. This has significant consequences for system availability. BlockTraffic Allow traffic to and from the specified zone. AllowTraffic Block traffic to and from the specified zone. If green agents are attempting to communicate to that zone this will result in penalties. TABLE 1: Available Actions for Blue Agents. Zone Local Work Fails Access Service Fails Red im- pact/access HQ Network -1 -1 -3 Contractor Network 0 -5 -5 Restricted Zone A -1 -3 -1 Operational Zone A -1 -1 -1 Restricted Zone B -1 -3 -1 Operational Zone B -1 -1 -1 Internet 0 0 0 TABLE 2: Rewards for Green Action Failures and Compro- mise critics that take as input the concatenation of all agent’s local observation. Each agent takes action only conditional on it’s local observation. To make it more a general case, each agent has it own critic. Also, we did not perform parameter sharing among the agents. This allows the al- gorithms take heterogeneous agents where each agent may have different observation spaces and action spaces. We also perform action masking since at each interaction, some of the actions are not valid. We hope action masking can help the actor perform more effectively. More information for action masking can be found in [26]. The hyper-parameter setup is as in table 3. In the ex- Figure 2: Model Performance. periment, each algorithm has 25 runs. Each run has 1000 episodes. Each episode has the maxim length of 50 steps. Then the average total return for each episode over 25 runs was calculated. The model training results are shown in Fig 2. We see IPPO and MAPPO perform similarly. This reflects the fact IPPO usually provides a strong baseline in multi-agent environment due to it’s stability and efficiency. It’s on policy nature can naturally mitigate the non-stationarity challenge in the multi-agent environments. Also, the agents in CAGE4 environment did not conduct explicit coordination among them to be successful. The segregation of agents into subnets limits their ability to influence the state of other agents. Those agent did not need frequent and intense physical interactions as those in MPEs from PettingZoo. This may further reduce the effectiveness of the centralized critics. Meanwhile, we see centralized MACC outperforms IAC. This shows centralized training indeed improves the effec- tiveness in multi-agent settings. Also, we see the MACC converges early that MAPPO. This is due to both two IAC and MAAC algorithms used larger learning rate. We test multiple learning rates in IAC and MAAC training. Smaller learning rate did not converge while large learning rate did. The reason may be that the agents did not learn quick enough to adapt the ever evolving non-stationary environment with the smaller learning rate. Larger learning rate makes the agent quickly learn the knowledge from the interaction with environment and act accordingly. But IPPO and MAPP was trained with much smaller learning rate, both of them still converge at the reasonable time. It may be due to that PPO learns in the policy space rather than parameter space. This makes PPO agents quickly adjust their policy up to the new environment after each updating. Lastly, we see both MAPP implementations out- perform MAAC’s implementation in both centralized and independent training. This reflects the fact that PPO as trust region base actor-critic has superior performance comparing to classic implementation of actor-critic algorithms due to it’s clipped surrogate objective function. Parameters Actor-Critic PPO Network [64. 64] [64, 64] Leaning Rate 0.05 1e-4 γ 0.99 0.95 Clip 0.2 Steps to Update 50 100 Activation ReLU ReLU Optimizer Adam Adam TABLE 3: Hyperparameters 6. Conclusion We explored both A2C and PPO in partially observable multi-agent cooperative setting. We demonstrated both al- gorithms can effectively learn from the interactions with the simulated environment through both IL and CTDE paradigms. MAPPO performs similarly in both training paradigms. AC performs better in CTDE than in IL. This is due to the less frequently interactions among the agents. In summary, we conclude both algorithms can learn efficiently in the multi-agent settings. Acknowledgments The authors would like to thank AiDN(AI-Defined Net- working) team provide invaluable support on this project. References [1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduc- tion. MIT press, 2018. [2] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al., “Mastering the game of go without human knowledge,” nature, vol. 550, no. 7676, pp. 354–359, 2017. [3] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, “Playing atari with deep reinforce- ment learning,” arXiv preprint arXiv:1312.5602, 2013. [4] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep reinforce- ment learning framework for autonomous driving,” arXiv preprint arXiv:1704.02532, 2017. [5] J. Ibarz, J. Tan, C. Finn, M. Kalakrishnan, P. Pastor, and S. Levine, “How to train your robot with deep reinforcement learning: lessons we have learned,” The International Journal of Robotics Research, vol. 40, no. 4-5, pp. 698–721, 2021. [6] M. Foley, M. Wang, C. Hicks, V. Mavroudis, et al., “Inroads into au- tonomous network defence using explained reinforcement learning,” arXiv preprint arXiv:2306.09318, 2023. [7] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High- dimensional continuous control using generalized advantage estima- tion,” arXiv preprint arXiv:1506.02438, 2015. [8] V. Mnih, “Asynchronous methods for deep reinforcement learning,” arXiv preprint arXiv:1602.01783, 2016. [9] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” arXiv preprint arXiv:1707.06347, 2017. [10] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region policy optimization,” in International conference on machine learning, pp. 1889–1897, PMLR, 2015. [11] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” nature, vol. 518, no. 7540, pp. 529–533, 2015. [12] C. Claus and C. Boutilier, “The dynamics of reinforcement learning in cooperative multiagent systems,” AAAI/IAAI, vol. 1998, no. 746-752, p. 2, 1998. [13] M. Tan, “Multi-agent reinforcement learning: Independent vs. coop- erative agents,” in Proceedings of the tenth international conference on machine learning, pp. 330–337, 1993. [14] C. S. De Witt, T. Gupta, D. Makoviichuk, V. Makoviychuk, P. H. Torr, M. Sun, and S. Whiteson, “Is independent learning all you need in the starcraft multi-agent challenge?,” arXiv preprint arXiv:2011.09533, 2020. [15] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu, “The surprising effectiveness of ppo in cooperative multi- agent games,” Advances in Neural Information Processing Systems, vol. 35, pp. 24611–24624, 2022. [16] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mor- datch, “Multi-agent actor-critic for mixed cooperative-competitive environments,” Advances in neural information processing systems, vol. 30, 2017. [17] J. Wiebe, R. A. Mallah, and L. Li, “Learning cyber defence tactics from scratch with multi-agent reinforcement learning,” arXiv preprint arXiv:2310.05939, 2023. [18] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, et al., “Value-decomposition networks for cooperative multi-agent learning,” arXiv preprint arXiv:1706.05296, 2017. [19] F. Contractor et al., “Learning to communicate in multi-agent rein- forcement learning for autonomous cyber defence,” 2024. [20] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson, “Learn- ing to communicate with deep multi-agent reinforcement learning,” Advances in neural information processing systems, vol. 29, 2016. [21] S. Sukhbaatar, R. Fergus, et al., “Learning multiagent communication with backpropagation,” Advances in neural information processing systems, vol. 29, 2016. [22] D. P. Kingma, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013. [23] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Ried- miller, “Deterministic policy gradient algorithms,” in International conference on machine learning, pp. 387–395, Pmlr, 2014. [24] M. Standen, M. Lucas, D. Bowman, T. J. Richer, J. Kim, and D. Marriott, “Cyborg: A gym for the development of autonomous cyber agents,” arXiv preprint arXiv:2108.09118, 2021. [25] T. C. W. Group, “Ttcp cage challenge 4.” https://github.com/ cage-challenge/cage-challenge-4, 2023. [26] S. Huang and S. Onta˜n´on, “A closer look at invalid action masking in policy gradient algorithms,” arXiv preprint arXiv:2006.14171, 2020.