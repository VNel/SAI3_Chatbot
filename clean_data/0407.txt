TTPXHunter: Actionable Threat Intelligence Extraction as TTPs from Finished Cyber Threat Reports Nanda Rani1, Bikash Saha1, Vikas Maurya1, Sandeep Kumar Shukla1 1Department of Computer Science and Engineering, Indian Institute of Technology Kanpur, Kanpur, India {nandarani,bikash,vikasmr,sandeeps}@cse.iitk.ac.in Abstract—Understanding the modus operandi of adversaries aids organizations to employ efficient defensive strategies and share intelligence in the community. This knowledge is often present in unstructured natural language text within threat analysis reports. A translation tool is needed to interpret the modus operandi explained in the sentences of the threat report and convert it into a structured format. This research introduces a methodology named TTPXHunter for automated extraction of threat intelligence in terms of Tactics, Techniques, and Pro- cedures (TTPs) from finished cyber threat reports. It leverages cyber domain-specific state-of-the-art natural language model to augment sentences for minority class TTPs and refine pinpointing the TTPs in threat analysis reports significantly. We create two datasets: an augmented sentence-TTP dataset of 39, 296 sentence samples and a 149 real-world cyber threat intelligence report- to-TTP dataset. Further, we evaluate TTPXHunter on the aug- mented sentence and report datasets. The TTPXHunter achieves the highest performance of 92.42% f1-score on the augmented dataset, and it also outperforms existing state-of-the-art TTP extraction method by achieving an f1-score of 97.09% when evaluated over the report dataset. TTPXHunter significantly improves cybersecurity threat intelligence by offering quick, actionable insights into attacker behaviors. This advancement automates threat intelligence analysis and provides a crucial tool for cybersecurity professionals to combat cyber threats. Index Terms—Threat Intelligence, TTP Extraction, MITRE ATT&CK, Natural Language Processing, Domain-specific Lan- guage model, TTP Classification, Cybersecurity I. INTRODUCTION In the ever-evolving landscape of cybersecurity, Advanced Persistent Threats (APTs) represent a significant challenge to worldwide security. Countering APTs requires the de- velopment of sophisticated measures, which depend on the detailed extraction and analysis of threat intelligence related to APTs [24], [27], [31]. It involves delving into the attacker’s modus operandi in terms of Tactics, Techniques, and Pro- cedures (TTPs) 1 [7] explained in the threat reports, blogs, bulletins released by security firms [1], [23]. These reports, written in unstructured natural language, describe cyber ad- versaries’ modus operandi. Converting this information into a machine-readable structured format improves threat intelli- gence efforts and is crucial for comprehending and mitigating potential threats [30]. 1Categorized in the MITRE ATT&CK Framework Extracting TTPs from such reports is also crucial for recom- mending defensive mechanisms. However, this process often encounters challenges, including a lack of publicly available structured data, difficulties posed by polymorphic words, and the challenge of interpreting the contextual meanings of sen- tences present in the threat report and mapping them to MITRE ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) TTP [2], [25]. We combat these challenges in one of our previous works, which presents a TTP extraction tool named TTPHunter [2]. It can automatically identify and catalog TTPs with the f1-score of 0.88 [2]. TTPHunter focused on the top 50 TTPs in the ATT&CK framework because of the limited data availability for the remainder of the TTP class. The remainder of the TTPs are particularly those that are either newly emerging or less commonly used. This gap undermines the effectiveness of TTPHunter, as incomplete TTP intelligence can lead to a reactive rather than proactive security posture. Addressing this gap by advancing TTPHunter is crucial to ensure that threat intelligence is exhaustive and reflects the diverse adversarial tactics. Therefore, this study presents TTPXHunter, an extended version of TTPHunter [2], which meticulously refines and expands to recognize an impressive array of 193 TTPs. We address the limited data problem of TTPHunter by intro- ducing an advanced data augmentation method. This method meticulously preserves contextual integrity and enriches our training dataset with additional examples for emerging and less commonly used TTPs. Moreover, we also employ a domain-specific language model that is finely tuned to grasp the nuanced, context-driven meanings within this domain and enhance both the augmentation process and the TTP classi- fication. Further, we also enable TTPXHunter to convert the extracted TTPs into the machine-readable format, i.e., STIX, which facilitates the automated exchange, easier analysis, and integration across different security tools and platforms [18], [34]. This advancement significantly broadens the scope of threat intelligence gleaned from threat reports and offers deeper insights into the TTPs employed by cyber adversaries in APT campaigns. By extending TTPHunter’s extraction capabilities, this research contributes to the critical task of threat intelligence gathering, providing security analysts and arXiv:2403.03267v3 [cs.CR] 21 Mar 2024 TABLE I NOTATIONS AND THEIR DESCRIPTION Notation Description Si ith sentence present in the report wi ith word in the sentence n No. of sentences present in the report Aug S List of augmented sentences for the sentence S θ Threshold to choose relevant sentences Sembed Embedding vector for sentence S Top 5 Top-5 selected words to create augmented sentence f(.) Embedding function M Fine-tuned classifier v Feature vector ti Predicted TTP class for ith sentence in the report Θ Threshold to filter irrelevant sentences from threat report SCORE Similarity score between augmented and original sentence ˆy True label multi-hot vector for given threat report y Predicted label multi-hot vector for given threat report HL Hamming Loss R A threat report which consists of n sentences (S) T Set of unique TTPs present in the database m No. of total unique TTPs present in the database ˆT Predicted set of TTPs d No. of words present in the sentence practitioners with a more comprehensive toolset for identify- ing, understanding, and countering APTs. The notations and their descriptions used to explain the methodology are listed in Table. I. TTPXHunter applies to a threat report denoted as R and extracts set of TTPs denoted as ˆT explained within the report, where ˆT ⊆T and T = {t1, t2, t3, . . . , tm} which denotes a set of target labels having m number of TTPs. The methodol- ogy initiates with the transformation of sentences segmented from threat report, denoted as R = {S1, S2, . . . , Sn}; n represents the number of sentences in the report, into a high- dimensional feature space, i.e., 768-dimension. This transfor- mation, achieved through a cyber domain-specific embedding function f(.), maps each segmented sentence Si to a 768- dimension feature vector vi as vi = {f(Si) ; ∀Si ∈R} The embedding function encapsulates cybersecurity language’s rich semantic and syntactic properties and the stage for nu- anced inference. TTPXHunter exhibits an inferential capability to predict a specific TTP for each segmented sentence based on its embedded feature vector: ti = M(vi); ti ∈T Where ti represents the predicted TTP class for the feature vector vi, and M embodies the fine-tuned classification model that aids in understanding the complex relationship between the features of the segmented sentence and the TTP class. Further, the TTP extraction from the threat report R is performed as: ˆT = n [ i=1 ti Our key contributions to this research are the following: • We introduce TTPXHunter2, an extended version of TT- PHunter [2], which extracts TTPs from threat intelligence reports using the SecureBERT [3] language model. We fine-tune the model on our prepared augmented sentence- TTP dataset and convert the extracted TTPs into struc- tured and machine-readable STIX format. • We introduce a data augmentation method that utilizes the cyber-domain-specific language model. This approach creates sentences related to TTPs by preserving their contextual meaning while preparing new and varied sen- tences. • By leveraging the presented data augmentation method, we build an augmented sentence-TTP dataset using the TTPHunter dataset prepared from the MITRE ATT&CK knowledgebase. We build the augmented dataset using the MLM (Masked Language Model) feature of the contextual natural language model, and it extends samples from 10, 906 sentences to 39, 296 sentences covering 193 ATT&CK TTPs. • We perform an extensive evaluation to measure the efficiency of TTPXHunter over two different types of datasets: The augmented sentence dataset and the Threat report dataset. We manually label 149 real-word threat reports for the report dataset. The remainder of the paper is structured as follows: Section II discusses the current literature, Section III presents the required background, Section IV discusses presented TTPX- Hunter method, Section V demonstrate the experiment done and evaluate the obtained results, Section VI presents the limitation in proposed method along with it is possible future direction and Section VII concludes this research contribution. II. RELATED WORK Research on TTP extraction from threat intelligence reports is widely based on ontology, graphs, and keyword-phrase matching methods [2], [25]. Initially, Husari et al. [5] present TTPDrill, which extracts threat actions based on ontology and matches TTP’s knowl- edge base with extracted threat action using the BM25 match- ing technique. Legoy et al. [10] present rcATT, a classification tool based on Machine Learning (ML) algorithms. They use Term Frequency–Inverse Document Frequency (TF-IDF) to prepare the dataset and perform multi-class classification for target labels as TTP. The rcATT is based only on sentence keywords, as TF-IDF ignores the sentence’s word sequence and context. The model may fail when synonym words are present and polymorphic nature words (the same words have a different meaning in a different context). Li. et al. [6] introduce AttacKG, a graph template matching technique. The method obtains IOCs and constructs entity-based dependency graphs for every TTP present in ATT&CK by leveraging descriptions on the MITRE website and matching them with the TTP’s templates prepared from MITRE website data. To match the templates, they use the graph alignment algorithm. 2We plan to include the source code link in the camera-ready version. TABLE II TTP EXTRACTION METHODS COMPARISON Research Work Year Extraction Technique Sentence Context- aware Identify Relevant Text STIX Sup- port Domain- specific Capability Range of MITRE ATT&CK TTPs Husari et al. [5] 2017 TF-IDF, Ontology-based, and improved BM25 similarity rank × ✓ ✓ ✓ All TTPs Legoy et al. [10] - rcATT 2020 TF-IDF and ML Models (KNN, Deci- sion Tree, Random Forest, and Extra Tree) × × ✓ × All TTPs Li et al. [6] - AttacKG 2022 Entity-based dependency graph and Graph-alignment algorithm × ✓ × ✓ All TTPs Rani et al. [2] - TTPHunter. 2023 BERT/RoBERTa followed by Linear Classifier ✓ ✓ × × 50 most fre- quently used TTPs only Alam et al. [4] - LADDER 2023 Extract attack phases using a sequence tagging model and map these patterns to TTP using cosine similarity ✓ ✓ × × All TTPs TRAM [8] 2023 SciBERT followed by Linear Classifier ✓ × × × 50 most fre- quently used TTPs only TTPXHunter (Proposed) 2024 Domain-specific SecureBERT with Linear Classifier ✓ ✓ ✓ ✓ All TTPs AttacKG struggles to capture techniques identified by ad- jectives (properties of IOCs) present in the sentences rather than verbs (threat actions), such as masqueraded identity and obfuscated malware. Alam et al. [4] introduce LADDER, a framework designed to enhance cyber threat intelligence (CTI) by extracting and analyzing attack patterns from CTI reports, addressing the limitations of traditional CTI that focuses on static indicators like IP addresses. LADDER contains a subpart named TTPClassifier, which is structured into three key steps: identifying sentences with attack pattern descriptions using a binary classification model, pinpointing and extracting these attack phrases with a sequence tagging model, and finally, mapping these patterns to TTP IDs via cosine similarity method. To deal with the polymorphic nature of words and leverage contextual information in the report sentences, Rani et al. [2] present TTPHunter. This tool leverages the language models BERT and RoBERTa to understand the context of the sentences present in the dataset and map it to the correct TTP ID present in the dataset. Due to a dearth of sentence datasets for many TTPs, TTPHunter is trained for only 50 sets of TTPs and has yet to map the full spectrum of TTPs present in the MITRE knowledge base. After that, MITRE also introduced a TTP extraction tool named TRAM (Threat Report ATT&CK Mapping) [8]. It is based on the scientific BERT model, named SciBERT [22], a fine-tuned BERT model on a collection of scientific reports. In this paper, we extend our recent work TTPHunter [2] to extract TTPs present in the collected threat report. The TTPHunter is fine-tuned on traditional BERT and RoBERTa models, whereas we fine-tune cyber-domain-specific language models to identify TTPs. Aghaei et al. [3] show that the cyber-domain-specific language model can perform better than the traditional model trained on general English sentences. Domain-specific words such as Windows and registry have different meanings regarding general and cybersecurity usage. In addition, we solve the limited dataset problem pointed out by [2] using the data augmentation method. Our proposed method can extract the full spectrum of TTPs in the MITRE ATT&CK knowledgebase. A comparison of our proposed model TTPXHunter with the literature is shown in Table II. III. BACKGROUND This section presents the details of every framework, tool, and method used to get insight into their background, which is required to understand the methodology. A. MITRE ATT&CK Framework The MITRE ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) Framework [7] is a globally ac- knowledged comprehensive knowledge base designed to un- derstand cyber threats and adversary behaviors across different stages of a cyber attack. It introduces a standardized lexicon and framework for the classification and detailed description of attackers’ tactics, techniques, and procedures (TTPs). The ATT&CK framework comprises three primary components: Tactics, Techniques, and Procedures. Tactics represent the objectives or goals adversaries aim to achieve throughout an attack, encompassing 14 distinct categories such as Reconnais- sance, Resource Development, Initial Access, Execution, and others [7]. Techniques detail the specific methods adversaries employ to fulfill these tactics, each with a unique identifier and a comprehensive description. Examples include phishing, password spraying, remote code execution, and PowerShell exploitation. Procedures delve into the detailed execution of techniques by attackers to achieve their tactical objectives, illustrating how specific methods are applied in practice, such as deploying a spearphishing email with malicious attachments to achieve initial access. Beyond cataloging techniques, the ATT&CK framework maps real-world attack scenarios to Sentences Dataset Data Augmentation Natural Language Model Contextual Embeddings Linear Classifier TTP Classifier Training Phase Testing Phase Sentence Extraction IOCs Replacement Prediction Probabilities Yes No Extracted TTP Lists Irrelevant Senteces IOCs Replacement STIX Format Fig. 1. TTPXHunter Architecture known techniques that enable organizations to understand better how adversaries navigate through the stages of an attack. This knowledge fosters the development of proactive defenses and offensive security practices, enhances incident response efforts, and enriches threat intelligence [23], [32]. B. BERT Language Model The natural language model plays a vital role in cyber threat intelligence, particularly in extracting threat intelligence from natural language cybersecurity texts. Among the popular lan- guage models, BERT (Bidirectional Encoder Representations from Transformers) [14] holds significant importance in tasks such as extracting threat intelligence, classifying threat data, NER (Name Entity Recognition), detecting spam and phishing attacks [3], [15], [17]. BERT comprises a 12-layer stacked en- coder part of the transformer, which generates contextualized embeddings for natural language inputs. The training of the BERT model involves two key components: Masked Language Model (MLM) and Next Sentence Prediction (NSP). In the MLM, random words within a sentence are masked, and the model learns to predict the masked word based on the context of surrounding words present in the sentence. The NSP focuses on understanding the context and meaning of a sentence by predicting the likelihood of the next sentence in a given pair of sentences. Both tasks demonstrate the BERT model’s ability to grasp the contextual understanding of sentences and capture the relationships between words within a sentence. The BERT model’s proficiency in understanding sentence context and word relationships makes it valuable for various downstream tasks in cyber threat intelligence. C. TTPHunter Our recent tool, TTPHunter [2], leverages the power of the language model BERT to extract threat intelligence in the form of TTPs from natural language threat report texts. The authors of TTPHunter fine-tune a pre-trained BERT model using a sentence-TTP dataset collected from the MITRE knowledge base to let the model understand the context of TTPs present in the MITRE ATT&CK matrix and map a given sentence to relevant TTPs. One noteworthy feature of TTPHunter is its ability to discern the significance of sentences. Rather than indiscriminately mapping all sentences from a threat report, TTPHunter selectively identifies and considers only those sentences that truly explain TTPs, disregarding irrelevant information. To achieve this, we implement a filtering mecha- nism on the model’s predicted probabilities. Our experiments discovered that sentences with a probability higher than 0.64 are relevant, while those below the threshold were considered irrelevant. We adopt the same threshold for identifying relevant sentences by TTPXHunter. IV. PROPOSED METHODOLOGY: TTPXHUNTER In this study, we introduce TTPXHunter, an extended version of TTPHunter, designed to overcome the challenges of limited data for less commonly encountered TTPs and improve the performance of TTPHunter. It incorporates a novel methodology encompassing data augmentation and domain- specific language models to enhance the performance of TTP classification. The notations used to explain the methodology are listed in Table. I. TTPXHunter follows the idea used in TTPHunter of lever- aging the natural language model to extract TTPs from threat reports. TTPHunter is based on the model, which is fine-tuned on general English sentences. The model trained on general sentences can mislead the contextual embedding for domain- specific words having different contexts comparatively. Cyber domain-specific words such as Windows and Registry have Algorithm 1 Data Augmentation Algorithm Input: S ←[w1, w2, . . . , wd] ▷Input Sentence consisting d−words Output: Aug S ▷List of augmented sentences for the input sentence 1: Aug S ←[ ] 2: for i = 1 to d do 3: S′ ←S 4: S′[i] ←< mask > ▷Mask the ith word in sentence S 5: Predicted words ←SECUREBERTMLM(S′) ▷Predict probable word using MLM 6: Top 5 ←SELECT TOP 5(Predicted words) ▷Select top-5 probable words based on their probabilities 7: for word in Top 5 do 8: S′[i] ←word ▷Augmented sentence obtained by replacing masked token 9: SCORE ←COSINE SIMILARITY(S, S′) 10: if SCORE ≥θ then 11: Aug S ←Aug S ∪S′ ▷Append augmented sentence to output list 12: end if 13: end for 14: end for 15: Return Aug S 16: function COSINE SIMILARITY(S, S′) 17: Sembed ←SENTENCE TRANSFORMER(S) 18: S′ embed ←SENTENCE TRANSFORMER(S′) 19: Return cosine(Sembed, S′ embed) 20: end function entirely different meanings in the cyber domain. Hence, we observe the need for a TTP extraction tool based on domain- specific language models to provide more accurate contextual embeddings. To fill this gap of domain-specific knowledge, we leverage the SecureBERT [3] to fine-tune the proposed TTPXHunter and map sentences to the relevant TTPs they represent. TTPXHunter incorporates a filtration mechanism to identify and exclude irrelevant sentences from the TTP extraction results obtained from the threat reports. The filtering mechanism ensures the model extracts the most pertinent and meaningful TTPs for further analysis and discards unrelated sentences. The overview and architecture of TTPXHunter are shown in Fig. 1. The TTPHunter [2] is limited to 50 prominent TTPs only out of 193 TTPs due to the limited sentence in the database for the remainder of the TTP class, which results in incomplete or limited threat intelligence. Enhancing its extraction capabilities is crucial for developing comprehensive and proactive security strategies by leveraging the full spectrum of threat intelligence. Therefore, we address this problem by presenting a data augmentation method in TTPXHunter. This method creates more samples for the minority TTP class. A. Contextual Data Augmentation We leverage the MLM (Masked Language Model) capa- bility of the natural language model to expand the dataset and address the limited samples problem of TTPHunter for the remainder of the TTP class. For MLM, we employ the domain-specific language model called SecureBERT [3]. In this process, we mask words in each sentence with a special token < mask > and employ SecureBERT to predict the masked word using its MLM capability. SecureBERT provides a list of candidate words and their corresponding probabilities, which maintain the contextual meaning when replacing the masked word. We can see at step 1⃝in Fig. 2, an example of in- put sentence as "Adversary obtained credentials using compromised systems". In this case, if we mask the word "Adversary", then the model predicts possible words that preserve the contextual meaning (step 2⃝in Fig. 2). The probabilities associated with each word indicate the confidence level of the predicted word. We select the top 5 words from this list and generate five new sentences for each input sentence (step 3⃝in Fig. 2). The sentence’s meaning may deviate after replacing the masked word with the predicted word. Therefore, we employ cosine similarity to compare each newly generated sentence with the original sentence (step 4⃝in Fig. 2) and select the sentence with the highest similarity index. To compute the cosine similarity, we generate contextual embedding for both the original and generated sentences using Sentence Transformer [13]. The similarity score ranges between 0 (not similar) to 1 (Exactly similar), and we set a threshold (θ) of 0.975. We retain only those sentences having similarity scores greater than θ. The sentence augmentation algorithm is outlined in Algorithm 1, which we employ to construct the augmented sentence-TTP dataset. More detailed information regarding the distribution of the augmented sentence-TTP dataset is present in the section V-A. TABLE III IOC REPLACEMENT Base Name Example Pattern Regex Registry HKEY_LOCAL_MACHINE\XXX\XXX\XX\Run (HKEY_LOCAL_MACHINE|HKEY_CURRENT_USER|HKEY_CLASSES_ROOT|HKE Y_USERS|HKEY_CURRENT_CONFIG)\\(?:[ˆ\\]+\\)*[ˆ\\]+ Email example@example.com [a-zA-Z0-9._%\+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,} IP Address 00.00.00.00 (\d{1,3}\.){3}\d{1,3} Domain www.domain.com [a-zA-Z0-9.-]+\.[a-zA-Z]{2,} File Path C:\XXX\yy ([a-zA-Z]:\\)?(?:[a-zA-Z0-9_-]+\\)*[a-zA-Z0-9_-]+\.[a-zA-Z0 -9_]+ File Name file.exe [a-zA-Z0-9_-]+\.[a-zA-Z0-9_]+ CVE CVE-YYYY-XXXX CVE-\d{4}-\d{4,} SecureBERT <mask> obtain credentials using compromised systems <CLS> <Sep> Input Attackers: 0.99, Malwares: 0.98, Hackers: 0.978,..... Attackers obtain credentials using compromised systems Malwares obtain credentials using compromised systems Hackers obtain credentials using compromised systems ........ ....... Data Augmentation Adversary obtain credentials using compromised systems Augmented Sentence Input Sentence Cosine Similarity Augmented Sentence Sentences TTP ID Software PackingA FinFisher variant uses a custom packer T1027 Sidewinder has added paths to executables in the Registry to establish persistence T1547 ................................................. ................................................. ........ ........ Metamorfo uses JavaScript to get the system time T1124 Augmented Dataset Predicted words with corresponding Probabilities Encoder Encoder Encoder Encoder 2 1 3 4 5 Fig. 2. Data Augmentation Steps B. Prepossessing & Fine-Tuning The sentences present in the dataset consist of irrelevant structures, which we fix during the pre-processing method and let the data add more value to the dataset. First, we remove the citation references from these sentences, which refer to past attack campaigns or threat reports. Further, we find various IOCs (Indicator of Compromise) patterns present in the sentences, which potentially obscure the contextual understanding of the sentences. For example, IP and do- main addresses, file paths, CVE IDs, emails, and registry paths. We implement an IOC replacement method to over- come the obstruction these patterns impose in sentences. This method uses regular expressions (regex) to substitute IOC patterns with their respective base names. For example, a sentence like "Upon execution, the malware contacts the C2 server at attacker-example.com, drops an executable payload.exe at C:\Users\Default\AppData\Roam ing, and creates an autorun entry in HKEY_LOCAL_MACHI NE\Software\Microsoft\Windows\CurrentVersi on\Run" get transformed to "Upon execution, the malware contacts the C2 server at domain, drops an executable file at file path, and creates an autorun entry in registry". This method lets the model add more contextual information rather than hindering the meaning, which helps better TTP identification. Table III details the considered IOCs example, their corresponding base name, and the use of regex pattern. The processed sentences get passed further for fine-tuning. The finetuning process is similar to the TTPHunter’s fine- tuning. As TTPHunter extracts sentence embedding from the traditional BERT model and passes it to a linear classifier for classification, we employ a similar method in TTPX- Hunter. It first extracts domain-specific embedding using the SecureBERT language model and passes embedding to a linear classifier for TTP classification. The finetuning process is meticulously orchestrated to optimize performance on the pro- cessed sentence dataset. We initiate the process by configuring essential parameters, setting the learning rate to 1e −5 to ensure subtle adjustments to the model, choosing a batch size of 64 to balance computational efficiency with training effectiveness, and training it for 10 epochs. We leverage the Hugging Face’s Transformers library [9] for its robust support of transformer models. Sentences are tokenized using SecureBERT’s tokenizer, aligning with its pre-trained under- standing of language structure, and inputs are standardized to a maximum length of 256 tokens. We finetune our model to let it understand the context for ATT&CK TTP, which further enhances the classification capability. The model’s finetuning Preprocessing SecureBERT Embeddings argmax TTP ID Mapping Text Data (n sentence sample) n x 768 n x 193 Domain-specific Contextual Embedding Linear Classifier Fig. 3. Fine tuning-steps is shown in Fig. 3. C. TTP Extraction TTPXHunter takes finished threat reports as input and breaks the report contents into a list of individual sentences. Then, each sentence is processed and passed to the TTPX- Hunter’s finetuning architecture for classification. The system classifies each sentence to identify TTPs. Finally, it aggregates these TTPs and creates a list of TTPs extracted from the ana- lyzed report. A threat report usually contains many irrelevant sentences that do not reflect TTPs. However, our classification module is a closed-world solution (the input will surely be classified into at least one target class). As a result, irrelevant sentences can also be classified as one of the TTPs in the target class, which can lead to huge false positives. To reduce this effect, we filter irrelevant sentences, similar to TTPHunter. We employ a threshold mechanism in the classification module to filter irrelevant sentences and only map sentences that explain TTPs, i.e., a relevant sentence. We filter irrelevant sentences based on classification confidence score. We fix a threshold (Θ) and filter the sentences if the classifier’s confidence score is below the threshold. We follow the same threshold value experimentally obtained in TTPHunter, i.e., 0.644. Once the linear classifier extracts the list of TTPs present in the threat report, TTPXHunter converts the list to the Struc- tured Threat Information eXpression (STIX) [18] format, sig- nificantly enhances cyber threat intelligence operations. It sup- ports detailed analysis and correlation of threats by providing a rich, structured representation of cyber threat information. This structured format facilitates automated processing and threat response, which increases operational efficiency. Moreover, the consistency and standardization offered by STIX improve communication within and between organizations to ensure a common understanding of cyber threats. V. EXPERIMENTS AND RESULTS This section presents details about the prepared dataset and the experiments. We also discuss the results obtained and chosen performance measures. A. Dataset We compute model performance on our test dataset through sentence-to-TTP mapping. However, in the real world, the threat data is in the form of reports containing a set of sen- tences mixed with relevant and irrelevant sentences. Therefore, we also evaluate the model’s performance report-wise. As a result, we prepared two datasets to measure the efficiency of TTPXHunter and the current literature: 1) The augmented Sentence-TTP dataset and 2) The Report-TTP dataset. 1) Augmented Sentence-TTP Dataset: To prepare the sentence-TTP dataset, we consider the TTPHunter dataset as a base dataset, prepared using MITRE ATT&CK knowledgebase [2]. The base dataset consists of two columns: sentences and their corresponding TTP ID. The dataset consists of 10, 906 sentences over 193 TTPs. We use our proposed data augmen- tation algorithm, explained in Algorithm 1, and extend the dataset to 39, 296 sentences distributed over 193 TTP classes. The TTP ID T1059 (Command and Scripting Interpreter) consists of the highest number of sentences as 800, and TTP ID T1127 (Trusted Developer Utilities Proxy Execution) consists of the lowest number of samples as 3. On average, the number of samples in our dataset is 203. The distribution of data samples is present in Appendix A and a glimpse of the dataset is shown at step 5⃝in Fig 2. 2) Report-TTP Dataset: Generally, threat data is present in the form of threat analysis reports. Therefore, We evaluate TTPXHunter on a document dataset, which demonstrates the performance of TTPXHunter in filtering irrelevant sentences from threat reports. It also tells us how efficiently TTPXHunter can extract threat intelligence from threat reports. We manually collect 149 threat reports published by various prominent security firms, and we manually label the set of TTPs present in each report to prepare the ground truth. The document-TTP dataset contains two columns: threat report and list of TTP present in the corresponding threat report. B. Evaluation We evaluate TTPXHunter using various performance met- rics and compare its performance with several current stud- ies. As our dataset is imbalanced, shown in Appendix Fig. 9, we consider macro-averaged precision, recall, and f1- score [11]. This approach ensures balanced evaluation across all classes [29]. By giving equal importance to each TTP class, these metrics prevent the majority class’s dominance from overshadowing the minority class’s performance. It promotes Precision Recall F1-Score 0.0 0.2 0.4 0.6 0.8 Score 0.84 0.80 0.81 0.94 0.93 0.92 TRAM[8] TTPXHunter(Proposed) Fig. 4. Performance Comparison between TRAM [8] and TTPXHunter over Augmented Dataset the development of effective and fair models across different TTP classes and is essential for nuanced TTP classification tasks. 1) Augmented Sentence-based Evaluation: We finetune TTPXHunter on the prepared augmented dataset and compare the other two BERT-based models, i.e., TRAM [8] and TT- PHunter [2] present in the literature. We divide the prepared augmented dataset into train and test sets with an 80 : 20 ratio. TTPXHunter vs TRAM : We fine-tune the TTPXHunter on the train set and evaluate its performance using the cho- sen performance metrics. Further, we finetune the literature TRAM [8] on the same dataset and evaluate its performance. By employing TRAM on the augmented dataset, we extend the capability of TRAM from the 50 most frequently used TTPs to the full spectrum of TTPs. As a result, it gives common ground for comparing TTPXHunter and TRAM. The result obtained by both methods and their comparison is shown in Fig. 4. As we can see, the TTPXHunter outperforms the TRAM, which reflects the difference between contextual embedding of general scientific BERT and domain-specific BERT embeddings. This result reflects that the domain-specific language model provides a better contextual understanding of embedding than the language model trained on general scientific terms. TTPXHunter vs TTPHunter : We assess the performance of our proposed TTPXHunter alongside state-of-the-art TT- PHunter. Our proposed extraction method, TTPXHunter, per- forms better than state-of-the-art TTPHunter. TTPXHunter’s superiority is due to using a cyber-domain-specific finetuned language model. Sentences containing domain-specific terms, such as ”window” and ”registry,” introduce a distinct context that differs from general English. This distinction allows our method, based on the domain-specific language model, to capture and interpret the contextual meaning more accurately than traditional models. In addition, TTPXHunter can identify Precision Recall F1-Score 0.0 0.2 0.4 0.6 0.8 Score 0.89 0.88 0.88 0.94 0.92 0.92 TTPHunter[2] TTPXHunter(Proposed) Fig. 5. Performance Comparison between TTPHunter [2] and TTPXHunter over TTPHunter’s 50-TTP set the range of 193 TTPs with 0.92 f1-score, whereas TTPHunter is limited to only 50 TTPs. The improvement in the result and the capability to identify all ranges of TTPs make TTPXHunter superior to TTPHunter. Further, to understand the efficiency of the data augmen- tation method and compare the performance of TTPXHunter and TTPHunter on the same base, we evaluate TTPXHunter on the ground of TTPHunter. We only selected 50 TTP sets for which we developed TTPHunter and evaluated both. The obtained result is shown in Fig. 5. As we can see, TTPX- Hunter outperforms TTPHunter on the 50-TTP set ground of TTPHunter. It reflects that augmenting more samples for a 50-TTP set and employing a domain-specific language model enables the classifier to better understand the context of TTP. 2) Report-based Evaluation: In the real world, we have threat reports in the form of natural language rather than sentence-wise datasets, and these reports contain information along with TTP-related sentences. So, we evaluate the TTPX- Hunter on the report dataset, which contains each sample as threat report sentences and a list of TTP explained in the report. Extracting TTPs from threat reports is a multi- label problem because a list of TTP classes is expected as output for any given sample, i.e., threat report in this case. The evaluation of such classification also requires careful consideration because of multi-label classification. Evaluation Metrics: In the multi-label problem, the pre- diction vector appears as a multi-hot vector rather than a one- hot vector in a multi-class problem. In the multi-label case, there may be a situation where not all expected TTP classes were predicted; instead, a subset of them is correctly predicted. However, the prediction may be wrong because the whole multi-hot vector does not match. For example, If the true label set contains {T1, T2, T3} and the predicted label is {T2, T3}, then it may be considered to be a mismatch even though T2 and T3 are correctly classified. So, relying on accuracy may TABLE IV COMPARISON WITH STATE-OF-THE-ART METHODS OVER REPORT DATASET Model Precision (%) Recall (%) F1-score (%) Hamming Loss ATTACKG [6] 88.58 95.22 88.52 0.14 rcATT [10] 30.47 44.03 30.56 0.64 Ladder [4] 92.97 95.73 93.90 0.10 TRAM [8] 94.54 94.33 93.49 0.10 TTPXHunter (Proposed) 97.38 96.15 97.09 0.05 not be a good choice for multi-label problems [28]; instead, we consider hamming loss as a performance metric to deal with such a scenario. The hamming loss measures the error rate label-wise [19], [21], [28]. It calculates the ratio of incorrect labels to all labels. For given k threat reports, the hamming loss is defined as: HL = Pk i=1[yi ⊕ˆyi] k (1) Where, yi and ˆyi are multi-hot predicted labels and true label for ith instance, respectively. The ⊕represents element- wise exclusive OR (XOR) operation. The low hamming loss represents that models make minimal wrong predictions. Further, we also evaluate macro precision, recall, and f1-score by leveraging a multi-label confusion matrix package from sklearn [11]. Then, we calculate true positive, false positive, and false negative for each class and calculate these performance metrics. Further, we calculate the macro average between all classes to get macro-averaged performance metrics for all chosen measures, i.e., precision, recall, and f1-score. We prefer the macro-average method to ignore biases towards the majority class and provide equal weight to all classes. We consider four state-of-the-art methods, i.e., [4], [6], [8], [10] for comparison against TTPXHunter based on these metrics over the report dataset. This comparison aims to understand the effectiveness of TTPXHunter over state-of- the-art for TTP extraction from finished threat reports. These methods provide the list of TTPs extracted from the given threat report and the model confidence score for each. Out of all extracted TTPs, only relevant TTPs are selected based on the threshold mechanism decided by each method. We evaluate the state-of-the-art method’s performance based on the threshold value given in their respective papers. For TTPXHunter, we obtain the same threshold experimentally chosen for TTPHunter, i.e., 0.644. The results obtained from all implemented methods on our report dataset are shown in Table IV. It demonstrates that TTPXHunter outperforms all implemented methods across all chosen metrics, i.e., the lowest hamming loss and the highest other performance metrics. It achieves the highest f1-score of 97.07%, whereas out of all state-of-the-art methods, LADDER [4] performs better than other state-of-the-art methods and achieves 2nd highest performance of 93.09% F1-score. This performance gain over state-of-the-art methods demonstrates the efficiency of the TTPXHunter, and we plan to make it open for the benefit of the community. As this experiment involves 193 target TTP classes, it is challenging to visualize the class-wise performance of the employed models. Therefore, we follow a different way to assess the class-wise efficiency of the employed models. We count the number of TTP classes whose chosen performance metrics lie within a range. We employ a range interval of 0.1, i.e., 10%, to calculate the number of TTP classes whose score falls into the range. We perform this calculation across all five methods and three chosen performance metrics. The obtained results are present in Figures 6, 7, and 8. The observation reveals that most TTP classes analyzed by rcATT fall within the 0 −0.10 range, contributing to its overall lower performance. This performance is due to the reliance on the TF-IDF method to transform sentences into vectors. TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection or corpus, balancing its frequent appearance within a document against its commonness across all doc- uments [12], [20], [26]. This method lacks the ability to understand the context and semantic relationships between words, making it unable to grasp the overall meaning of sentences [16], [33]. ATTACKG, conversely, exhibits a lower score within the 0 −0.10 range for certain TTP classes, which adversely affects its overall performance. However, TRAM shows a minimum performance range of 0.3 −0.4 for TTP classes, which indicates better performance than rcATT and ATTACKG. LADDER ensures a performance score of at least 0.4 −0.5 for a TTP class, which positions it ahead of the aforementioned methods, including rcATT, ATTACKG, and TRAM. Our proposed model, TTPXHunter, assures a minimum score of 0.6 −0.7 for TTP classes, with the ma- jority exhibiting scores between 0.9 −1.0, which underscores TTPXHunter’s significant advantage over the other methods. It demonstrates the effectiveness of domain-specific models for domain-specific downstream tasks. VI. LIMITATIONS & FUTURE DIRECTIONS In addition to the advantages of TTPXHunter, like threat intelligence extraction, threat profiling, and sharing, it has some limitations that one should be careful about while using for actionable insights. The MITRE ATT&CK framework is not a kind of fixed knowledge base. Instead, MITRE threat researchers are continuously updating it. One may need to finetune the model again if new TTPs are added to get the 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Precision-score Range 0 25 50 75 100 125 150 175 Number of Classes TTPXHunter(Proposed) 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Precision-score Range LADDER[4] 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Precision-score Range ATTACKG[6] 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Precision-score Range TRAM[8] 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Precision-score Range RCATT[10] Fig. 6. Class-wise Precision Score Comparison with State-of-the-art Methods 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Recall-score Range 0 25 50 75 100 125 150 175 Number of Classes TTPXHunter(Proposed) 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Recall-score Range LADDER[4] 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Recall-score Range ATTACKG[6] 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Recall-score Range TRAM[8] 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Recall-score Range RCATT[10] Fig. 7. Class-wise Recall Score Comparison with State-of-the-art Methods 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 F1-score Range 0 25 50 75 100 125 150 175 Number of Classes TTPXHunter(Proposed) 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 F1-score Range LADDER[4] 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 F1-score Range ATTACKG[6] 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 F1-score Range TRAM[8] 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 F1-score Range RCATT[10] Fig. 8. Class-wise F1 Score Comparison with State-of-the-art Methods newer TTPs predicted. While finetuning on newer TTPs may require using our proposed data augmentation method to create new augmented sentences specific to newer TTPs. Therefore, we plan to make it public so one can adapt our method for any new upcoming versions of the ATT&CK framework. Further, the TTPXHunter contains a one-to-one classifier model, which maps a given sentence to a single TTP. Like a single sentence, a sentence containing one-to-many mapping may explain more than one TTP. For example, the sentence is "The attacker gained initial access through a phishing email and obtained persistence via run registry modification". TTPXHunter maps this sentence to T1566 (Phishing) or T1037 (Boot or Logon Initialization Scripts) in such a scenario. Ex- tending the capability of TTPXHunter to identify such one-to- many mapping can help us improve the model’s performance. We plan to take up this challenge to improve the efficiency of TTPXHunter in the future. VII. CONCLUSION This research demonstrates the efficiency of domain-specific language models in extracting threat intelligence in terms of TTPs, which can share the attack patterns and acceler- ate the threat response and detection mechanisms. The tool TTPXHunter extends the TTPHunter’s capability over multiple dimensions, such as expanding to the full spectrum of TTP extraction and improving efficiency by leveraging domain- specific language models. We evaluate the efficiency of TTPX- Hunter over the prepared augmented sentence-TTP dataset and report-TTP dataset. On the augmented dataset, TTPXHunter outperforms both BERT-variant models, i.e., TRAM and TT- PHunter. TTPXHunter also outperforms the state-of-the-art TTP extraction methods by achieving the highest F1 score and lowest hamming loss. TTPXHunter’s performance over the report dataset demonstrates the model’s efficiency in capturing relevant sentences from threat reports and correctly classifying them to the TTP class. The conversion of extracted TTPs to STIX makes integrating threat intelligence into security opera- tions easier. TTPXHunter aids in improving the threat analyst’s capability to share intelligence, analyze threats, understand the modus operandi of sophisticated threat actors, and emulate their behavior for red teaming. Therefore, TTPXHunter can support various cybersecurity teams, including red, blue, and purple teams in an organization. REFERENCES [1] Xander Bouwman, Harm Griffioen, Jelle Egbers, Christian Doerr, Bram Klievink, and Michel Van Eeten. A different cup of {TI}? the added value of commercial threat intelligence. In 29th USENIX security symposium (USENIX security 20), pages 433–450, 2020. [2] Nanda Rani, Bikash Saha, Vikas Maurya, and Sandeep Kumar Shukla. ”TTPHunter: Automated Extraction of Actionable Intelligence as TTPs from Narrative Threat Reports.” In Proceedings of the 2023 Australasian Computer Science Week, pp. 126–134, 2023. [3] Ehsan Aghaei, Xi Niu, Waseem Shadid, and Ehab Al-Shaer. ”Se- cureBERT: A Domain-Specific Language Model for Cybersecurity.” In International Conference on Security and Privacy in Communication Systems, pp. 39–56. Springer, 2022. [4] Md Tanvirul Alam, Dipkamal Bhusal, Youngja Park, and Nidhi Rastogi. ”Looking beyond IoCs: Automatically extracting attack patterns from external CTI.” In Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses, pp. 92–108, 2023. [5] Ghaith Husari, Ehab Al-Shaer, Mohiuddin Ahmed, Bill Chu, and Xi Niu. ”Ttpdrill: Automatic and accurate extraction of threat actions from unstructured text of cti sources.” In Proceedings of the 33rd annual computer security applications conference, pp. 103–115, 2017. [6] Zhenyuan Li, Jun Zeng, Yan Chen, and Zhenkai Liang. ”AttacKG: Constructing technique knowledge graph from cyber threat intelligence reports.” In European Symposium on Research in Computer Security, pp. 589–609. Springer, 2022. [7] MITRE. ”ATT&CK Framework.” 2023. Available online: https://attack. mitre.org. [Accessed July 22, 2023]. [8] MITRE. ”Threat Report ATT&CK Mapper (TRAM).” 2023. Available online: https://github.com/center-for-threat-informed-defense/tram/. [Accessed Feb 22, 2024]. [9] Hugging Face. Transformers, 2024. Available online: https://huggingface.co/docs/transformers/en/index [10] Valentine Legoy, Marco Caselli, Christin Seifert, and Andreas Peter. ”Automated retrieval of att&ck tactics and techniques for cyber threat reports.” arXiv preprint arXiv:2004.14322, 2020. [11] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, ”Scikit-learn: Machine Learning in {P}ython,” Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011. [12] G. Salton, A. Wong, and C.-S. Yang, “A vector space model for automatic indexing,” Communications of the ACM, vol. 18, no. 11, pp. 613–620, 1975, ACM New York, NY, USA. [13] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. [14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [15] Nafiz Rifat, Mostofa Ahsan, Md Chowdhury, and Rahul Gomes, “BERT against social engineering attack: Phishing text detection,” in 2022 IEEE International Conference on Electro Information Technology (eIT), IEEE, 2022, pp. 1–6. [16] S. Selva Birunda and R. Kanniga Devi, ”A review on word embedding techniques for text classification,” in Innovative Data Communication Technologies and Application: Proceedings of ICIDCA 2020, pp. 267– 281, Springer, 2021. [17] M. Tikhomirov, N. Loukachevitch, A. Sirotina, and B. Dobrov, “Using BERT and augmentation in named entity recognition for cybersecurity domain,” in Natural Language Processing and Information Systems: 25th International Conference on Applications of Natural Language to Information Systems, NLDB 2020, Saarbr¨ucken, Germany, June 24–26, 2020, Proceedings 25, Springer, 2020, pp. 16–24. [18] S. Barnum, “Standardizing cyber threat intelligence information with the structured threat information expression (STIX),” Mitre Corporation, vol. 11, pp. 1–22, 2012. [19] A. C. P. L. F. de Carvalho and A. A. Freitas, “A tutorial on multi-label classification techniques,” in Foundations of Computational Intelligence Volume 5: Function Approximation and Classification, Springer, 2009, pp. 177–195. [20] K. Sparck Jones, “A statistical interpretation of term specificity and its application in retrieval,” Journal of Documentation, vol. 28, no. 1, pp. 11–21, 1972, MCB UP Ltd. [21] M.-L. Zhang and Z.-H. Zhou, “A review on multi-label learning algo- rithms,” IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 8, pp. 1819–1837, 2013, IEEE. [22] Beltagy, Iz and Lo, Kyle and Cohan, Arman. “SciBERT: A pretrained language model for scientific text.” arXiv preprint arXiv:1903.10676, 2019. [23] Daszczyszak, Roman and Ellis, Dan and Luke, Steve and Whitley, Sean. TTP-Based Hunting. MITRE CORP MCLEAN VA, Tech. Rep, 2019. [24] B. Tang, J. Wang, Z. Yu, B. Chen, W. Ge, J. Yu, and T. Lu, ”Advanced Persistent Threat intelligent profiling technique: A survey,” Computers and Electrical Engineering, vol. 103, pp. 108261, 2022, Elsevier. [25] Rahman, Md Rayhanur and Williams, Laurie. ”From Threat Reports to Continuous Threat Intelligence: A Comparison of Attack Tech- nique Extraction Methods from Textual Artifacts.” arXiv preprint arXiv:2210.02601 (2022). [26] G. Salton and M.E. Lesk, “Computer evaluation of indexing and text processing,” Journal of the ACM (JACM), vol. 15, no. 1, pp. 8–36, 1968, ACM New York, NY, USA. [27] Md Rayhanur Rahman, Rezvan Mahdavi Hezaveh, and Laurie Williams. What are the attackers doing now? Automating cyberthreat intelligence extraction from text on pace with the changing threat landscape: A survey. ACM Computing Surveys, 55(12):1–36, 2023, ACM New York, NY. [28] Bikash Saha, Nanda Rani, and Sandeep Kumar Shukla. MalXCap: A Method for Malware Capability Extraction. In International Confer- ence on Information Security Practice and Experience, pages 230–249. Springer, 2023. [29] Grandini, Margherita and Bagli, Enrico and Visani, Giorgio. Metrics for multi-class classification: an overview. arXiv preprint arXiv:2008.05756, 2020. [30] Bader Al-Sada, Alireza Sadighian, and Gabriele Oligeri. “MITRE ATT&CK: State of the Art and Way Forward,” arXiv preprint arXiv:2308.14016, 2023. [31] Yinghai Zhou, Yi Tang, Ming Yi, Chuanyu Xi, and Hai Lu. CTI view: APT threat intelligence analysis system. Security and Communication Networks, 2022:1–15, 2022. Hindawi Limited. [32] Abdul Basit Ajmal, Munam Ali Shah, Carsten Maple, Muhammad Nabeel Asghar, and Saif Ul Islam. Offensive security: Towards proactive threat hunting via adversary emulation. IEEE Access, 9:126023–126033, 2021. [33] R. Patil, S. Boit, V. Gudivada, and J. Nandigam, ”A survey of text representation and embedding techniques in nlp,” IEEE Access, 2023, IEEE. [34] O.C. Briliyant, N.P. Tirsa, and M.A. Hasditama, “Towards an automated dissemination process of cyber threat intelligence data using STIX,” in 2021 6th International Workshop on Big Data and Information Security (IWBIS), pp. 109–114, 2021, IEEE. APPENDIX A. Sentence-TTP Dataset Distribution This section plots the Sentence-TTP dataset distributed over the 193-TTP classes. The sentence-TTP dataset contains sentences as data sample and their corresponding TTP ID. The number of samples in each TTP class is shown in Fig 9. In this figure, each tile represents a block for each TTP class, and contains TTP ID and their corresponding number of samples present in the dataset. T1059 800 T1027 732 T1070 561 T1016 490 T1547 471 T1071 459 T1082 455 T1057 441 T1083 440 T1036 435 T1140 391 T1518 384 T1021 383 T1053 376 T1105 369 T1055 348 T1049 342 T1555 339 T1012 333 T1005 331 T1011 325 T1564 316 T1132 316 T1204 315 T1018 312 T1041 311 T1106 311 T1566 310 T1056 306 T1007 300 T1562 297 T1560 296 T1588 290 T1014 289 T1573 289 T1046 283 T1010 283 T1003 282 T1218 282 T1020 279 T1113 279 T1033 273 T1119 271 T1068 269 T1048 268 T1497 266 T1025 266 T1550 262 T1008 260 T1102 258 T1037 256 T1574 254 T1546 253 T1047 247 T1530 243 T1114 241 T1553 239 T1052 236 T1594 235 T1622 231 T1486 230 T1505 229 T1040 227 T1120 227 T1216 226 T1090 224 T1567 223 T1615 222 T1039 220 T1571 219 T1092 219 T1195 219 T1598 218 T1190 218 T1590 215 T1496 215 T1185 214 T1203 211 T1495 210 T1548 208 T1133 208 T1112 207 T1189 206 T1580 205 T1599 204 T1556 204 T1091 204 T1593 203 T1006 202 T1095 201 T1611 201 T1568 201 T1620 197 T1001 196 T1561 193 T1123 193 T1552 193 T1498 192 T1104 190 T1212 189 T1569 187 T1482 187 T1210 185 T1596 185 T1098 184 T1199 184 T1558 183 T1134 181 T1072 181 T1570 178 T1587 178 T1601 177 T1087 176 T1554 176 T1221 171 T1211 171 T1535 170 T1619 167 T1525 166 T1202 160 T1572 159 T1529 159 T1485 156 T1595 156 T1491 154 T1583 153 T1612 153 T1600 152 T1222 152 T1586 151 T1201 151 T1649 149 T1187 149 T1129 147 T1565 144 T1557 144 T1609 143 T1578 142 T1219 140 T1610 140 T1217 140 T1608 137 T1592 137 T1200 136 T1543 136 T1069 133 T1589 133 T1213 132 T1030 128 T1542 124 T1559 123 T1110 123 T1115 121 T1563 119 T1585 115 T1539 114 T1648 111 T1526 109 T1614 108 T1078 107 T1537 103 T1591 101 T1207 100 T1602 97 T1124 96 T1531 95 T1029 92 T1137 89 T1584 87 T1074 84 T1205 83 T1125 76 T1480 75 T1136 72 T1597 69 T1606 63 T1176 58 T1613 57 T1220 53 T1647 50 T1135 48 T1197 46 T1534 40 T1528 37 T1621 37 T1490 29 T1489 28 T1538 15 T1484 7 T1499 4 T1111 4 Fig. 9. Sentence-TTP Dataset Data Distribution over 193-TTP Class