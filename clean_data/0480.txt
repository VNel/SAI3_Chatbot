An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents Gregory Palmer 1 Luke Swaby 1 Daniel J.B. Harrold 1 Matthew Stewart 1 Alex Hiles 1 Chris Willis 1 Ian Miles 2 Sara Farmer 3 Abstract The recent rise in increasingly sophisticated cyber- attacks raises the need for robust and resilient au- tonomous cyber-defence (ACD) agents. Given the variety of cyber-attack tactics, techniques and pro- cedures (TTPs) employed, learning approaches that can return generalisable policies are desir- able. Meanwhile, the assurance of ACD agents remains an open challenge. We address both chal- lenges via an empirical game-theoretic analysis of deep reinforcement learning (DRL) approaches for ACD using the principled double oracle (DO) algorithm. This algorithm relies on adversaries iteratively learning (approximate) best responses against each others’ policies; a computationally expensive endeavour for autonomous cyber op- erations agents. In this work we introduce and evaluate a theoretically-sound, potential-based re- ward shaping approach to expedite this process. In addition, given the increasing number of open- source ACD-DRL approaches, we extend the DO formulation to allow for multiple response ora- cles (MRO), providing a framework for a holistic evaluation of ACD approaches. 1. Introduction Deep reinforcement learning (DRL) has emerged as a promising approach for training autonomous cyber-defence (ACD) agents that are capable of continuously investigating and neutralising cyber-threats at machine speed. However, to learn robust and resilient ACD policies, DRL agents must overcome three open problem areas: i.) Vast, dynamic, high-dimensional state-spaces; ii.) Large, combinatorial ac- tion spaces, and; iii.) Adversarial learning against a non- stationary opponent (Palmer et al., 2023). While the autonomous cyber-operations (ACO) commu- 1BAE Systems Applied Intelligence Labs, United Kingdom (UK) 2Frazer-Nash Consultancy Limited, UK 3Defence Science and Technology Laboratory (Dstl), UK. Correspondence to: Gre- gory Palmer <gregory.palmer@baesystems.com>. nity has taken steps towards solving the first two chal- lenges (Symes Thompson et al., 2024; Tran et al., 2022), the third challenge has received comparatively little attention. ACD agents are often benchmarked against a stationary set of autonomous cyber-attacking (ACA) agents. For example, The Technical Cooperation Program (TTCP)’s annual Cy- bORG4 CAGE Challenges (CCs) rank submissions based on their performance against the same rules-based ACA agents that are used during training (TTCP CAGE Working Group, 2022). This formulation is concerning from a gener- alisation standpoint; it encourages solutions that over-fit to known adversaries. In real-world scenarios, by contrast, the stationarity of an opponent is a strong assumption. Further- more, ACD agents will often be unable to perform online policy updates once deployed, effectively limiting them to a fixed-policy regime. Therefore, a framework is required for learning and assuring generalisable ACD policies prior to deployment. In this work we advocate using variations of the principled double oracle (DO) algorithm (McMahan et al., 2003) to obtain ACD agents that can generalise across ACA agents and provide assurance regarding their performance against a resource bounded worst case opponent.5 Our contributions can be summarised as follows: i.) The DO algorithm offers flexibility with respect to select- ing algorithms for learning responses.6 The availability of cyber-defence gyms has resulted in numerous open source (OS) ACD-DRL approaches, which have frequently been infused with domain knowledge to simplify learning tasks, e.g., wrappers for pre-processing observations and buffers for storing critical information (Vyas et al., 2023). This raises questions such as: “Which approach should ACD practitioners select?” and “Are there benefits in using more than one approach?” To answer these questions, we extend the DO algorithm to allow for the computation of responses using multiple approaches, giving rise to the multiple re- sponse oracles algorithm (MRO). We also add a theoretical evaluation of MRO, showing that the convergence guaran- tees from the underlying DO algorithm remain intact. 4Cyber Operations Research Gym (Standen et al., 2021). 5Bounded by computational resources (Oliehoek et al., 2018). 6Referred to as policy space response oracles (PSRO) when using DRL oracles (Lanctot et al., 2017). 1 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. arXiv:2501.19206v1 [cs.AI] 31 Jan 2025 An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents ii.) While the DO algorithm offers the benefit of theoret- ical convergence guarantees, it suffers from lengthy wall- times due to the requirement that both adversaries iteratively compute (approximate) best responses (ABRs) against each others’ latest policies (Li et al., 2023). This challenge is amplified for autonomous cyber-operations (ACO), which confront learning agents with high-dimensional observation and action spaces. Therefore, we are interested in exten- sions that facilitate the learning of ABRs and reduce the number of response iterations for achieving convergence. Here, we propose leveraging the lessons learned by previous responses by using their value functions for potential-based reward shaping (VF-PBRS) (Ng et al., 1999). In addition, instead of computing responses from scratch, we initialise policies from pre-trained models (PTMs) (Li et al., 2023). iii.) Using the MRO algorithm, we provide empirical game-theoretic analyses of state-of-the-art ACD-DRL ap- proaches for two cyber-defence gym environments: CCs 2 and 4 (TTCP CAGE Working Group, 2022; 2023). We find that the ACD agents obtained via adversarial learning are robust towards learning attackers, which struggle to find new successful tactics, techniques and procedures (TTPs). In addition, we provide empirical evidence that VF-PBRS ora- cles can converge upon significantly stronger ACD policies compared to vanilla approaches. Finally, while pre-trained models allow for shorter response iterations, in CC2 we find that additional “full” approximate best response iterations are required to achieve convergence, with cyber-attacking responses using random neural network initialisations. 2. Related Work Despite ACD being, by definition, an adversarial game be- tween defending and attacking agents, there exists limited work on the topic of adversarial learning. Evaluations typ- ically feature a stationary set of opponents (Hicks et al., 2023; Nguyen et al., 2020; O’Driscoll et al., 2024; Tran et al., 2022). A possible explanation for this is that current cyber-defence gyms do not support adversarial learning. For example, while the CybORG CCs support both the training of ACD and ACA agents, modifications are necessary to enable learning against a DRL opponent. An exception here is work conducted by Shashkov et al. (2023), who adapt and compare DRL, evolutionary strategies, and Monte Carlo tree search methods within CyberBattleSim (Seifert et al., 2021). To the best of our knowledge, we are the first to conduct a holistic empirical game-theoretic analysis of ACD-DRL approaches within topical cyber-defence gyms. The topic of reward shaping has been gathering at- tention from the ACD community (Lopes Antunes & Llopis Sanchez, 2023; Miles et al., 2024). For instance, to study the sensitivity of DRL to the magnitude of received penalties (Bates et al., 2023). In contrast to existing work, we seek a principled reward shaping function that will pre- serve the DO algorithm’s convergence guarantees. Here, we identify the principled PBRS as a suitable approach, and propose a novel formulation that utilises VFs from pre- vious ABR iterations as a potential function. Finally, Li et al. (2023) highlight the benefits of using DO training with PTMs for solving large-scale pursuit-evasion games. We believe we are the first to make use of this approach within the context of autonomous cyber-defence. 3. Background 3.1. Partially Observable Markov Games Our ACD and ACA agents are situated within partially ob- servable Markov games (POMGs). A POMG M is defined as a tuple (n, S, Ω, O, A, P, R, γ), consisting of: a finite state space S; a joint action space (A1 × ... × An) for each state s ∈S, with Ap being the set of actions available to player p; a state transition function P : St × A1 × ... × An × St+1 →[0, 1], returning the probability of transition- ing from a state st to st+1 given a joint-action profile a; a set of joint observations Ω; an observation probability function defined as Op : S × A1 × ... × An × Ω→[0, 1]; a discount rate γ, and; for each player p, a reward function Rp : St×A1×...×An×St+1 →R, returning a reward rp. 3.2. The Adversarial Learning Challenge Our focus is on adversarial learning scenarios that feature cyber-defence (Blue) and cyber-attacking (Red) agents. For- mally, for each agent (player) p, the policy πp is a mapping from the state space to a probability distribution over actions, πp : Sp →∆(Ap). Transitions within POMGs are deter- mined by a joint policy π. Joint policies excluding agent p are defined as π−p. The notation ⟨πp, π−p⟩refers to a joint policy with agent p following πp while the other agents follow π−p. As noted above, the stationarity of an attacking agent’s policy is a strong assumption. Therefore, select- ing defending agents based on their performance against a pool of known attackers runs the risk of being blindsided by an unfamiliar attacker. Here, a more desirable solution concept commonly used in this class of games is the Nash equilibrium (Nash, 1951): Definition 3.1 (Nash Equilibrium). A joint policy π∗is a Nash equilibrium iff no player p can improve their gain through unilaterally deviating from π∗: ∀p, ∀πp, Gp(⟨π∗ p, π∗ −p⟩) ≥Gp(⟨πp, π∗ −p⟩). (1) Our focus is on finite two-player zero-sum games, where an equilibrium is referred to as a saddle point, representing the value of the game v∗. Given two policies π1, π2, the equilibrium of a finite zero-sum game is: Theorem 1 (Minmax Theorem). In a finite two-player 2 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents zero-sum game v∗ = maxπ1minπ2Gi(⟨π1, π2⟩) = minπ2maxπ1Gi(⟨π1, π2⟩). The above theorem states that every finite, zero-sum, two- player game has optimal mixed strategies (Von Neumann, 1928). However, in complex games, including ACD scenar- ios, finding a Nash equilibrium is typically intractable due to the complexity of the strategy space for computing actual best responses. Here the concept of an approximate Nash equilibrium (ϵ-NE) is helpful (Oliehoek et al., 2018): Definition 3.2 (ϵ-Nash Equilibrium). π∗is an ϵ-NE iff: ∀i, ∀πi, Gi(⟨π∗ i , π∗ −i⟩) ≥Gi(⟨πi, π∗ −i⟩) −ϵ. (2) Assuming an ϵ-Nash Equilibrium has not yet been found, our interest turns towards measuring the difference in the gain that an agent j can achieve through unilaterally de- viating from the current joint-policy profile. This metric is known as exploitability, and can be computed using the following equation (Lanctot et al., 2017): GE,i = Gj(⟨πi, π′ j⟩) −Gj(⟨πi, πj⟩). (3) In the above equation GE,i is the exploitability for agent i, and π′ j is an approximate best response policy against πi. If GE,i ≤0, then π′ j is unable to improve on πj. As a result πi cannot be exploited. If the combined exploitability in a two-player zero-sum game is less than or equal to ϵ, GE = GE,i + GE,j ≤ϵ, then players i and j are in an ϵ-NE. 3.3. Approximate Double Oracles One of the long-term objectives of adversarial learning is to limit the exploitability of agents deployed in competitive environments (Lanctot et al., 2017). While a number of the- oretically grounded methods exist for limiting exploitability, computing the value of a game remains challenging in prac- tise, even for relatively simple games. Here, we provide a recap of a popular, principled adversar- ial learning frameworks for finding a minimax equilibrium and reducing exploitability: the DO algorithm (McMahan et al., 2003). This algorithm defines a two-player zero-sum normal-form game N, where actions correspond to policies available to the players within an underlying (PO)MG M. Payoff entries within N are determined through computing the gain G for each policy pair within M: RN i (⟨ar 1, ac 2⟩) = GM i (⟨πr 1, πc 2⟩). (4) In Equation 4, r and c refer to the respective rows and columns inside the normal-form (bimatrix) game. The normal-form game N is subjected to a game-theoretic anal- ysis, to find an optimal mixture over actions for each player, representing a probability distribution over policies for the game M. The DO algorithm assumes that both players have access to a best response oracle that returns a best response policy πi for agent i against a mixture of policies played by the oppo- nent: πi ←Oi(µj).7 Here, µj defines a sampling weighting over policies Πj available to agent j. Best responses are subsequently added to the list of available policies for each agent. As a result each player has an additional action that it can choose in the normal-form game N. Therefore, N needs to be augmented through computing payoffs for the new row and column entries. Upon augmenting N an- other game-theoretic analysis is conducted, and the steps described above are repeated. If no further best responses can be found, then the DO algorithm has converged upon a minimax equilibrium (McMahan et al., 2003). Learning an exact best response is often intractable. In games that suffer from the curse-of-dimensionality, O(µ) will often only return an approximate best response, giving rise to the approximate DO algorithm (ADO, Algorithm 1). This approach is guaranteed to converge upon an ϵ-resource bounded NE (ϵ-RBNE) (Oliehoek et al., 2018). Algorithm 1 The Approximate Double Oracle Algorithm 1: ⟨πBlue, πRed⟩←INITIALPOLICIES() 2: ⟨µBlue, µRed⟩←⟨{πBlue}, {πRed}⟩▷Set initial mixtures 3: while True do 4: πBlue ←OBlue(µRed) 5: πRed ←ORed(µBlue) 6: GE ←GBlue(πBlue, µRed) + GRed(µBlue, πRed) 7: if GE ≤ϵ then 8: break ▷Found ϵ-RBNE 9: end if 10: N ←AUGMENTGAME(N, πBlue, πRed) 11: ⟨µBlue, µRed⟩←SOLVEGAME(N) 12: end while 3.4. Potential-Based Reward Shaping The iterative learning of ABRs presents a computationally costly endeavour, highlighting the need for methods that expedite the search for ABRs (Liu et al., 2022). We observe that learning ACO agents are confronted with the tempo- ral credit assignment problem. For example, in numerous cyber-defence scenarios, large penalties are associated with ACA agents impacting high-value targets, such as opera- tional hosts and servers (Standen et al., 2021). However, attacks on an operational subnet will typically be preceded by Blue/Red actions to protect/compromise more accessible user and enterprise subnets. Here, reward shaping offers a means of mitigating the temporal credit assignment prob- lem via a modified reward function R′ = R + F, where F represents the shaping reward (Grzes & Kudenko, 2009b). A shaping reward can be implemented using domain knowl- edge or learning approaches (Grzes & Kudenko, 2009a). However, unprincipled reward shaping can lead to policies 7We discuss the benefits of mixed strategies in Appendix J. 3 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents that behave (near)-optimally under R′ while performing sub-optimally with respect to R (Ng et al., 1999). Classic examples include agents learning to circle around a goal location to maximise a distance-based shaping reward (Ran- dløv & Alstrøm, 1998) and soccer agents losing interest in scoring goals upon receiving a shaping reward for success- fully completing passes (Ng et al., 1999). To address the above challenge, Ng et al. (1999) proved that F being a potential-based shaping function, F(s, s′) = τ(γΦ(s′) −Φ(s)), (5) is a necessary and sufficient condition to guarantee consis- tency when learning an optimal policy on an MDP M′ = (S, A, P, R+F, γ) instead of M = (S, A, P, R, γ). Here, Φ : S →R is a real-valued potential function, defined over source and destination states, and τ > 0 is used to scale the shaping reward (Grzes & Kudenko, 2009b). Crucially, PBRS’s convergence guarantees hold even when using a suboptimal potential function (Gao & Toni, 2015). 4. Methods 4.1. Value-Function Potential-Based Reward Shaping In their seminal work on policy invariance under PBRS transformations, Ng et al. (1999) state that one way for defining a good potential function Φ is to approximate the optimal value function (VF) V ∗ M(s) for a given problem M. We observe that any ADO run featuring oracles using VFs can provide a plethora of approximate VFs, VMk(s), en- abling a VF driven PBRS (VF-PBRS); with Mk represent- ing the (PO)MG that an ABR k was trained on. The number of VFs available after multiple ABR iterations raises the question of which VMk to select. Here, the ADO algorithm’s current mixture µi captures the best response for player i using available policies within our empirical matrix game against player j’s mixture µj. We hypothesise that µi can also help us select informative VFs. Rather than sampling a single VMk we propose using a weighted ensemble. However, normalization is required when ensembling VFs, as the magnitudes of approximated value estimates are often not directly comparable (Garcia & Caarls, 2024). We address this by applying Z-score nor- malization to each VF: Z(VMk(s)). The resulting potential function is the weighted sum of normalised value estimates: Φ(s) = |µ| X k=1 µk i × Z(VMk(s)). (6) Theoretical Analysis: Oracles using VF-PBRS do not im- pact the theoretical guarantees underpinning the DO algo- rithm. Given a two-player zero-sum game, a Nash equilib- rium has been found when neither player can compute a best response that outperforms their respective current mixture: GE = Gi(⟨Oi(µj), µj⟩) + Gj(⟨µi, Oj(µi)⟩) ≤0 = Gi(⟨µi, µj⟩) + Gj(⟨µi, µj⟩). (7) It follows that GE ≤0 relies on Oi(µj) and Oj(µi) re- turning best responses. Below we show that oracles using VF-PBRS meet this requirement in Markov games. Theorem 2. Given a best response oracle Oi that uses a shaping reward R′ = R + F, then F being a potential- based reward shaping function is a sufficient condition to guarantee that πi ←Oi(µj) is a best response for M′ = (n, S, A, P, R + F, γ) and M = (n, S, A, P, R, γ). Proof. The DO algorithm requires that the opponent poli- cies sampled from the mixture µj remain stationary while player i computes a best response. Given that player j is treated as part of the environment under this formu- lation, both M and M′ are equivalent to Markov deci- sion processes (MDPs) M = (S, A, P, R, γ) and M′ = (S, A, P, R+F, γ). Ng et al. (1999)’s proof of sufficiency8 shows that if F is a potential-based shaping function, then: π∗ M′(s) = argmax a∈A Q∗ M′(s, a) = argmax a∈A Q∗ M(s, a), (8) with Q∗representing the optimal Q-value for state s and action a. Therefore, π∗ M′ represents a best response against the opponent mixture µj in both M′ and M. The complexity of our target domain requires oracles that rely on function approximators. Therefore, the best we can hope for are ABRs. Here, Ng et al. (1999) observe that PBRS is robust in the sense that near-optimal policies are also preserved. Given a PBRS function F, any near-optimal policy learnt in M′ will also be a near-optimal policy in M, meaning that an ABR policy π obtained using VF-PBRS, with |V π M′(s) −V π∗ M′(s)| < ε, will also be near optimal in M: |V π M(s) −V π∗ M (s)| < ε. Therefore, ADO’s theoretical guarantees for converging upon an ϵ-RBNE remain intact. 4.2. Multiple Response Oracles One of the disadvantages of the ADO formulation is that the function Oi(µj) only returns a single policy πi per ABR it- eration. What if we have multiple approaches for computing responses against an opponent mixture, and do not know in advance which one is most likely to return the ABR? Here, we propose a novel ADO formulation to address this limi- tation. We call this approach the multiple response oracles algorithm (MRO). In contrast to the ADO algorithm, MRO replaces ABR oracles Oi with functions Πi ←Ri(µj), that return a set of responses. We note that this extension does not change the underlying theoretical properties of the 8Included in Appendix A.3 for convenience. 4 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents ADO algorithm. In the set of response policies computed by agent i against the opponent mixture µj, using a responses function Ri(µj), there will exist an ABR policy π∗ i , where: Gi(π∗ i , µj) ≥Gi(πi, µj), ∀πi ∈Ri(µj). (9) Therefore, assuming an oracle Oi, that can identify the ABR policy as per Equation 9, we have: π∗ i ←Oi(Ri(µj)) (See Appendix A.2 for a detailed theoretical analysis). Algorithm 2 Multiple Response Oracles 1: ⟨πBlue, πRed⟩←INITIALPOLICIES() 2: ⟨µBlue, µRed⟩←⟨{πBlue}, {πRed}⟩▷set initial mixtures 3: while True do 4: ⟨π0 Blue, ..., πn Blue⟩←RBlue(µRed) 5: ⟨π0 Red, ..., πm Red⟩←RRed(µBlue) 6: GE ←GBlue(π∗ Blue, µRed) + GRed(µBlue, π∗ Red) 7: if GE ≤ϵ then 8: break ▷found ϵ-RBNE 9: end if 10: N ←AUGMENTGAME(N, π0,...,n Blue , π0,...,m Red ) 11: ⟨µBlue, µRed⟩←SOLVEGAME(N) 12: end while 4.3. Pre-trained Model Sampling Training ACO agents requires lengthy wall-times to en- sure an exhaustive exploration of semantically similar state- action pairs. To address this challenge, a general consensus has emerged within the AI community that PTMs should be utilised when possible, rather than training models from scratch (Han et al., 2021). Within the context of DO-based approaches, agents have the luxury of an iteratively ex- panding pool from which PTMs can be sampled. Here, once again, the mixtures µi can provide guidance; on this occasion for sampling from PTMs that represent the best available response against the current opponent mixture µj. However, one of the strengths of the DO algorithm is the exploration of the strategy space (Wellman & Mayo, 2024). We propose balancing the above trade-off by adding ϵ- greedy exploration to our oracles. In each ABR iteration a mixture-guided PTM is sampled with a probability 1 −ϵ. Initially ϵ is set to 1. A decay rate d ∈[0, 1) is subsequently applied after each ABR iteration. Exploratory iterations can either consist of training a freshly initialised agent, or using a designated PTM as a starting point, e.g., a policy that can be classed as a generalist. In contrast, in greedy iterations the mixture weights are used as sampling probabilities. 5. Evaluation Environments Below we conduct an empirical game-theoretic evaluation of DRL approaches in two post-exploitation lateral movement scenarios: CybORG CAGE Challenges 2 and 4 (TTCP CAGE Working Group, 2022; 2023). Network diagrams and experiment settings can be found in Appendix B and C. CAGE Challenge 2 (CC2): A cyber-defence agent (Blue) is tasked with defending a computer network containing key manufacturing and logistics servers distributed across three subnets: user, enterprise and operations. Each episode begins with an attacking agent (Red) having root ac- cess on a user host on the user subnet. Red’s objectives are to reach an operational server and degrade network services. Blue’s objective is to minimise the presence of the Red agent and ensure that network functionality is maintained through running detailed analyses on hosts, launching decoy ser- vices, and removing malicious software (Kiely et al., 2023). For cases where the Red agent is too well established, Blue can also resort to restoring hosts using a clean backup. CAGE Challenge 4 (CC4): Blue is tasked with defend- ing sub-networks responsible for supporting military oper- ations. Red’s goal is to disrupt a base-station from which Blue unmanned aerial vehicle activities are coordinated. The network in CC4 is highly segmented to increase the level of security. Therefore, a multi-agent (MA) cyber- defence solution is required. Here each agent is assigned a zone that it must protect within two high value Deployed Networks (A and B). Each network has a Restricted and Operational zone. Successful Red attacks on these zones result in large penalties for Blue. 6. Cyber-Defence & Attacking Oracles Cardiff University (Blue): Our first Blue oracle is a modi- fied version of the CC2 winning submission from Cardiff University (Hannay, 2022). This approach, based on the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017), benefits from a significantly reduced action space. It defines a single decoy action per host that greedily selects from available decoys. The submission also uses a “fingerprinting” function for identifying the two rules-based Red agents supplied by CC2 and countering with respective ABR policies. However, fingerprinting new Red policies is beyond our current scope. We therefore disable this method. Cybermonic Inductive Graph-PPO (GPPO, Blue): Given that cyber-defence scenarios typically feature underlying dynamics unique to graph-based domains, ACD agents are increasingly benefitting from incorporating graph machine learning approaches (Symes Thompson et al., 2024). King & Bowman (2024) and King (2024) have implemented two OS Graph-PPO algorithms for CC2 and CC4. The ap- proaches utilise graph observation wrappers for converting CybORG’s observations into an augmented graph environ- ment. An internal state is maintained that keeps track of changes in the graph structure. Graph convolutional net- works (Kipf & Welling, 2016) are used to extract features from the graph representation. An independent learner (IL) approach is used in CC4, with each agent updating indepen- dent actor-critic networks using local observations. 5 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents Action Masking PPO (AM-PPO, Red): The cyber- attacking agents within CC2&4 must also cope with a high-dimensional action space. Therefore, to confront the Blue agents with challenging opponents, our cyber-attacking agent has the benefit of being able to use action masking during training. This was enabled by adding a Red action- masking wrapper to CC2, and implementing (MA)DRL action masking agents. For CC4 our AM-MAPPO agents also use an IL approach. We set the Red rewards to the negation of the Blue rewards in both environments. 7. Empirical Game-Theoretic Analysis 7.1. CybORG CAGE Challenge 2 During lengthy preliminary runs using 5M time-steps per response, it was observed that the agents would often make significant improvements after 2.5M time-steps (See Ap- pendix G). However, this corresponds to approximately one day per ABR iteration. To reduce wall-time, we instead ini- tialise from “generalist” PTMs obtained from previous 5M step runs. We allow 1.5M environment steps per response. Responses are evaluated using the mean episodic reward from 100 evaluation episodes. Impact of using PTMs and VF-PBRS: Upon completing around 20 ABR iterations, we find that Blue and Red oracles using PTMs struggle to improve on the Nash payoffs under the joint mixture profile ⟨µBlue, µRed⟩(see Figure 1). At this point we confront the Blue mixture with a Red oracle that uses a random initialisation and a budget of 5M time- steps. This Red oracle initially finds an ABR, π21 Red, that significantly improves upon the Nash payoff, from 27.30 under ⟨µBlue, µRed⟩to 93.05 for ⟨µBlue, π21 Red⟩. However, upon adding π21 Red to the empirical game, the Blue agent neutralises the new Red TTPs through adjusting its mixture, decreasing the Nash payoff to 29.75. The Red agent is subsequently unable to find a significantly better response. 0 3 6 9 12 15 18 21 24 Approximate Best Response (ABR) Iteration 200 175 150 125 100 75 50 25 Mean Episodic Rewards (Blue) Cyber-Defence ABRs Cyber-Attack ABRs Nash Payoff: Blue( Blue, Red) VF-PBRS Graph-PPO (PTM=True) VF-PBRS Cardiff (PTM=True) VF-PBRS AM-PPO (PTM=True) VF-PBRS AM-PPO (PTM=False) Figure 1. A depiction of Blue and Red ABRs from an MRO run on CC2. Rewards are plotted from Blue’s perspective. Both agents are unable to find ABRs that significantly improve on the Nash payoff (the black line) between iterations 11 – 20. Red oracles use PTMs for the first 20 iterations, but subsequently switch to random initialisations (PTM=False). Initially this oracle setting finds a policy that significantly improves on the Nash payoff. However, the Blue agent immediately counters through adjusting its mixture. We continue to probe the final mixtures, seeking oracle settings that outperform the responses from the final ABR iteration. However, the resulting responses do not signif- icantly deviate from the final value of the game estimate, GBlue(µBlue, µRed) = −29.63. Nevertheless, our evalua- tion yields interesting insights into the benefits of VF-PBRS. For instance, when comparing “full” (PTM=False, 5M time- step) training runs, VF-PBRS responses outperform the corresponding vanilla responses (see Table 1). Gathering a sufficient number of 5M step runs to test the statistical significance of the above result is a computation- ally expensive endeavour. Instead we initialise from PTMs and gather ten 1.5M step runs per setting. We compare two VF-PBRS configurations (τ = {1, 0.5}) against vanilla re- sponses. For Red we find that on average VF-PBRS (τ = 1: 24.72, and; τ = 0.5: 24.85) marginally outperform the vanilla responses (24.22). However, the differences are not significant. This result also shows that the generalist policy πG Red is a poor PTM choice against advanced Blue mixtures, when compared against the results from the full runs. In contrast, upon conducting the same experiment with Blue GPPO responses, both VF-PBRS using VF ensem- bling (VFE) and individual VFs significantly outperform the vanilla responses (see Figure 2). Using VFE outper- forms the majority of runs conducted with individual VFs (all using τ = 1). Finally, no significant difference is found when using a VFE setting of τ = 0.5, compared to the VFE runs with τ = 1. Agent PTM Description Payoffs Blue True VFk (µk i ) VF1 (0.1) −33.8 (±3.7) VF2 (0.2) −32.1 (±1.0) VF3 (0.3) −34.0 (±3.9) VF4 (0.1) −31.0 (±1.7) VF5 (0.3) −29.4 (±0.5) VFE (τ) τ = 1 −29.8 (±1.2) τ = 0.5 −31.3 (±1.2) Vanilla τ = 0 −38.3 (±2.7) False VFE (τ = 1) GPPO −27.8 (±1.5) CAR −30.3 (±1.5) Vanilla GPPO −29.5 (±1.7) CAR −31.2 (±2.3) Red True VFE (τ) τ = 1 24.7 (±0.4) τ = 0.5 24.9 (±0.5) Vanilla τ = 0 24.2 (±0.5) False VFE τ = 1 29.5 (±0.8) Vanilla τ = 0 28.6 (±0.4) Table 1. A comparison of additional responses learnt against the penultimate CC2 mixtures, following iteration 22. We compare the impact of using PTMs and VF-PBRS vs Vanilla training, i.e., without reward shaping. For VF-PBRS, we make the distinction between using VF ensembling (VFE) and using individual VFs k with a mixture weighting µk i > 0. Responses learnt without PTMs represent a single run per setting, while ten runs were obtained for configurations using PTMs. Listed are the mean and standard error upon conducting 100 evaluation episodes per response. 6 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents VF1 ( = 0.1) VF2 ( = 0.2) VF3 ( = 0.3) VF4 ( = 0.1) VF5 ( = 0.3) Vanilla VFE ( = 0.5) VFE ( = 1) Response Oracle Settings 55 50 45 40 35 30 25 20 15 10 Blue Reward ** * ** * * Figure 2. The box plot above compares VF-PBRS runs using VF ensembling (VFE) and individual VFs k (where µk i > 0) against vanilla training runs (10 runs per setting). Statistical p-values of less than 0.05 and 0.01 are flagged with one and two asterisks respectively. Outliers are plotted as separate black circles. Final Mixture Composition: Figure 3 illustrates the final empirical game for CC2. We observe that the final mixture composition for Blue consists of GPPO policies, which of- ten generalise well across learnt Red policies. This naturally raises the question of how these policies differ from those not included. Here, we compare the original Blue GPPO agent πo Blue, provided by King & Bowman (2024), against the final Blue mixture agent µBlue. G 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 G O 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 G O 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Cybermonic Graph-PPO CardiffUniversity Action-Masking PPO (Final three ABRs obtained without using PTMs) 0.215 0.006 0.006 0.295 0.427 0.052 Red Mixture Red 0.166 0.192 0.278 0.107 0.241 0.015 Blue Mixture Blue 0 25 50 75 100 125 150 175 200 225 Rewards from the cyber-attacking agent (Red)'s perspective Figure 3. An illustration of CC2’s empirical game and Nash mix- tures. Cells represent the mean episodic reward for each policy pairing plotted from Red’s perspective (100 evaluation episodes). X and Y ticks indicate the ABR iteration in which a response was learnt. We also include original (O) and generalist (G) policies. Darker cells represent match-ups that are favorable for Red. We evaluate πo Blue against Red’s best response, π2 Red: GBlue(πo Blue, π2 Red) = −117.82. This attacking policy thrives against πo Blue through gaining privileged access on User2 and revealing Enterprise1’s IP address 9. It subsequently launches BlueKeep and EternalBlue at- tacks against Enterprise1, gaining privileged access on 59.68% of time-steps10. This forces πo Blue into repeat- edly selecting the costly restore action. In contrast, under ⟨µBlue, π2 Red⟩(GBlue = −21.96) the attacker is mostly re- stricted to the user network (See Figure 4).11 None User Privileged User1 User2 User3 User4 Enterprise0 Enterprise1 0.0 0.2 0.4 0.6 0.8 1.0 Percentage (a) ⟨πo Blue, π2 Red⟩ User1 User2 User3 User4 Enterprise0 Enterprise1 0.0 0.2 0.4 0.6 0.8 1.0 (b) ⟨µBlue, π2 Red⟩ Figure 4. Depicted are the percentage of steps per episode where Red can obtain User and Privileged access on the listed nodes. We compare privileges obtained by Red against the original GPPO parameterisation, πo Blue, and the final mixture agent µBlue. The attacking policy is Red’s ABR against πo Blue: π2 Red. 7.2. CybORG CAGE Challenge 4 For CC4 we launch an MRO run consisting of 2.5M step ABR iterations using PTMs. Blue and Red each use two oracles: i.) VF-PBRS, and; ii.) Vanilla. The MA-GPPO parameterisation πo Blue from King (2024) represents the initial policy for Blue. For Red we compute an initial ABR against πo Blue for 5M time-steps, which serves as πo Red. As with our CC2 experiment, we switch to “full” ABRs once the MRO run with PTMs converges. The mean Blue payoff12 under the initial policies is GBlue(πo Blue, πo Red) = −448.24 (100 evaluation episodes). The Red Vanilla and VF-PBRS responses degrade the mean payoff to −492.33 and −484.31 respectively. Similarly, the GPPO oracles find better responses against πo Red, −378.27 for Vanilla and −368.76 for VF-PBRS. The value of the game following the first ABRs is GBlue(µBlue, µRed) = −408.67. Following five ABR iterations this value does not change significantly, sitting at −412.7. With both Blue and Red unable to find ABRs that signif- icantly improve on the mixtures we switch to “full” 5M 9CC2’s network diagram is provided in Appendix B. 10These exploits give direct SYSTEM access to windows nodes. See the action tutorial in TTCP CAGE Working Group (2022). 11Additional policy characteristics are discussed in Appendix H. 12In CC4 each Blue agent receives an identical (team) reward. 7 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents Restricted Zone A Operational Zone A AllowTrafficZone Analyse BlockTrafficZone DeployDecoy Monitor Remove Restore Sleep 17.5% 35.0% 52.5% 70.0% (a) πo Blue AllowTrafficZone Analyse BlockTrafficZone DeployDecoy Monitor Remove Restore Sleep 15.0% 30.0% 45.0% 60.0% (b) π1 Blue AllowTrafficZone Analyse BlockTrafficZone DeployDecoy Monitor Remove Restore Sleep 22.5% 45.0% 67.5% 90.0% (c) πo Blue AllowTrafficZone Analyse BlockTrafficZone DeployDecoy Monitor Remove Restore Sleep 22.5% 45.0% 67.5% 90.0% (d) π1 Blue Figure 5. Action percentage comparison for CC4 agents tasked with defending restricted and operational zones A under the joint-policy profiles ⟨πo Blue, πo Red⟩and ⟨π1 Blue, πo Red⟩. Here, πo Blue are the original parameterisations from King (2024); πo Red Red’s ABR against πo Blue, and; π1 Blue, Blue’s ABR against πo Red. The latter agents are more active in blocking traffic, and thereby mitigating Red’s attacks. step responses after five ABR iterations. However, both VF-PBRS and Vanilla Red runs converge after 2M time- steps, and are unable to match the performance of the Red mixture agent13. The final estimated value of the game is GBlue(µBlue, µRed) = −411.91. We therefore observe a trivial change in the mixture payoffs following the first iteration. The reason for this becomes apparent upon inspecting the agents’ action profiles. Under the joint-policy profiles ⟨πo Blue, πo Red⟩, agents defending the Restricted and Operational zones of the net- work rarely select the BlockTrafficZone action (See Figure 5). Large penalties are associated with successful attacks on these areas of the network. In contrast, π1 Blue agents are more active in blocking traffic to these zones. The πo Blue mixture weight is zero after the first ABR iteration. 8. Empirical Game Augmentation Complexity While MRO provides a means through which to conduct a principled evaluation of cyber-defence and attacking approaches, it does come with an increase in complex- ity for augmenting the empirical payoff matrix. Given n = |RBlue(µRed)| approaches for computing responses for Blue, and m = |RRed(µBlue)| approaches for Red, in each iteration the MRO algorithm will need to compute n × |ΠRed| + m × |ΠBlue| −n × m new entries. Here, ΠRed and ΠBlue represent the sets of polices available at the start of an ABR iteration. A popular approach for dealing with large payoff matrices is to remove strategies that are found to be either strictly or weakly dominated (Conitzer & Sandholm, 2005; Kuzmics, 2011)14. Upon evaluating the payoff matrix from CC2 after 23 ABR iterations we are unable to identify any strictly dominated Blue policies. However, we do identify weakly dominated policies, including the majority of Cardiff ABRs 13See Appendix I for CC4’s convergence plot and payoff matrix. 14Definitions are provided in Appendix D. (excluding ABR 17) and GPPO ABRs 2, 7 – 10, 12 and 18. Therefore, an aggressive pruning would substantially re- duce the number of evaluations required for augmenting the empirical payoff matrix in future iterations. Double oracle based approaches are navigating a space of hidden game views that is gradually expanded as new re- sponses are added to the empirical game (Wellman & Mayo, 2024). As a result policies that are currently strictly/weakly dominated may become relevant in future iterations. There- fore, a periodic re-evaluation of pruned policies against recent ABRs is likely necessary to assess their status. 9. Discussion & Conclusion Our empirical game-theoretic evaluations show the risk posed by deploying autonomous cyber-defence agents that have only been confronted with a limited set of cyber-attack TTPs during training. Once deployed, cyber-defence agents will typically be unable to make online policy adjustments for damage limitation. Therefore, Blue’s adversarial learn- ing process must feature Red agents that pose a similar or worse threat to those that will be encountered once deployed. Put plainly, even the most principled and wall-time efficient adversarial learning framework will struggle to produce ro- bust and resilient cyber-defence agents if the approach is trained against weak Red solutions. Our work shows that oracles using a combination of VF- PBRS and PTMs enable the computation of time-efficient, robust and resilient responses. Our multiple response ora- cles algorithm provides a principled solution, not only for evaluating the approaches against each other, but combin- ing the best policies obtained from the learning approaches into a, potentially heterogeneous, mixture agent. We have shown the benefits of these approaches within the challeng- ing context of ACD, and note their potential for any other application area where multiple private and public organisa- tions are seeking to ensemble their custom built approaches. 8 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents Impact Statement Through highlighting the perils of not viewing autonomous cyber-operations through the lens of adversarial learning, our work is intended to serve as a wake-up call for the ACD community. We show that a number of design principles need to be adhered-to for obtaining an idealised, robust, and resilient ACD agent. These principles include: i.) Not having to re-learn defence strategies that were previously successful against opponents; ii.) Using a principled method for determining how to re-use past knowledge against cur- rent threats; iii.) The ability to ensemble policies drawn from a heterogeneous set of policies; iv.) The ability to rapidly (re)train policies, and; v.) That learning truly robust and resilient ACD policies will only be possible if the learn- ers are confronted with worst-case cyber-attacking policies. The fifth principle naturally raises a number of ethical con- siderations. Researchers and developers have justifiable concerns that fully enabling adversarial learning could lead to the nefarious use of cyber-attacking agents trained within a high-fidelity gym environment (Seifert et al., 2021). How- ever, our work highlights the weaknesses of agents trained against sub-optimal opponents. Therefore, we hope that our work will open up debates on how to confront this challenge head on, while simultaneously ensuring compliance with relevant guidelines and best practices. Acknowledgements Research funded by Frazer-Nash Consultancy Ltd. on be- half of the Defence Science and Technology Laboratory (Dstl) which is an executive agency of the UK Ministry of Defence providing world class expertise and delivering cutting-edge science and technology for the benefit of the nation and allies. The research supports the Autonomous Resilient Cyber Defence (ARCD) project within the Dstl Cy- ber Defence Enhancement programme. We would also like to thank Prof. Rahul Savani (University of Liverpool) for valuable conversations on the topic of principled adversarial learning. References Bates, E., Mavroudis, V., and Hicks, C. Reward Shaping for Happier Autonomous Cyber Security Agents. In Proc. of the 16th ACM Workshop on Artificial Intelligence and Security, AISec ’23, pp. 221–232, 2023. Björnerstedt, J. and Weibull, J. W. Nash equilibrium and evolution by imitation. Technical report, 1994. Brandt, F., Brill, M., Fischer, F., and Harrenstein, P. On the complexity of iterated weak dominance in constant-sum games. Theory of Computing Systems, 49:162–181, 2011. Conitzer, V. and Sandholm, T. Complexity of (iterated) dominance. In Proceedings of the 6th ACM Conference on Electronic Commerce, pp. 88–97, 2005. Fang, F., Liu, S., Basak, A., Zhu, Q., Kiekintveld, C. D., and Kamhoua, C. A. Introduction to Game Theory. Game Theory and Machine Learning for Cyber Security, pp. 21–46, 2021. Gao, Y. and Toni, F. Potential based Reward Shaping for Hierarchical Reinforcement Learning. In IJCAI, 2015. Garcia, R. and Caarls, W. Online weighted Q-ensembles for reduced hyperparameter tuning in reinforcement learning. Soft Computing, 28(13):8549–8559, 2024. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative Adversarial Nets. In Proc. of NIPS, pp. 2672–2680, 2014. Grzes, M. and Kudenko, D. Learning shaping rewards in model-based reinforcement learning. In Proc. of AAMAS Workshop on Adaptive Learning Agents, volume 115, 2009a. Grzes, M. and Kudenko, D. Theoretical and empirical analysis of reward shaping in reinforcement learning. In Proc. of ICMLA, pp. 337–344. IEEE, 2009b. Han, X., Zhang, Z., Ding, N., Gu, Y., Liu, X., Huo, Y., Qiu, J., Yao, Y., Zhang, A., Zhang, L., et al. Pre-trained models: Past, present and future. AI Open, 2:225–250, 2021. Hannay, J. CardiffUni - TTCP CAGE Challenge 2 Winning Submission. https://github.com/ john-cardiff/-cyborg-cage-2, 2022. Hicks, C., Mavroudis, V., Foley, M., Davies, T., Highnam, K., and Watson, T. Canaries and Whistles: Resilient Drone Communication Networks with (or without) Deep Reinforcement Learning. In Proc. of the 16th ACM Work- shop on AI and Security, AISec ’23, pp. 91–101, 2023. Kiely, M., Bowman, D., Standen, M., and Moir, C. On autonomous agents in a cyber defence environment. arXiv preprint arXiv:2309.07388, 2023. King, I. KEEP: A GNN-based PPO Model for MARL. https://github.com/cybermonic/ cage-4-submission, 2024. King, I. and Bowman, B. CybORG CAGE Chal- lenge 2 KEEP Agent. https://github.com/ cybermonic/CAGE-2_KEEP-Agent, 2024. Kingma, D. P. and Ba, J. Adam: A Method for Stochastic Optimization. In Proc. of ICLR, 2014. 9 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents Kipf, T. N. and Welling, M. Semi-supervised classifica- tion with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. Kuzmics, C. On the elimination of dominated strategies in stochastic models of evolution with large populations. Games and Economic Behavior, 72(2):452–466, 2011. Lanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., Pérolat, J., Silver, D., and Graepel, T. A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning. In Proc. of NIPS, pp. 4190–4203, 2017. Li, S., Wang, X., Zhang, Y., Xue, W., ˇCern`y, J., and An, B. Solving large-scale pursuit-evasion games using pre- trained strategies. In Proc. of AAAI, volume 37, pp. 11586– 11594, 2023. Liu, S., Marris, L., Hennes, D., Merel, J., Heess, N., and Graepel, T. NeuPL: Neural Population Learning. Proc. of ICLR, 2022. Lopes Antunes, D. and Llopis Sanchez, S. The Age of fight- ing machines: the use of cyber deception for Adversarial Artificial Intelligence in Cyber Defence. In Proc. of the 18th International Conference on Availability, Reliability and Security, pp. 1–6, 2023. McMahan, H. B., Gordon, G. J., and Blum, A. Planning in the presence of cost functions controlled by an adversary. In Proc. of ICML, pp. 536–543, 2003. Miles, I., Farmer, S., Foster, D., Harrold, D., Palmer, G., Parry, C., Willis, C., Mont, M. C., Gralewski, L., Menzies, R., et al. Reinforcement learning for autonomous resilient cyber defence. Presented at Black Hat USA, 2024. Nash, J. Non-Cooperative Games. Annals of mathematics, pp. 286–295, 1951. Ng, A. Y., Harada, D., and Russell, S. Policy invariance under reward transformations: Theory and application to reward shaping. In Proc. of ICML, volume 99, pp. 278–287, 1999. Nguyen, H. V., Nguyen, H. N., and Uehara, T. Multiple Level Action Embedding for Penetration Testing. In Proc. of ICFNDS, pp. 1–9, 2020. O’Driscoll, R., Hagen, C., Bater, J., and Adams, J. M. Multi- Objective Reinforcement Learning for Automated Re- silient Cyber Defence. arXiv preprint arXiv:2411.17585, 2024. Oliehoek, F. A., Savani, R., Gallego, J., Pol, E. v. d., and Groß, R. Beyond local Nash equilibria for adversarial net- works. In Benelux Conference on Artificial Intelligence, pp. 73–89. Springer, 2018. Palmer, G., Parry, C., Harrold, D. J., and Willis, C. Deep Re- inforcement Learning for Autonomous Cyber Operations: A Survey. arXiv preprint arXiv:2310.07745, 2023. Randløv, J. and Alstrøm, P. Learning to Drive a Bicycle Using Reinforcement Learning and Shaping. In Proc. of ICML, volume 98, pp. 463–471, 1998. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal Policy Optimization Algorithms. arXiv preprint arXiv:1707.06347, 2017. Seifert, C., Betser, M., Blum, W., Bono, J., Farris, K., Goren, E., Grana, J., Holsheimer, K., Marken, B., Neil, J., Nichols, N., Parikh, J., and Wei, H. Cyber- battlesim. https://github.com/microsoft/ cyberbattlesim, 2021. Microsoft Defender Re- search Team. Shashkov, A., Hemberg, E., Tulla, M., and O’Reilly, U.-M. Adversarial agent-learning for cybersecurity: a compari- son of algorithms. The Knowledge Engineering Review, 38:e3, 2023. Slantchev, B. L. Game theory: Dominance, nash equilib- rium, symmetry. Department of Political Science, Uni- versity of California–San Diego, 2008. Standen, M., Lucas, M., Bowman, D., Richer, T. J., Kim, J., and Marriott, D. CybORG: A Gym for the Development of Autonomous Cyber Agents. In IJCAI-21 1st Interna- tional Workshop on Adaptive Cyber Defense, 2021. Sutton, R. S. and Barto, A. G. Reinforcement learning: An introduction. MIT press, 2018. Symes Thompson, I., Caron, A., Hicks, C., and Mavroudis, V. Entity-based Reinforcement Learning for Autonomous Cyber Defence. In Proc. of the Workshop on Autonomous Cybersecurity, AutonomousCyber ’24, pp. 56–67, 2024. Tran, K., Standen, M., Kim, J., Bowman, D., Richer, T., Akella, A., and Lin, C.-T. Cascaded reinforcement learn- ing agents for large action spaces in autonomous penetra- tion testing. Applied Sciences, 12(21):11265, 2022. TTCP CAGE Working Group. TTCP CAGE Challenge 2. https://github.com/cage-challenge/ cage-challenge-2, 2022. TTCP CAGE Working Group. TTCP CAGE Challenge 4. https://github.com/cage-challenge/ cage-challenge-4, 2023. Von Neumann, J. Zur Theorie der Gesellschaftsspiele. Math- ematische annalen, 100(1):295–320, 1928. 10 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents Von Neumann, J. and Morgenstern, O. Theory of games and economic behavior: 60th anniversary commemorative edition. In Theory of games and economic behavior. Princeton university press, 2007. Vyas, S., Hannay, J., Bolton, A., and Burnap, P. P. Au- tomated cyber defence: A review. arXiv preprint arXiv:2303.04926, 2023. Wellman, M. P. and Mayo, K. Navigating in a space of game views. JAAMAS, 38(2):31, 2024. A. Theoretical Analysis A.1. Approximate Double Oracles In games that suffer from the curse-of-dimensionality an oracle can at best hope to find an approximate best response (ABR) (Oliehoek et al., 2018): Definition A.1 (Approximate Best Response). A policy πi ∈Πi of player i is an ABR, also referred to as a re- source bounded best response (RBBR) (Oliehoek et al., 2018), against a mixture of policies µj, iff, ∀π′ i ∈Πi, Gi(⟨πi, µj⟩) ≥Gi(⟨π′ i, µj⟩). (10) ABRs are also used to estimate the exploitability GE of the current mixtures: GE ←Gi(⟨Oi(µj), µj⟩) + Gj(⟨µi, Oj(µi)⟩) (11) If GE ≤0, then the oracles have failed to find ABRs, and a resource bounded Nash equilibrium (RBNE) has been found (Oliehoek et al., 2018). Resources in this context refers to the amount of computational power available for obtaining an ABR: Definition A.2 (Resource Bounded Nash Equilibrium). Two mixtures of policies ⟨µ1, µ2⟩are a resource-bounded Nash equilibrium iff, ∀iGi(⟨µi, µj⟩) ≥Gi(⟨Oi(µj), µj⟩). (12) Extending a proof from (Oliehoek et al., 2018) for genera- tive adversarial networks (GANs) (Goodfellow et al., 2014), we can show that GE ≤0 implies we have found a resource bounded Nash equilibrium for the ADO algorithm when applied to adversarial deep reinforcement learning: Theorem 3. If the termination condition from Equation 12 is met, then a resource bounded Nash equilibrium has been found. Proof. It can be shown that GE ≤0 implies that the agents have converged upon a resource bounded Nash equilibrium: GE = Gi(⟨Oi(µj)), µj⟩) + Gj(⟨µi, Oj(µi)⟩) ≤0 = Gi(⟨µi, µj⟩) + Gj(⟨µi, µj⟩). (13) Note that, as per Definition A.1, Gi(⟨Oi(µj), µj⟩) ≥Gi(⟨µi, µj⟩) (14) for all computable πi ∈ΠO i , where ΠO i is the set of ABRs that can be computed using an oracle O for player i. The same holds for player j. Therefore, the only way that Equa- tion 14 could fail to hold is, if µi includes policies that are not computable (not in ΠO i ) that provide a higher payoff. However, the support of µi is composed of policies com- puted in previous iterations, therefore this cannot be the case. Together with Equation 14 this directly implies the following for each agent: Gi(⟨µi, µj⟩) = Gi(⟨Oi(µj), µj⟩). (15) We have therefore found a resource bounded Nash equilib- rium. Theorem 4. Given sufficient resources to compute ϵ-best responses, then a resource bounded Nash equilibrium is an ϵ-Nash equilibrium (ϵ-NE) (Oliehoek et al., 2018). Proof. Starting from the resource bounded Nash equilib- rium with respect to the mixture profiles ⟨µi, µj⟩, we shall assume an arbitrary response by agent i. By definition of a resource bounded Nash equilibrium (Definition A.2): Gi(⟨µi, µj⟩) ≥Gi(⟨Oi(µj), µj⟩) ≥ max µ′ i Gi(⟨µ′ i, µj⟩) −ϵ. (16) A.2. Multiple Response Oracles A.2.1. CONVERGENCE GUARANTEES In this section we shall first theoretically show that our MRO algorithm does not disrupt the convergence guarantees of the double oracle formulation. Next, we shall show that, given a set of assumptions, MRO is guaranteed to produce stronger mixture agents than the ADO. We begin with the termination condition, where we note that GE can now be computed with respect to the best performing approaches from each agent’s respective sets of responses: GE ←Gi(⟨Oi(Ri(µj)), µj⟩)+ Gj(⟨µi, Oj(Rj(µi))⟩). (17) In the above equation, Oi(Ri(µj)) and Oj(Rj(µi)) return π∗ i and π∗ j , i.e., the best response within their respective response sets π∗ i ∈Ri(µj) and π∗ j ∈Rj(µi), as defined in Equation 9. Therefore, it remains the case that if GE ≤ 0, then the oracles have failed to improve on the current mixtures, and a resource bounded Nash equilibrium (RBNE) has been found. 11 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents Proof. It can be shown that GE ≤0 implies that the agents have converged upon a resource bounded Nash equilibrium for the MRO algorithm: GE = Gi(⟨Oi(Ri(µj)), µj⟩) + Gj(⟨µi, Oj(Rj(µi))⟩) ≤0 = Gi(⟨µi, µj⟩) + Gj(⟨µi, µj⟩). (18) As per Definition A.1 and Equation 9, we have: Gi(⟨Oi(Ri(µj)), µj⟩) ≥Gi(⟨µi, µj⟩) (19) for all computable πi ∈ΠO i , where ΠO i is the set of ABRs that can be computed using an oracle O for player i. There- fore, given that π∗ i ←Oi(Ri(µj)), Gi(⟨π∗ i , µj⟩) ≥Gi(⟨µi, µj⟩). (20) The same holds for player j. The only way that Equation 20 could fail to hold is if µi includes policies that are not computable by Ri, and therefore not in ΠO i , that provide a higher payoff. However, the support of µi is composed of policies computed in previous iterations, therefore this cannot be the case. We conclude that Gi(⟨Oi(Ri(µj)), µj⟩) ≥Gi(⟨µi, µj⟩) (21) for each agent, based on the respective set of responses computed using the resource bounded response function Ri. Together with Equation 19 this directly implies the follow- ing for each agent: Gi(⟨µi, µj⟩) = Gi(⟨Oi(Ri(µj)), µj⟩). (22) We have therefore found a resource bounded Nash equilib- rium. A.2.2. IMPROVED MIXTURES Next we show that, based on the following assumptions, our MRO algorithm is guaranteed to produce stronger mixtures compared to the ADO algorithm. Assumption A.1. There exists a policy πi,n p1 in the support for player p1, that therefore has a weighting wi > 0 within the mixture µp1. However, πi,n p1 was not the ABR against µn p2. Here, n represents the best response iteration during which πi,n p1 was computed. Formally, there exists another policy πj,n p1 that was a better response to the opponent mix- ture in iteration n: ∃πj,n p1 , G(⟨πj,n p1 , µn p2⟩) > G(⟨πi,n p1 , µn p2⟩). (23) Assumption A.2. The policy πi p1 belongs either to a dom- inant strategy equilibrium or a mixed strategy equilibrium that has undergone a process of iteratively eliminating weakly dominated policies using the empirical payoff ma- trix N. Therefore, given the above assumptions, the policy πi p1 cannot be removed from the agent’s set of policies without reducing the gain for p1, giving us a minimal viable support: Definition A.3 (Minimal Viable Support). Given a mixture µp1 with a minimum viable support set of polices, we cannot remove a policy πi,n p1 without reducing the gain for p1. Gp1(⟨µp1, µp2⟩) > Gp1(⟨µ′ p1, µp2⟩), (24) with µ′ p1 representing a mixture profile without πi,n p1 . Theorem 5. As per equation Equation 24, we cannot re- move the policy πi,n p1 from µp1 without impacting the value of the game. Therefore, by adding πi,n p1 to the mixture µp1, the MRO algorithm is able to obtain a stronger mixture policy for p1 than the traditional ADO algorithm with a mixture µ′ p1. Proof. ADO’s mixture µ′ p1 outperforming the MRO algo- rithm’s µp1 would imply that: Gp1(⟨µ′ p1, µp2⟩) > Gp1(⟨µp1, µp2⟩). (25) However, as per Definition 5, we cannot remove πi,n p1 with- out reducing the gain for p1. Therefore, Equation 25 could only be true, iff, µ′ p1 contained additional policies not found within µp1. However, µ′ p1 ⊂µp1. (26) Therefore, the condition from Equation 24 applies, meaning the MRO algorithm has found a mixture that achieves a larger gain when compared to the mixture obtained by the ADO algorithm. A.3. VF-PBRS - Proof of Sufficiency Ng et al. (1999) show that a reward shaping function F being a potential-based shaping function is a necessary and sufficient condition to guarantee consistency when learning an optimal policy on an MDP M′ = (S, A, P, R + F, γ) instead of M = (S, A, P, R, γ). Regarding necessity, the authors note that if F is not a potential-based shaping function, then there exist transition and reward functions such that no optimal policy in M′ is also optimal in M. With respect to sufficiency, if F is a potential-based shaping function, every optimal policy in M′ will also be optimal in M. For convenience we include Ng et al. (1999)’s proof of sufficiency below. As in the original PBRS paper the proof assumes scenario-optimal policies can be obtained, hence we relate it to VF-PBRS within the context of double oracles rather than the ADO algorithm, where at best we can hope to obtain approximate best responses. First, to recap: 12 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents Theorem 6. We assume a state space S, actions A and discount factor γ. Given a reward shaping function F : S × A × S →R, F is considered a potential-based shaping function if there exists a real-value function Φ : S →R such that for all s ∈S −{s0}, a ∈A, s′ ∈S, where s0 represents an absorbing state, F(s, a, s′) = γΦ(s′) −ϕ(s). (27) Note that in the above equation, if S does not have an absorbing state, S −{s0} = S then γ < 1. Given the above, F being a potential-based shaping function is a sufficient condition to guarantee that any policy obtained under M′ = (S, A, P, R + F, γ) will be consistent with an optimal policy for M = (S, A, P, R, γ) (and vice versa). Proof. We assume a VF-PBRS function F of the form pro- vided in Equation 6. If γ = 1, the replacing of Φ(s) with Φ′(s) = Φ(s) −k for any constant k will not impact the shaping reward F. This is due to F being the difference in potentials. Therefore, given an absorbing state s0, we can replace Φ(s) with Φ′(s) = Φ(s) −Φ(s0), and assume that our potential function Φ(s) satisfies Φ(s0) = 0. We know that for the original MDP M the optimal Q- function satisfies the Bellman Equations (Sutton & Barto, 2018): Q∗ M(s, a) = Es′ Ps,a(·)  R(s, a, s′)+ γ max a′∈A Q∗ M(s′, a′)  . (28) If we subtract Φ(s) from both sides and apply some simple algebraic manipulations we get: Q∗ M(s, a) −Φ(s) = Es′ Ps,a(·)  R(s, a, s′)+ γΦ(s′) −Φ(s)+ γ max a′∈A  Q∗ M(s′, a′) −Φ(s′)  . (29) Upon defining ˆQM′(s, a) ≜Q∗ M(s, a)−Φ(s) and substitut- ing that and F(s, a, s′) = γΦ(s′) −Φ(s) into Equation 29, we get: ˆQM′(s, a) = Es′ R(s, a, s′) + F(s, a, s′) + γ max a′∈A ˆQM′(s′, a′)  , = Es′ R′(s, a, s′) + γ max a′∈A ˆQM′(s′, a′)  . (30) This is the Bellman equation for M′. Therefore: Q∗ M′(s, a) = ˆQM′(s, a) = Q∗ M(s, a) −Φ(s). (31) As a result, the optimal policy found by our oracle using VF-PBRS for M ′ satisfies: π∗ M′(s) = argmax a∈A Q∗ M′(s, a), = argmax a∈A Q∗ M(s, a) −Φ(s), = argmax a∈A Q∗ M(s, a). (32) B. Network Diagrams B.1. CAGE Challenge 2 In Figure 6 we provide an illustration of the network layout for CC2. We train our agents on Scenario2, and include a depiction of the Red agent’s access routes (red dotted line). A detailed description of this challenge can be found in TTCP CAGE Working Group (2022). 0 4 3 2 1 0 1 2 0 1 2 UserX EnterpriseX Op_HostX Defender Op_Server0 Firewall X X X Red Access Route Figure 6. CAGE Challenge 2 Network Layout. B.2. CAGE Challenge 4 Figure 7 depicts the network layout for CC4. The network is defended by five Blue agents. Deployed networks A and B have two ACD agents each, one per Restricted / Operational zone. The remaining ACD agent is tasked with defending the HQ network. At every step there is a small probability that a Red cyber-attacking agent will spawn if a Green (user) agent opens a phishing email. Red agents can also spawn upon a Green user accessing a compromised service. Further details can be found in TTCP CAGE Working Group (2023). 13 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents Public Access Zone Admin Network Office Network Restricted Zone A Operational Zone A Restricted Zone B Operational Zone B Deployed Network B Deployed Network A HQ Network and Public Services Contractor Network Figure 7. CAGE Challenge 4 Network Layout. C. Experiment Settings Below we provide an overview of our experiment settings. We begin by outlining the configurations that are used across our MRO runs. We subsequently provide details regarding CAGE Challenge 2 and 4 specific configurations. Policy evaluations: Policy evaluations consist of 100 eval- uation episodes. This includes: i.) The evaluation of re- sponses following each ABR iteration; ii.) Payoff table augmentations; iii.) Significance testing, and; iv.) The visualisation of policy characteristics. Pre-Trained Model Sampling: With respect to PTM sam- pling, both our CC2 and CC4 runs use our discussed ϵ- greedy based approach for gradually sampling from policies computed during the respective runs. An ϵ discount rate of 0.95 is applied following each ABR iteration. VF-PBRS: Unless specified otherwise, the VF-PBRS ora- cles use a scaling coefficient τ = 1. A further key parameter for VF-PBRS is the value to select for γ. As noted by Ng et al. (1999): “Future experience with potential-style shap- ing rewards may also lead one to occasionally try shaping rewards that are inspired by potentials, but which are per- haps not strictly of the form given”. The authors go on to note that for certain problems, it may be easier for an expert to propose an undiscounted shaping function Φ(s′) −Φ(s), even when using γ ̸= 1 for training the agents. Grzes & Kudenko (2009b) note that when given two states s and s′, such that Φ(s) < Φ(s′), then the agent should be rewarded by the shaping reward for transitioning from s →s′. Furthermore, the agent should not be rewarded for transitioning from s′ →s or remaining in s, s →s. Therefore, given a potential function Φ, a suitable value for γ is required where the following properties hold for the mentioned s and s′: F(s, s′) = γΦ(s′) −Φ(s) ≥0, (33) F(s′, s) = γΦ(s) −Φ(s′) ≤0, (34) F(s, s) = γΦ(s) −Φ(s) ≤0. (35) Grzes & Kudenko (2009b) show that finding a γ that meets the above requirements is challenging, even when dealing with well defined heuristics, such as the straight line distance to a given target. In this work we observe that this challenge is amplified when utilising value functions as potential func- tions. Therefore, we consider two discount rates, using γϕ to refer to the discount factor applied to the potential of the follow-on state Φ(s′). We often encounter scenarios where, despite Φ(s) < Φ(s′), we observe γϕΦ(s′) −Φ(s) < 0 when using γϕ = γ with a standard discount rate, e.g., 0.99. As a result, for our current experiment we use a discount rate γ = 1. Evaluating the impact of the value of γ on VF- PBRS for double oracles represents and interesting avenue for future research. C.1. Cage Challenge 2 For CAGE Challenge 2 we use the default configuration provided with the repository for conducting our training runs. Each training and evaluation episode consisted of 100 time-steps. Unless specified otherwise, each oracle uses 1.5M training steps per response. Below we provide an overview of oracle specific configurations. Cardiff University (Blue): We make the following changes in order to utilise the CAGE Challenge 2 winning ap- proach from Hannay (2022) as an oracle: i.) We re- move the fingerprinting mechanism designed to iden- tify the rules-based Red agents used in the challenge, and; ii.) We add parallel data gathering to reduce wall- time (Schulman et al., 2017). As a starting point, we subsequently obtain a CardiffUniform agent, which is trained in a setting where we uniformly sample over the two provided rules-based Red agents (B-line and Me- ander). CardiffUniform achieves comparable results to the version using fingerprinting (−14.74 compared to 13.76 against the Meander, and −16.85 compared to 16.6 against B-line, with the latter scores being those achieved by Cardiff’s submission to CAGE Challenge 2). While we find all CardiffUniform (and provided) Cardiff university approaches to be highly exploitable when confronted with Red ABRs, we note that the approach has a lot of merit with respect to: i.) Reducing the action space in relation to available decoys, using domain knowledge, and; ii.) Buffers for keeping track of various properties throughout each episode. Therefore, we hypothesise that the Cardiff implementation can be used to reduce exploitability, if trained within an adversarial learning framework where the approach is presented with a set of more challenging Red opponents. For this purpose we add the Cardiff approach as a response oracle. With respect to hyperparameters our oracles use the same ones as those provided by Hannay (2022): 14 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents Hyperparameter Value Optimiser: Adam (Kingma & Ba, 2014) Learning rate: 0.002 VF loss coefficient: 0.5 Adam betas: 0.9, 0.99 Clipping ϵ: 0.2 Discount rate γ: 0.99 Num. epochs: 6 Num. workers: 10 Horizon (Environment Timesteps): 100 Entropy coefficient: 0.01 Batch Size: 1, 000 Table 2. Hyperparameters used by the Cardiff oracle for CC2. Cybermonic Graph Inductive PPO (Blue): No modifi- cations are made to King & Bowman (2024)’s Graph-PPO agents. The hyperparameters used by the algorithm are: Hyperparameter Value Optimiser: Adam (Kingma & Ba, 2014) Actor learning rate: 0.0003 Critic learning rate: 0.001 VF loss coefficient: 0.5 Clipping ϵ: 0.2 Discount rate γ: 0.99 Num. epochs: 4 Num. workers: 10 Horizon (Environment Timesteps): 100 Entropy coefficient: 0.01 Batch Size: 2, 048 Table 3. Inductive GPPO Hyperparameters. Action-Masking PPO (Red): Our action masking PPO agent is implemented within RLlib, and masks invalid actions through adding a large negative value to the respective action logits. The hyperparameters used by the algorithm are: Hyperparameter Value Optimiser: Adam (Kingma & Ba, 2014) Learning rate: 1e −5 VF loss coefficient: 0.25 Clipping ϵ: 0.2 Discount rate γ: 0.99 Num. epochs: 4 Num. workers: 10 Horizon (Environment Timesteps): 100 Entropy coefficient: 0.005 Batch Size: 1, 000 Table 4. Action-Masking PPO Hyperparameters. C.2. Cage Challenge 4 Cybermonic Graph Inductive Multi-Agent PPO (Blue): The main differences from the single agent Graph-PPO agent framework by King & Bowman (2024) and the multi- agent version (King, 2024) are as follows: Hyperparameter Value Num. workers: 25 Horizon (Environment Timesteps): 500 Batch Size: 2, 500 Table 5. Multi-Agent GPPO Hyperparameters. Action-Masking Multi-Agent PPO (Red): The hyperpa- rameters used by our MAPPO Red agent are largely un- changed, compared to the single agent case. The only dif- ferences being: Hyperparameter Value Num. workers: 25 Horizon (Environment Timesteps): 500 Entropy coefficient: 0.001 Batch Size: 1, 024 Table 6. Action-Masking MAPPO Hyperparameters. D. Strictly and Weakly Dominated Strategies In this section we shall consider the different types of mix- tures ⟨µi, µj⟩that a Nash solver may yield for agents i and j given a normal-form game N, and the definitions for strictly and weakly dominated strategies. First we distinguish be- tween a pure and a mixed strategy equilibrium. As the name indicates, for a pure strategy Nash equilibrium both players place a 100% probability on playing a single strategy re- spectively. A mixed strategy Nash equilibrium, meanwhile, involves at least one player using a weighted randomised strategy selection process. Von Neumann & Morgenstern (2007) showed that a mixed-strategy Nash equilibrium will exist for any zero-sum game with a finite set of strategies. Here, it is worth noting that, given a mixed strategy Nash equilibrium ⟨µi, µj⟩, for a mixed strategy µi each pure strat- egy included in the mix (i.e., with a weighting above zero) is itself a best response against µj, and will yield the same payoff as Gi(⟨µi, µj⟩) (Fang et al., 2021). Theorem 7 (Mixed Strategy Theorem). If a mixed strategy µi, belonging to player i, is a best response to the (mixed) strategy µj of player j, then, for each pure strategy (action) ak with a weight µi(ak) > 0 it must be the case that ak is itself a best response to µj. Therefore, Gi(⟨ak, µj⟩) must be the same for all strategies (actions) included in the mix. We note that a payoff matrix may feature strictly dominated strategies, weakly dominated strategies and strictly domi- nant strategies. Strictly dominated strategies (actions) are strategies that always deliver a worse outcome than at least 15 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents one alternative strategy, regardless of the strategy chosen by the opponent. Strictly dominated strategies can, therefore, be safely deleted from a normal-form game. We provide for- mal definitions for the remaining concepts below (Slantchev, 2008). Definition D.1 (Strictly Dominant Action). Strictly dom- inant actions are always optimal, therefore, a∗ i ∈A is a strictly dominant action for player i iff: Gi(⟨a∗ i , a−i⟩) > Gi(⟨ai, a−i⟩)∀ai ̸= a∗ i , ∀a−i ∈A−i. Within the above definition A−i refers to the set of opponent actions. Next we consider weakly dominated actions: Definition D.2 (Weakly Dominant Action). An action is weakly dominant action if it is always optimal, while every other action is sometimes sub-optimal: Gi(⟨a∗ i , a−i⟩) ≥Gi(⟨ai, a−i⟩)∀ai ̸= a∗ i , ∀a−i ∈A−i, with a strict inequality for some a−i ∈A−i, for any ai ̸= a∗ i . It is often feasible, within zero-sum games, to remove weakly dominated strategies without impacting the value of the game. However, the order in which weakly dominated strategies are eliminated can impact the value of the subse- quent sub-game, compared to original game. Determining the correct order for removing weakly dominated strategies from a normal-form game, without impacting the value of the game, represents a computationally challenging problem, with respect to the number of strategies available (Brandt et al., 2011). E. Payoff Matrix Augmentation We illustrate the computational impact of using our pro- posed MRO formulation, compared to the original ADO algorithm. First we highlight the exponential growth of the payoff table following each response iteration for MRO, compared to the quadratic growth for the ADO algorithm, in Figure 8a. For this plot we assume that both Blue and Red use four approaches for computing responses, i.e., m = n = 4. Next, in Figure 8b, we consider the num- ber of additional cells that are added to the empirical payoff matrix in each iteration, again assuming four response ap- proaches for Blue and Red. We observe that in iteration 100 over 3,000 new cells would need to be added to the payoff matrix. Finally, in Figure 9 we depict the exponen- tial growth in the size of the payoff matrix after 100 ABR iterations when varying the number of response approaches for MRO. 0 20 40 60 80 100 Best response iteration 0 20000 40000 60000 80000 100000 120000 140000 160000 Payoff Matrix Size Multi Response Oracles Approximate Double Oracles (a) Payoff Matrix Growth 0 20 40 60 80 100 Best response iteration 0 500 1000 1500 2000 2500 3000 Blue-Red Joint-Policy Evaluations Multi Response Oracles Approximate Double Oracles (b) Matrix Augmentations Figure 8. In the plots above we illustrate the computational com- plexities of MRO compared to the ADO algorithm with respect to the size of the increase in the size of the empirical payoff matrix (Figure 8a) and the number of new cells that need to be computed in each iteration (Figure 8b). 0 20 40 60 80 100 Number of response approaches. 0 20000000 40000000 60000000 80000000 100000000 Payoff matrix size (100 iters) Figure 9. We depict the impact of the number of methods used from computing responses within each response iteration on the size of the payoff matrix. F. Preliminary CAGE Challenge 2 Run During one of our early preliminary runs within CAGE Challenge 2 we evaluated the impact of using: i.) VF- PBRS; ii.) initialising from PTMs; iii.) VF-PBRS & PTMs, and; Vanilla runs without PTMs and VF-PBRS. Results are shown in Figure 10. 2 4 6 8 10 12 14 16 Approximate Best Response (ABR) Iteration 100 80 60 40 20 Mean Episodic Rewards (Blue) Nash Payoff: blue( blue, red) Blue ABRs Red ABRs Blue VF-PBRS & PTMs Blue PTMs Red VF-PBRS & PTMs Red VF-PBRS Red PTMs Figure 10. A preliminary CAGE Challenge 2 run, comparing dif- ferent types of oracles. While four oracles (VF-PBRS; PTMs; VF-PBRS & PTMs, and; Vanilla) are used in each iteration, we only depict the best performing response. We find the majority of ABRs were obtained by oracles using VF-PBRS & PTMs for both Blue and Red. Although Red computed two noteworthy responses using only VF-PBRS in iterations 2 and 7. 16 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents We found that for both Blue and Red oracles using VF- PBRS & PTMs returned the majority of ABRs. Interestingly, in this preliminary run we also had two Red VF-PBRS ABRs that converged upon significantly better responses than the Red mixture in iterations 2 and 7, already hinting at the benefits of using both PTM and freshly initialised runs during training. G. Delayed Convergence Examples During preliminary trial runs we found that when train- ing Blue agents (without PTMs), they would often achieve significant improvements after a large number of training steps. This is illustrated in Figure 11, where GPPO and Cardiff ABRs (in the third ABR iteration of a preliminary run) significantly improved their policies after 2.5M and 1.5M time-steps respectively. Therefore, we turn to PTMs, to address the long wall-times associated with these “full” runs (approximately one day per run). 0 1000 2000 3000 4000 5000 1000 steps per tick 100 80 60 40 20 Blue Reward VF-PBRS GPPO VF-PBRS Cardiff Figure 11. An example of ABRs from preliminary trial runs, which made noteworthy improvements after several million training steps. H. Policy Characteristics – CAGE Challenge 2 In this section we expand upon the comparison between the starting point of our training process, πo Blue (the parame- terisation available in the repository maintained by King & Bowman (2024)), and the final mixture µBlue. We note that the final mixture agent consists of GPPO agents trained using King & Bowman (2024)’s approach. The difference between πo Blue and the policies used by µBlue is the oppo- nents faced during training. To recap, as our empirical game in Figure 3 illustrates, the majority of the Blue policies with an above zero sampling weighting within the final mixture generalise well against all the responses computed by Red during the MRO run. In contrast, for πo Blue there exist a number of Red responses capable of forcing it into taking costly actions. The second Red ABR, π2 Red, has the biggest impact on πo Blue, reducing the Blue agent’s payoff to −117.82. In contrast, µBlue achieves a mean payoff of −21.96 against π2 Red. We find that π2 Red’s success against πo Blue comes from ini- tially targeting User2 during the first 5 time-steps, and sub- sequently shifting its attention to Enterprise1 (See Fig- ure 12). In contrast, Blue spends the first 5 steps launching decoy services on Enterprise0. Red meanwhile obtains the IP address of Enterprise1 upon gaining a hold of User2 (see the network digram in Figure 6). From this point on the two agents are locked in a cycle where πo Blue restores Enterprise1, only for Red to regain access. Red Targets User0 User1 User2 User3 User4 Defender Enterprise0 Enterprise1 Enterprise2 Op_Host0 Op_Host1 Op_Host2 Op_Server0 17.5% 35.0% 52.5% 70.0% (a) Time-steps 1–5 User0 User1 User2 User3 User4 Defender Enterprise0 Enterprise1 Enterprise2 Op_Host0 Op_Host1 Op_Host2 Op_Server0 15.0% 30.0% 45.0% 60.0% (b) Time-steps 6–100 Blue Targets User0 User1 User2 User3 User4 Defender Enterprise0 Enterprise1 Enterprise2 Op_Host0 Op_Host1 Op_Host2 Op_Server0 17.5% 35.0% 52.5% 70.0% (c) Time-steps 1–5 User0 User1 User2 User3 User4 Defender Enterprise0 Enterprise1 Enterprise2 Op_Host0 Op_Host1 Op_Host2 Op_Server0 15.0% 30.0% 45.0% 60.0% (d) Time-steps 6–100 Figure 12. Nodes targeted by the original GPPO parameterisation πo Blue and the second Red ABR π2 Red when facing each other. Values are averaged over 100 evaluation episodes. In contrast, under ⟨µBlue, π2 Red⟩(GBlue(µBlue, π2 Red) = −21.96), µBlue proactively analyses and restores user hosts, resulting in π2 Red being mostly contained to the user net- work Figure 13. In fact, the majority of µBlue’s actions focus on User0 (See Figure 13a). This indicates that in- stances where Red manages to gain user or system level privileges on the user hosts 1 – 4 are rare. User0 User1 User2 User3 User4 Defender Enterprise0 Enterprise1 Enterprise2 Op_Host0 Op_Host1 Op_Host2 Op_Server0 15.0% 30.0% 45.0% 60.0% (a) Blue Targets User0 User1 User2 User3 User4 Defender Enterprise0 Enterprise1 Enterprise2 Op_Host0 Op_Host1 Op_Host2 Op_Server0 7.5% 15.0% 22.5% 30.0% (b) Red Targets Figure 13. Policy characteristics, actions and targets from time- steps 1 - 100 for the final Blue mixture agent µBlue against π2 Red. 17 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents To complement the spider plots in Figures 12 and 13, we provide a breakdown of the action distributions for Blue and Red agents conditioned on the level of Red access in Tables 7 – 10. The tables enable a comparison of the ac- tions taken by Blue and Red under the joint policy profiles ⟨πo Blue, π2 Red⟩in and ⟨µBlue, π2 Red⟩, respectively. Specifi- cally, they list the percentage of actions used on listed nodes under three conditions: i.) Actions targeting nodes before User/Privileged access by Red has been acquired; ii.) Ac- tions upon Red obtaining user access, and; iii.) Actions upon Red obtained privileged (i.e., SYSTEM/root) access. Blue Actions Overview for ⟨πo Blue, π2 Red⟩ Host Analyse DecoyApache DecoyFemitter DecoyHarakaSMPT DecoySSHD DecoySmss DecoySvchost DecoyTomcat DecoyVsftpd Remove Restore Blue Actions before User/Privileged Access Enterprise0 - - 0.77 0.06 - 0.02 - 0.12 0.03 - - Enterprise1 - - 0.13 - - - - - - - 0.87 Enterprise2 - - 0.99 - - 0.01 - - - - - Op_Host0 - - 1.00 - - - - - - - - Op_Server0 - - 1.00 - - - - - - - - User1 - - 0.36 - - - - - - - 0.64 User2 - - - - - - - - - - 1.00 User3 0.30 - 0.23 - - - - - - - 0.47 User4 0.11 - - - - - - - - - 0.89 Blue Actions in response to User Access Enterprise1 - - 0.20 - - - - - - - 0.80 User2 - - 0.17 0.04 - - - 0.13 - - 0.65 User3 0.03 - 0.03 0.05 - - - 0.87 - - 0.03 User4 - - 0.04 - - - - 0.91 - - 0.04 Blue Actions in response to Privileged Access Enterprise1 - - 0.08 - - - - - - - 0.91 User0 0.41 - 0.42 - 0.08 0.03 - - - - 0.05 User1 0.09 - 0.45 - - - - 0.36 - - 0.09 User2 0.02 - 0.22 0.34 - - - 0.33 - - 0.09 User3 - - - 0.13 - - - 0.87 - - - User4 - - 0.30 0.10 - - - 0.60 - - - Table 7. CAGE Challenge 2 – Blue action distribution on hosts and servers under the joint-policy profiles ⟨πo Blue, π2 Red⟩. Blue Actions Overview for ⟨µBlue, π2 Red⟩ Host Analyse DecoyApache DecoyFemitter DecoyHarakaSMPT DecoySSHD DecoySmss DecoySvchost DecoyTomcat DecoyVsftpd Remove Restore Blue Actions before User/Privileged Access Defender - 0.52 - 0.39 - - - - 0.10 - - Enterprise0 0.70 0.12 0.03 0.05 - - 0.06 - 0.02 - 0.02 Enterprise1 - - 0.17 0.71 - 0.02 0.03 - - - 0.07 Enterprise2 - - 0.67 0.33 - - - - - - - Op_Host0 0.21 0.07 - 0.11 - 0.57 - - 0.04 - - Op_Host1 0.48 0.10 - 0.31 - - - - 0.12 - - Op_Host2 - 0.20 - 0.65 - - - - 0.15 - - Op_Server0 0.47 0.06 0.01 0.01 - - 0.34 - 0.01 0.11 - User1 - 0.02 0.12 0.09 - - - - - - 0.76 User2 - 0.01 - - - - - - - - 0.99 User3 0.19 0.31 - 0.26 - - - - 0.01 - 0.23 User4 0.22 0.09 - 0.15 - - - - 0.02 - 0.52 Blue Actions in response to User Access Enterprise0 0.98 - - - - - - - - - 0.02 User1 - - - - - - - - - - 1.00 User2 0.68 - 0.04 0.10 - - - - 0.01 - 0.18 User3 0.34 - 0.01 0.63 0.01 - - - - - 0.02 User4 0.74 - 0.03 0.20 - - - - - - 0.03 Blue Actions in response to Privileged Access Enterprise1 - - - - - - - - - - 1.00 User0 0.40 - 0.08 0.02 0.49 - 0.01 - - 0.01 - User1 0.56 - 0.01 0.40 - - - - - - 0.02 User2 0.72 - - 0.02 - - - - - - 0.25 User3 0.88 - - 0.11 - - - - - - 0.01 User4 0.80 0.02 0.04 0.13 - - - - - - 0.02 Table 8. CAGE Challenge 2 – Blue action distribution on hosts and servers under the joint-policy profiles ⟨µBlue, π2 Red⟩. Tables 7 and 8 show that, in contrast to πo Blue, µBlue ded- icates time towards analysing the user hosts and restoring them when necessary. The policies also differ in their ap- proach towards protecting the operational subnet. The mix- ture agent µBlue is proactive, analysing and safe-guarding the operational subnet on 5.66% of time-steps, despite π2 Red not posing a threat. In contrast, πo Blue dedicates only 0.02% of time-steps towards the operational subnet. Red Actions Overview for ⟨πo Blue, π2 Red⟩ Host BlueKeep DiscoverNetworkServices EternalBlue ExploitRemoteService FTPDirectoryTraversal HTTPRFI HTTPSRFI HarakaRCE Impact PrivilegeEscalate SQLInjection SSHBruteForce Red Actions before obtaining User Access Enterprise1 - 0.02 0.80 0.01 0.09 - - 0.04 - - 0.04 - User1 0.71 - 0.23 - - 0.01 0.02 0.01 - - 0.01 - User2 - 0.18 - 0.05 0.47 0.03 0.02 0.02 0.03 0.15 0.03 0.03 User3 0.65 0.10 0.19 0.01 0.03 - - - - - 0.02 0.01 User4 0.86 0.02 0.08 0.01 0.04 - - - - - - - Red Actions upon obtaining User Access Enterprise1 0.29 - - - - - 0.43 - - - - 0.29 User1 - - 0.10 - - - - - - - - 0.90 User2 0.96 - 0.03 - - - - - - - - - User3 0.11 0.04 0.04 - 0.07 0.09 0.61 - - - - 0.04 User4 0.12 - - - - 0.31 0.23 - - - 0.04 0.31 Red Actions upon obtaining Privileged Access Enterprise1 1.00 - - - - - - - - - - - User0 0.58 - 0.26 0.01 0.04 0.01 0.01 0.02 0.01 0.03 0.02 - User1 0.16 - 0.18 - 0.63 - - - - - 0.02 - User2 0.29 0.01 0.46 0.01 0.03 0.01 0.01 - - 0.16 0.01 - User3 0.44 - 0.07 0.04 0.04 - 0.04 0.37 - - - - User4 0.36 0.04 - - 0.04 - 0.04 0.20 - - 0.32 - Table 9. CAGE Challenge 2 – Red action distribution on hosts and servers under the joint-policy profiles ⟨πo Blue, π2 Red⟩. With respect to Red attacks against πo Blue, upon obtaining Enterprise1’s IP address through gaining privileged access on User2, π2 Red begins launching EternalBlue attacks against Enterprise1 (See Table 9). Here, it is worth noting that there are classes of attacks that allow Red to jump straight to privileged access on windows machines, including BlueKeep and EternalBlue15. Hosts on the user and enterprise subnets belong to this category. As a result Blue πo Blue is forced into taking the restore action (Table 7). I. CAGE Challenge 4 Results As discussed in Section 7.2, upon applying MRO to CC4 we observe convergence following only a single ABR itera- tion. This is captured in Figure 14; where following ABR iteration 1 both defending and attacking agents are unable to significantly improve on the Nash payoff (the black line). With respect to the two oracles settings, VF-PBRS oracles compute the first three ABRs for Blue, followed by two iterations where Vanilla responses outperform VF-PBRS. The final Blue ABR is also produced by a VF-PBRS oracle. For Red only 2/6 ABRs originate from a VF-PBRS oracle. Interestingly 2/3 responses featured in the final Red mixture stem from VF-PBRS oracles (Figure 15). Similarly, the final 15See action tutorials in TTCP CAGE Working Group (2022). 18 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents Red Actions Overview for ⟨µBlue, π2 Red⟩ Host BlueKeep DiscoverNetworkServices EternalBlue ExploitRemoteService FTPDirectoryTraversal HTTPRFI HTTPSRFI HarakaRCE Impact PrivilegeEscalate SQLInjection SSHBruteForce Red Actions before obtaining User Access Enterprise1 - - 0.04 - - 0.19 0.65 - - - 0.12 - User1 0.62 - 0.18 - - 0.09 0.03 0.05 - - 0.02 - User2 - 0.01 - 0.02 0.17 0.05 0.04 0.06 0.01 0.60 0.03 0.01 User3 0.54 0.04 0.23 - 0.14 - - - - - 0.03 0.02 User4 0.52 0.01 0.25 - 0.22 - - - - - - - Red Actions upon obtaining User Access Enterprise0 0.80 0.01 0.01 0.14 - - - - - - - 0.03 User1 - - 0.02 - 0.02 - - - - - - 0.95 User2 0.75 - 0.12 0.01 0.09 - - 0.03 - - - - User3 0.30 0.02 0.04 - 0.04 0.26 0.24 0.04 - - 0.03 0.05 User4 0.31 - 0.10 - 0.08 0.14 0.15 0.04 - - 0.04 0.14 Red Actions upon obtaining Privileged Access Enterprise1 1.00 - - - - - - - - - - - User0 0.56 - 0.18 - 0.13 0.03 0.03 0.05 - 0.01 0.01 - User1 0.38 - 0.15 - 0.46 0.01 - - - - - - User2 0.12 - 0.81 - 0.05 - - 0.01 - - - - User3 0.27 0.01 0.08 0.01 0.08 - 0.02 0.53 - 0.01 - - User4 0.22 - 0.01 - 0.07 0.05 0.01 0.34 0.02 0.03 0.23 0.02 Table 10. CAGE Challenge 2 – Red action distribution on hosts and servers under the joint-policy profiles ⟨µBlue, π2 Red⟩. 1 2 3 4 5 6 Approximate Best Response (ABR) Iteration 480 460 440 420 400 380 Mean Episodic Rewards (Blue) Nash Payoff: blue( blue, red) Blue ABRs Red ABRs Blue VF-PBRS MAGPPO (PTM=True) Blue MAGPPO (PTM=True) Blue VF-PBRS MAGPPO (PTM=False) Red AM-MAPPPO (PTM=True) Red VF-PBRS AM-MAPPPO (PTM=True) Red AM-MAPPPO (PTM=False) Figure 14. A depiction of Blue and Red ABRs from our MRO run on CC4. Rewards are plotted from Blue’s perspective. Both agents are unable to find ABRs that significantly improve on the Nash payoff (the black line) following the first ABR iteration. Blue mixture also features two VF-PBRS responses, and one vanilla response. Both of the final Blue responses computed without PTMs are present in the final mixture. However, upon inspecting the empirical game it becomes apparent that there are no significant differences between VF-PBRS and Vanilla responses (with the exception of VAN-1 and PBRS- 4). The top row of the payoff matrix meanwhile shows that the original Cybermonic parameterisation is vulnerable against all the computed Red responses. J. Why do we need mixed strategies? Due to our empirical games for CC2&4 being finite zero- sum games, a Nash solver can be used to find the optimal weighting for each set of policies. This is enabled by us selecting a zero-sum reward for training, with Red receiving the negation of the Blue reward. Therefore, Red is in effect learning to degrade Blue’s performance. At this point it is worth noting, each pure strategy that is O 1 2 3 4 5 6 * 1 2 3 4 5 6 * O 1 2 3 4 5 6 * 1 2 3 4 5 6 * Vanilla VF-PBRS Vanilla VF-PBRS 0.355 0.445 0.2 Red Mixture Red 0.279 0.415 0.306 Blue Mixture Blue 360 380 400 420 440 460 480 500 Rewards from the cyber-attacking agent (Red)'s perspective Figure 15. An illustration of CC4’s empirical game and Nash mix- tures. Cells represent the mean episodic reward for each policy pairing (100 evaluation episodes). Darker cells represent match- ups that are favorable for Red. X and Y ticks indicate the oracle used–VF-PBRS (PBRS) and Vanilla (VAN)–and the ABR iteration in which a response was learnt. The top row and left column for Blue and Red represent the original/starting parameterisations (O). Responses in iteration 6 are computed without using PTMs for initialisation. These, “full” runs consist of 5M environment steps, compared to the 2.5M used when initialising from PTMs. found within a mixture, regardless of weighting, is itself a best response against the current opponent mixture (Björn- erstedt & Weibull, 1994). We illustrate this fact in Figures 16 and 17 for CC2 and CC4 respectively; all policies with a mixture weighting above zero achieve an identical max reward against the current opponent mixture. Figures 16 and 17 may give the impression that an in- dividual best response policy could be deployed. How- ever, this is equivalent to repeatedly using a predictable pure strategy in the game of rock paper scissors. For example, if Blue chose to only deploy Cybermonic (PTM=True) 14, then Red could respond with select- ing AM-PPO (PTM=False) 1, and achieve a payoff of 99.00 (See Figure 3). In contrast, with Blue playing the mixture µBlue, Red will always obtain a worse payoff upon deviating from µRed within the current empirical game. 19 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents 0.0 0.1 0.2 0.3 Mixture Blue 0.17 0.19 0.28 0.11 0.24 0.01 Cybermonic-G Cybermonic-O Cybermonic (PTM=True) 1 Cybermonic (PTM=True) 2 Cybermonic (PTM=True) 3 Cybermonic (PTM=True) 4 Cybermonic (PTM=True) 5 Cybermonic (PTM=True) 6 Cybermonic (PTM=True) 7 Cybermonic (PTM=True) 8 Cybermonic (PTM=True) 9 Cybermonic (PTM=True) 10 Cybermonic (PTM=True) 11 Cybermonic (PTM=True) 12 Cybermonic (PTM=True) 13 Cybermonic (PTM=True) 14 Cybermonic (PTM=True) 15 Cybermonic (PTM=True) 16 Cybermonic (PTM=True) 17 Cybermonic (PTM=True) 18 Cybermonic (PTM=True) 19 Cybermonic (PTM=True) 20 Cybermonic (PTM=False) 1 Cybermonic (PTM=False) 2 Cybermonic (PTM=False) 3 Cardiff-G Cardiff-U Cardiff (PTM=True) 1 Cardiff (PTM=True) 2 Cardiff (PTM=True) 3 Cardiff (PTM=True) 4 Cardiff (PTM=True) 5 Cardiff (PTM=True) 6 Cardiff (PTM=True) 7 Cardiff (PTM=True) 8 Cardiff (PTM=True) 9 Cardiff (PTM=True) 10 Cardiff (PTM=True) 11 Cardiff (PTM=True) 12 Cardiff (PTM=True) 13 Cardiff (PTM=True) 14 Cardiff (PTM=True) 15 Cardiff (PTM=True) 16 Cardiff (PTM=True) 17 Cardiff (PTM=True) 18 Cardiff (PTM=True) 19 Cardiff (PTM=True) 20 Cardiff (PTM=False) 1 Cardiff (PTM=False) 2 Cardiff (PTM=False) 3 Blue Policies 120 100 80 60 40 20 0 Blue Reward -62.7582 -30.2283 -64.8543 -35.5853 -51.0073 -43.0466 -36.0292 -31.8589 -32.9285 -36.4371 -31.9779 -32.0558 -54.7496 -45.4719 -29.6237 -29.6237 -29.6237 -31.6621 -29.6237 -37.3743 -58.5912 -40.4985 -58.5542 -29.6237 -29.6237 -83.2728 -30.8293 -84.0896 -55.4609 -77.1591 -57.6298 -46.6335 -75.54 -50.6479 -49.5815 -47.6905 -46.1008 -62.185 -81.1587 -51.502 -57.9288 -50.3626 -48.4127 -35.3976 -45.6496 -90.5292 -79.3782 -87.2259 -35.11 -34.7022 blue( ) = 0, Blue( , Red) blue( ) > 0, Blue( , Red) blue( Blue, Red) (a) Performance of each Blue response against the final Red mixture µRed. 0.0 0.2 0.4 Mixture Red 0.21 0.01 0.01 0.3 0.43 0.05 AM-PPO-G AM-PPO (PTM=True) 1 AM-PPO (PTM=True) 2 AM-PPO (PTM=True) 3 AM-PPO (PTM=True) 4 AM-PPO (PTM=True) 5 AM-PPO (PTM=True) 6 AM-PPO (PTM=True) 7 AM-PPO (PTM=True) 8 AM-PPO (PTM=True) 9 AM-PPO (PTM=True) 10 AM-PPO (PTM=True) 11 AM-PPO (PTM=True) 12 AM-PPO (PTM=True) 13 AM-PPO (PTM=True) 14 AM-PPO (PTM=True) 15 AM-PPO (PTM=True) 16 AM-PPO (PTM=True) 17 AM-PPO (PTM=True) 18 AM-PPO (PTM=True) 19 AM-PPO (PTM=True) 20 AM-PPO (PTM=False) 1 AM-PPO (PTM=False) 2 AM-PPO (PTM=False) 3 Red Policies 0 5 10 15 20 25 30 35 Red Reward 23.1954 27.8934 25.8979 25.307 23.0868 10.7286 27.791 21.8026 24.4933 28.0928 29.6237 29.6237 24.9945 25.5819 29.6237 24.751 29.6237 29.1828 29.6237 25.7915 29.4562 29.6237 28.8344 28.6654 Red( ) = 0, Red( , Blue) Red( ) > 0, Red( , Blue) Blue( Blue, red) (b) Performance of each Red response against the final Blue mixture µBlue. Figure 16. This figure provides a pure strategy analysis for individual Blue (Figure 16a) and Red (Figure 16b) policies against the final Red and Blue mixture agents within CAGE Challenge 2. Within each sub-figure, the upper sub-plot illustrates the sampling probabilities for the individual policies within the respective mixture µ, while the lower sub-plots depict the empirical rewards achieved by each policy against the respective mixture. We observe that each of the policies with an above zero sampling weighting achieve an identical max payoff against the respective mixtures. This is inline with one of the key properties of a mixed strategy Nash equilibrium, where each of the pure strategies involved in the mix must itself be a best response against the opponent mixture (Fang et al., 2021). 20 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved. An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents 0.0 0.2 0.4 Mixture Blue 0.42 0.31 0.28 cybermonicrepo Cybermonic (PTM=True) 1 Cybermonic (PTM=True) 2 Cybermonic (PTM=True) 3 Cybermonic (PTM=True) 4 Cybermonic (PTM=True) 5 VF-PBRS Cybermonic (PTM=True) 1 VF-PBRS Cybermonic (PTM=True) 2 VF-PBRS Cybermonic (PTM=True) 3 VF-PBRS Cybermonic (PTM=True) 4 VF-PBRS Cybermonic (PTM=True) 5 VF-PBRS Cybermonic (PTM=False) 1 Cybermonic (PTM=False) 1 Blue Policies 500 400 300 200 100 0 Blue Reward -491.193 -415.707 -429.017 -412.095 -413.396 -412.16 -411.921 -428.421 -411.795 -415.154 -416.916 -411.795 -411.795 blue( ) = 0, Blue( , Red) blue( ) > 0, Blue( , Red) blue( Blue, Red) (a) Performance of each Blue response against the final Red mixture µRed. 0.0 0.2 0.4 Mixture Red 0.35 0.44 0.20 AM-MAPPO-G AM-MAPPO (PTM=True) 1 AM-MAPPO (PTM=True) 2 AM-MAPPO (PTM=True) 3 AM-MAPPO (PTM=True) 4 AM-MAPPO (PTM=True) 5 VF-PBRS AM-MAPPO (PTM=True) 1 VF-PBRS AM-MAPPO (PTM=True) 2 VF-PBRS AM-MAPPO (PTM=True) 3 VF-PBRS AM-MAPPO (PTM=True) 4 VF-PBRS AM-MAPPO (PTM=True) 5 VF-PBRS AM-MAPPO (PTM=Fale) 1 AM-MAPPO (PTM=Fale) 1 Red Policies 0 100 200 300 400 Red Reward 373.622 408.675 406.086 408.208 404.424 411.795 404.614 410.694 407.894 411.795 411.795 406.119 406.504 Red( ) = 0, Red( , Blue) Red( ) > 0, Red( , Blue) Blue( Blue, red) (b) Performance of each Red response against the final Blue mixture µBlue. Figure 17. As in Figure 16, this figure provides a pure strategy analysis for individual Blue and Red policies, this time within CAGE Challenge 4. In Figure 17a we observe that the majority of Blue responses are only marginally outperformed by the policies within an above zero weighting in the mixture µBlue. 21 Unpublished Work. Copyright 2025 BAE Systems. All Rights Reserved.