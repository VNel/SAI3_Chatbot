Design-Time Quantification of Integrity in Cyber-Physical-Systems Eric Rothstein Morris, Carlos G. Murguia, Martín Ochoa Singapore University of Technology and Design eric_rothstein,murguia_rendon,martin_ochoa@sutd.edu.sg ABSTRACT In a software system it is possible to quantify the amount of in- formation that is leaked or corrupted by analysing the flows of information present in the source code. In a cyber-physical sys- tem, information flows are not only present at the digital level, but also at a physical level, and to and fro the two levels. In this work, we provide a methodology to formally analyse a Cyber-Physical System composite model (combining physics and control) using an information flow-theoretic approach. We use this approach to quantify the level of vulnerability of a system with respect to at- tackers with different capabilities. We illustrate our approach by means of a water distribution case study. CCS CONCEPTS • Security and privacy →Formal security models; Logic and verification; Information flow control; KEYWORDS Information Flow, Cyber-physical Systems, Control Theory, Non- interference 1 INTRODUCTION A cyber-physical system (CPS) is a system that intertwines com- ponents from the physical and digital worlds. Some examples of CPSs include: cars, aircrafts, water treatment plants, industrial con- trol systems, and critical infrastructures. Security violations in a safety-critical CPS has notable effects in the physical world, e.g., in late 2007 or early 2008, the Stuxnet attack against an Iranian control system allegedly sabotaged centrifuges in uranium enrich- ment plants, causing them to rapidly deteriorate [1, 32], and in 2014, hackers struck a steel mill in Germany and disrupted the control system, which prevented a blast furnace from properly shutting down, causing massive damage to the facility [2, 3]. Although in the scientific literature on information security con- fidentiality has traditionally enjoyed more attention than integrity, as observed for instance by Clark and Wilson [16]: “in the com- mercial environment, preventing disclosure is often important, but preventing unauthorized data modification is usually paramount.” This holds particularly true for many Industrial Control Systems (ICSs) at yet another level: their security priority is not the pro- tection of confidential data, but the protection of their physical assets. Gollmann and Krotofil reinforce this paradigm in [26], stat- ing that the traditional CIA (Confidentiality-Integrity-Availability) triad should be reversed when studying the security of CPSs. They argue that the enforcement of integrity in CPSs should not only consider the “traditional approach for IT systems”, i.e., protecting component logic and communication, but that we must also protect the integrity of observations; more precisely, we have to protect the veracity of sensor data and check its plausibility. Traditionally, the enforcement of CPS integrity has focused on guaranteeing that systems perform as their designers intended. This is usually evaluated using control-theoretic methodologies, specifi- cally fault-detection techniques. Control theory researchers model attacks to CPSs as time-series with specific structures affecting sensor measurements and/or control signals. Depending on their capabilities, attackers have the power to control when, where, and how attacks are induced to the system. Research on attack detec- tion and mitigation has mainly focused on the so-called integrity attacks, i.e., attacks that put at risk the proper operation and physi- cal integrity of CPSs. Integrity attacks include stealthy attacks [19], message replay [36], covert attacks [50], and false-data injection [37], among others. Due to their focus on fault-detection techniques, many results based on control theory aim to protect CPSs from integrity attacks by relying on monitoring the physics of the system to detect anomalies (see, e.g., [5, 13, 35, 38, 44, 52]). In sum, most control-theoretical approaches to CPS security assume the existence of a probabilistic behavioural model that is used as a “white-list” of normal behaviour (attack-free), and their goal is to monitor and protect the integrity of the system with respect to this ideal model at runtime, when attacks might arise. There are some limitations with this approach. On the one hand, most of the work following this approach is reactive, and as such it does not shed light on how to improve a given CPSs design in order to make it more resilient to attacks (although there are some recent results on redesigning controllers and models to improve robustness of CPSs against attacks, e.g., [42, 55]). On the other hand, an inherent limitation of many behavioural models for CPS is that they usually are approximate due to linearisation, and might produce a high false positive rate when used in practice; also, many works (e.g., [35, 52, 56]) rely on the assumption that it is possible to determine whether the system is operating normally or under attack, but there could be non-malicious deviations of a given behavioural model due to routine maintenance operations or other random factors. Moreover, only few approaches (such as [9, 42, 56]) quantify the severity and consequences of attacks on the system. Information flow analysis (IFA) has traditionally focused on de- termining whether sensitive/secret information flows to where it is not intended to. There is a large body of literature on the appli- cation of IFA for the attestation of data confidentiality, including [11, 15, 20], among others. Although it is well known that concep- tually integrity can be seen as the dual of confidentiality, arguably the attestation of integrity has not received much attention from the IFA community. We thus suggest, as an interesting application 1 arXiv:1708.04798v1 [cs.CR] 16 Aug 2017 Preprint scenario of IFA, to develop an alternative and complementary ap- proach to control theory to reason about CPS security based on well-known computer science foundations for integrity. We make the fundamental observation that in practice we often want to pro- tect the integrity of the state of a (physical) process variable (i.e. the level of water of a tank, the temperature of a certain device or material, the chemical concentration of a given medium etc.). We propose then to model a CPS as the composition of a cyber (digital) state-machine modelling the controller logic, and the cor- responding expected physical world reaction. In this setting we can apply information-flow inspired definitions and techniques to formally assess and quantify the effects of an attacker controlling certain aspects of the system (sensors, actuators, or communication channels) on the critical physical variables we intend to protect. Although our approach can shed light on the dependencies be- tween certain inputs and certain critical physical states of the sys- tem, we remark that our analysis assumes some reasonable limi- tations on the considered attackers. In particular, we assume that attackers control only a subset of sensors/actuators and that at- tackers do not subvert the controller’s logic. Clearly, if attackers control all sensors and actuators or are able to replace the control logic (as in the case of Stuxnet), then they can drive a CPS to any desired state (unless orthogonal controls are in place). However, we believe that the analysis proposed in this work can be useful to decide where and if to include redundancy in the number of sensors and/or in the control logic, in order to make attacks more difficult to perform. Given that our approach is intended to verify the security of CPSs at design-time, we make assumptions that might be unre- alistic when considered in an approach that monitors security at runtime (in particular, we assume that we know the state of the controller and of the physical process). Nevertheless, we consider our assumptions to be realistic at design-time, given the existence of a models that describe the dynamical systems (where states are explicitly described), and given the existence of a state machine that models the controller (where the current state is usually known). Problem statement: To summarize, in this work we address the following questions: Can we identify at design time which inputs to a CPS are most harmful if controlled by an attacker? Can we improve the design of the CPS at design time and justify our models formally with respect to a precise integrity notion? Approach: We propose to model a CPS as a composition of the control logic and the expected systems behaviour in terms of a finite and deterministic state machine. We then formally quantify the impact of various attackers (controlling physical or logical aspects of the system) to the integrity of critical physical states by means of information flow analysis. Contributions: We make the following contributions a) We present a logical framework to formally reason about integrity in CPS models, reconciling information flow analysis and control theoretical aspects. b) We discuss how our approach can be used to spot and quantify harmful information flows in CPS. c) We illustrate the usefulness of our approach by means of simple but realistic models concerning a water distribution CPS and show that we can identify non-trivial integrity-harming flows in a CPS. 2 CONTROL THEORY PRELIMINARIES Gollmann and Krotofil highlight in [26] that “to work on cyber- physical systems, one has to combine the devious mind of the security expert with the technical expertise of the control engineer.” Unfortunately, this expertise is “not commonly found in the IT se- curity community,” according to the authors of [25]. Moreover, this lack of expertise is worsened by the language barrier between the disciplines of control theory and IT security. Thus, in this section, we present the model-based techniques for CPSs security broadly used by the systems and control community. 2.1 Linear Time-Invariant Models. During the last decade, there has been an increasing tendency to use physics-based models of CPSs to detect and quantify the effect of attacks on the system performance [13, 35, 38–40, 42, 52]. These physics-based models focus on the normal operation of the CPS and work as prediction models that are used to confirm that control commands and measurements are valid and plausible. Often, dynamical models of physical systems are approximated around their operation points or approximated using input-output data. These lead to approximated models which are often linear and time-invariant. Following the work in [13, 35, 38–40, 42, 52], here, we only consider Linear-Time Invariant (LTI) models, although the same ideas are employed when considering more complicated dynamics. In particular, a model of a CPS that uses LTI stochastic difference equations is of the form ( x(tk+1) = Ax(tk) + Bu(tk) + v(tk), y(tk) = Cx(tk) + η(tk), (1) with k ∈N, sampling time-instant tk, physical state of the sys- tem x ∈Rn (i.e., an n-dimentional vector of physical variables associated with the dynamics of the CPS), sensor measurements y := (y1, . . . ,ym)T ∈Rm , control signals u := (u1, . . . ,um)T ∈Rl , real-valued matrices A, B, and C of appropriate dimensions, and i.i.d. multivariate zero-mean Gaussian noises v ∈Rn and η ∈Rm with covariance matrices R1 ∈Rn×n and R2 ∈Rm×m, respectively. The matrix A describes the dynamic evolution of the physical process, B is used to model the effect of actuators on the system dynamics, and the matrix C models the part of the state xk available from sensor measurements. 2.2 Attacks on Models At the time-instants tk,k ∈N, the output of the process y(tk) is sampled and transmitted over a communication network. The received output is used to compute control actions u(tk) which are sent back to the physical process. The complete control-loop is assumed to be performed instantaneously, i.e., the sampling, transmission, and arrival time-instants are supposed to be equal. In between transmission and reception of sensor data and control commands, an attacker may replace the signals coming from the sensors to the controller and from the controller to the actuators, acting as a Man-in-the-Middle. Thus, after each transmission and 2 Preprint reception, the attacked output ¯y and attacked input ¯u take the form ( ¯y(tk) := y(tk) + δy(tk), ¯u(tk) := u(tk) + δu(tk), (2) where δy(tk) ∈Rm and δu(tk) ∈Rl denote additive sensor and actuator attacks, respectively. Henceforth, we denote x(tk) by xk, ¯u(tk) by ¯uk, v(tk) by vk, ¯y(tk) by ¯yk, η(tk) by ηk, δy(tk) by δy k , and δu(tk) by δu k . Then, a system under attack is modelled by  xk+1 = Axk + B(uk + δu k ) + vk, ¯yk = Cxk + ηk + δy k . (3) 2.3 State Estimation The vast majority of work on attack detection uses fault detection techniques [14, 31] to identify attacks. The main idea behind fault detection theory is the use of an estimator to forecast the evolution of the system. If the difference between what it is measured and the estimation is larger than expected, then there may be a fault/attack on the system. For LTI CPSs of the form (1), to estimate the state of the physical process, people from the systems and control community use mainly two types of estimators: the Kalman filter [8] and the Luenberger observer [33]. Here, we consider Luenberger observers of the form  ˆxk+1 = Aˆxk + B¯uk + L ¯yk −C ˆxk , ˆyk = C ˆxk, (4) with estimated state ˆxk ∈Rn and observer gain matrix L ∈Rn×m which must be designed to ensure that the estimated state ˆxk con- verges to the actual state xk asymptotically, i.e., L is selected such that limk→∞(xk −ˆxk) = 0. Define the estimation error ek := xk −ˆxk and the residual sequence rk ∈Rm as rk := ¯yk −C ˆxk = Cek + ηk + δy k . (5) Given the system dynamics (3) and the observer (4), the estimation error dynamics and the residual sequence are governed by the difference equation: ( ek+1 =  A −LCek + vk −Lηk −Lδy k + Bδu k , rk = Cek + ηk + δy k . (6) At this point, having introduced the residual dynamics (6), the main idea behind residual-based (model-based) attack/fault detection procedures is to characterize the ‘behavior’ of the residual sequence a priori in the attack-free case. Then, we can test (in real-time) whether the actual behavior of the CPS matches the attack-free one. If it is not the case, alarms are raised indicated a possible fault/attack on the system. More precisely, the asymptotic density distribution [47] of the residual sequence is obtained given the system and observer matrices (A, B,C,R1,R1, L); then, statistical change detection procedures (e.g., Cumulative Sum (CUSUM) [27, 43], Generalized Likelihood Ratio (GLR) testing [10], Compound Scalar Testing (CST) [23], etc.) are employed to test whether the residual rk, at every time step k, belongs to this distribution. If it is not the case, alarms are raised. For transparency, we do not go into further details about these ideas. However, for the interested reader, we include some extra material about these techniques in the appendix. Figure 1: Supervisor model 3 QUANTITATIVE INTEGRITY ANALYSIS ON CPS Traditional IT security mechanisms such as authentication and message integrity are insufficient for CPS security. However, as stated in [13], the major distinction of control systems with respect to other IT systems is the interaction with the physical world. In [25], the authors state that CPSs security is specifically concerned with attacks that cause physical impact; since IT security mecha- nisms do not usually account for the physical part of the system, they are thus ineffective against attacks that either target or exploit the physical components of CPSs. In this section, we present an Information Flow (IF) inspired integrity analysis for CPSs which accounts for the physical process. 3.1 A Discrete Model for CPSs Figure 1 presents the supervisor model [45], which serves as the starting point to define an IF framework for CPSs. The supervisor model consists of four systems: a discrete cyber system, i.e., the controller, a continuous-time physical system i.e., the process, and two cyber-and-physical subsystems i.e., the sensor and the actuator. The controller, whose state is q, receives a digital input i and produces a digital control signal o which is then transformed into an analog control signal u by the actuator; in turn, the process, whose state is x, reacts to u and yields an analog observation y, which is transformed into the digital input i′ by the sensor, and the cycle repeats. Ultimately, our objective is to protect the integrity of the physical variable observed by y; i.e., (a part of) the state x of the process. To formally reason about this model, we define a notion of state as follows. Definition 3.1 (States of a CPS). Leti be the vector of digital inputs, o be the vector of digital outputs, u be the vector of analog control signals, q be the current state of the controller, x be the current state of the process, and y be the vector of analog observations. A state of the CPS is a tuple σ = ((i,o,u), (q,x),y) (7) where (i,o,u) is the controllable part of σ, (q,x) is the hidden part of σ and y is the observable part of σ. We assume that the behaviours of the actuator and of the sensor are time-invariant; thus we do not track their state. Let I be the set of all values that i can take. Similarly, we define O,U , Q, X and Y as the sets of all values that o,u, q x andy can take, respectively. We define the set of all states Σ = I ×O ×U ×Q ×X ×Y. 3 Preprint Figure 2: Water Tank Model. We denote valves by ▷◁and sen- sors by ⊗. Example 3.2 (A Water Tank). Consider the water tank with one input valve and one output valve shown in Figure 2. The tank has a capacity of L litres of water, and a water level sensor measures the current level of water in the tank. A very simple LTI model of the process is xk+1 = xk + 5uin k −3uout k , (8) yk = xk, (9) subject to 0 ≤xk ≤L. If the in valve is open, then 5 litres of water flow into the tank, and if the out valve is open, then 3 litres of water flow out the tank. For simplicity, there is no noise, and the only part of the state that we are interested in is the water level (not temperature or pressure, for example). The controller of the tank has a single state ∗, and it receives a single input i, i.e., the level of the tank in digital format, and issues an output of the form (oin,oout ) where oin and oout are each either open or close, which respectively represent the actions of opening and closing the in and out valves. Opening valve v at time k forces uv k to be 1 and closing valve v at time k forces uv k to be 0. A state for this system at time k could be ((0, (open,close), (1, 0)), (∗, 3), 3). We model the behaviour of the sensor, the actuator, the controller and the process as a set of black-box functions, which we compose in order to define the dynamics of the CPS. Definition 3.3 (Single-Cycle Semantics). Let Σ = I × O × U × Q × X × Y be the set of states of the CPS. Now, consider the following functions: • δIO : Q × I →Q, the transition function of the controller, • γIO : Q × I →O, the output function of the controller, • γAD : Y →I, the output function of the sensor, • γDA : O →U , the output function of the actuator, • γΦ : X × U →Y, the output function of the process, • δΦ : X × U →X, the transition function of the process. We define the single-cycle semantics function J·K: Σ →Σ that describes the evolution of the state ((i,o,u), (q,x),y) of the CPS as follows J(i,o,u), (q,x),y)K =  (i′,o′,u′), (q′,x ′),y′ (10) where i′ = γAD(y), o′ = γIO(q,i), u′ = γDA(o) (11) q′ = δIO(q,i), x ′ = δΦ(x,u) (12) y′ = γϕ(x,u). (13) The evolution over k + 1 cycles is defined by iterating J·K, i.e., J·Kk+1 = J·K ◦J·Kk . (14) 3.2 Integrity and Attacker Model We have yet to consider the presence of an attacker. There are three non-excluding cases: 1) if an attacker controls a sensor, then the process IO receives a corrupted version of i, 2) if an attacker physically controls an actuator, then Φ receives a corrupted version of u, and 3) if an attacker acts as a Man-in-the-Middle between the controller and the actuator, then the actuator receives a corrupted version of o. To formally reason on the impact of such attacks, we characterise integrity and an attacker model. Attackers that control all sensor and/or control signals, or that can change the logic of controller can drive the process to any state they desire; these attackers are too powerful to defend against. However, it may happen that an attacker that only controls a subset of sensors and actuators has enough power to drive the process to any state they want, and defending against these attackers requires the implementation of orthogonal controls and/or reimplementa- tion of the controller’s logic, and is thus desirable to detect these vulnerabilities at the design phase of the CPS. Clarkson and Schneider state in [18] that they know of no widely accepted definition of integrity, but they remark that an “informal definition seems to be the ‘prevention unauthorized modification of information’." Clarkson and Schneider use two formal notions to characterise and quantify corruption, i.e., the damage to integrity. One notion is contamination, which is a generalisation of taint anal- ysis that tracks information flows from untrusted inputs to outputs that are supposed to be trusted. The other notion is suppression, which occurs when implementation outputs omit information about correct outputs (with respect to the specification). Contamination is the dual to information leakage under the Biba duality; however, there is no apparent Biba dual to suppression [12]. In this work, the intuition behind corruption is closer to that of contamination, and we assume that the system presents no suppression; i.e., the CPS and its components are correctly designed. Goguen and Meseguer introduced the concept of noninterference [24], which provides a formal foundation for the specification and analysis of security policies and the mechanisms to enforce them. In the case of integrity, noninterference for programs means that a variation of public inputs does not cause a variation of critical val- ues. Noninterference is traditionally used to characterise unwanted flows from a “high/secret” security level H to a “low/public” secu- rity level L, as they violate the confidentiality of data; however, given the Biba duality, in the case of integrity we say that the inputs of the attacker belong to L, that critical values belong to H, and that our objective is to prevent unwanted flows from L to H, as they violate the integrity of data. Noninterference-based confiden- tiality notions have been formally defined over state-based models (see [48]), trace-based models (see [17]), and semantics-based mod- els (see [49]) to name a few. In the following, we define a notion of (noninterference-based) integrity for CPS using a semantics-based model. Definition 3.4 (Attack). Let Σ = I ×O ×U ×Q ×X ×Y be the set of states of the CPS. An attack is a function f : I ×O ×U →I ×O ×U that changes the controllable components of a given state σ of the CPS. The extension of an attack on a single state can be naturally extended to an attack over a trace τ = (σ0,σ1, ...) ∈Σ∗, where the 4 Preprint attacker can apply arbitrary functions fj on each state σi of the trace. This definition of attacks is inspired by Equation (2), where attackers change the value of control signals and measurements. We remark that these attack functions do not directly change the current state of the controller nor the state of the process; in- stead, they attempt to model the interference caused by the at- tacker at either the cyber or physical level. For instance, if the CPS is in a state ((i,o,u), (q,x),y), then an attack on a sensor yields a state ((i′,o,u), (q,x),y), and an attack on the network layer that communicates the controller with an actuator yields a state ((i,o′,u), (q,x),y). This interference must propagate in order to change the state of the controller or the state of the process, which is why we believe performing an information flow analysis is both sensible and adequate in this case. Example 3.5 (Attacking the Water Tank). Consider the water tank from Example 3.2. An example attack on the water level sensor is characterised by the function that maps the controllable tuple (i,o,u) to the tuple (L,o,u); in this case, the attacker is going to fool the controller into thinking that the water tank is full. Definition 3.6 (Attacker). Every attacker α controls a set of con- trollable components, i.e., a set of digital inputs, digital outputs, and/or analog control signals. We denote the set of the components that α controls by Cα . The attacker α can perform an attack se- quence f1, f2, . . . fk over k cycles that changes the semantics of Equation (14) to J·Kk+1 = J·K ◦fk ◦J·Kk . (15) Example 3.7 (An Attacker of the Water Tank). Consider the water tank from Example 3.2. An example attacker α that only controls uin models an attacker that can physically open or close the in valve at will, and the only means for this attacker to attack the system is through the physical manipulation of such valve. The objective of an attacker is to control one or more of the process variables of the CPS; e.g., an attacker wants to control how much water is in a tank so that she can overflow it. These physical variables are represented in our CPS model by the output of the pro- cess, y, which is a vector of their analogue observations. Ultimately, our notion of security will depend on the vectory. We now formally define security based on classical notions of noninterference. Definition 3.8 (k-Process Integrity for CPSs). Letα be an attacker.We label each component controlled by α, i.e., c ∈Cα as L, and we label all other components of the state as H. We use the expression lvl(c) to denote the security level of the component c. Now, let σ and σ ′ be states of the CPS. We say that σ and σ ′ are H-equivalent iff they are equal in all H-labeled components; formally, we say that σ =H σ ′ iff ∀c ∈C : lvl(c) = H ⇒σ(c) = σ ′(c). (16) We say that σ and σ ′ are process-equivalent, denoted σ ≈Φ σ ′ iff their process variables are equal; more precisely, if σ = ((i,o,u), (q,x),y), and (17) σ ′ = ((i′,o′,u′), (q′,x ′),y′), (18) we say that σ ≈Φ σ ′ iff y = y′. Finally, for k ∈N with k > 0, we say that the CPS satisfies k- process integrity against the attacker α if and only if, for all states σ and σ ′, we have σ =H σ ′ ⇒JσKk ≈Φ Jσ ′Kk . (19) The previous definition is qualitative (either the system satisfies process integrity or it does not). However, it does not shed light on the level of damage that this integrity violation may imply. As with many security notions based on noninterference, an attacker might slightly deviate the behaviour of the process without any serious repercussions; thus, we are interested in a quantitative measure of security in order to precisely estimate the level of control that the attacker has over the process. Definition 3.9 (k-Controllability of an Attacker). Let α be an at- tacker, let S be the set of states of the CPS, and let σ0 be the initial state of the CPS. We inductively define the set of states Σkα , for k ∈N, by Σ0 α = { σ0 } (20) Σk+1 α = n JσK σ ∈Σ,σ ′ ∈Σk α : σ =H σ ′ o . (21) In other words, for each element σ ′ ∈Σkα , we find every H- equivalent state σ, and we include the resulting state JσK in Σk+1 α . Thus, the set Σk+1 α contains all reachable states after the kth cycle accounting for all attacks that α could use against the CPS during these k cycles (according to the interaction between the attacker and the system shown in Equation 15). Now, consider the function πY : Σ →Y that projects the state ((i,o,u), (q,x),y) to y; we use the set πY (Σk) = n πY (σ) σ ∈Σk o (22) to determine how many different values for y the attacker α can force the system to have after k cycles. The k-controllability of α defined by the number of elements in πY (Σk), i.e., |πY (Σk)|, quan- tifies the degree of control that the attacker α has over the process after a period of k cycles. Note that this definition, being quantitative, allows us to compare the impact of different attackers on a CPS over well-defined control logics. Note also that however, whether this level of controllability is enough to drive the CPS to a critical state requires an additional analysis as we will discuss in the following section. Nevertheless, as in the case of traditional leakage, we observe that higher levels of controllability (that can be thought of as a dual of leakage, or corruption), are in general more dangerous, since they imply a higher degree of separation from the intended “legitimate” state. We remark that we can show that a system does not satisfy pro- cess integrity against an attackerα if, for somek, thek-controllability of α is greater than 1 , as shown in the following Corollary. Corollary 3.10 (From Controllability to Integrity). Let α be an attacker. For k ∈N, if |πY (Σkα )| > 1, then the system does not satisfy k-process integrity against α. Proof. If |πY (Σkα )| > 1, then there exist σ ∈Σ and σ ′ ∈Σk−1 α with σ =H σ ′ such that JσK 0Φ Jσ ′K. According to Equation (19), this would imply that the CPS does not satisfy 1-process integrity against α. □ 5 Preprint We are ultimately interested in using the measure |πY (Σkα )| to reduce the controllability that the attacker has over the process by changing the logic of the controller. We will discuss this in Section 4.3 by means of a simple but not trivial case study. 3.3 Composition of CPSs Cyber-Physical Systems are often distributed systems that compose several control units supervising certain aspects of the process and communicating with each other. Thus, it might be possible for an attacker to attack a physical component in a module of a CPS by in- terfering with a digital component of a different module. Intuitively, this requires the existence of channels that enable the propagation of the interference caused by the attacker from module to module. These channels can be either digital or physical: when two PLCs communicate, their message passing opens a digital channel, but they might also supervise or control a physical “channel” (such as the same water pipe at different points). Consider the composition of two modules shown in Figure 3; we assume that an attacker exists somewhere on the left module, and her objective is to affect the process variables of the right one. For that purpose, she will try to propagate her interference through the channels i∗(digital) and u∗(analogue). Figure 3: Composition framework To analyse the module on the right of Figure 3, we can extend its state from ((i,o,u), (q,x),y) to (((i,i∗),o, (u,u∗))(q,x),y); i.e., we extend the vectors i and u to now consider the new channels. Con- sequently, we need not compose the two modules in their entirety to perform an analysis, but only provide additional semantics for i∗and u∗(which could be given by state machines); this opens the door to a modular analysis of networks of CPSs, which can pro- vide compositional results under some assumption/commitment restrictions. 4 CASE STUDY: A WATER DISTRIBUTION CPS We now study a simple but illustrative example on how to provide useful design insights by means of our formal approach. The following case study is based on an example taken from [28]. Consider the system shown in Figure 4 which consists of two water tanks, T1 and T2, both with maximum capacity L. Each tank has a constant outflow of v1 and v2, respectively. Water is added to the system at a constant rate w through a hose which, at any point in time is dedicated to either T1 or T2. We assume that the hose can switch between the tanks instantaneously, and we also assume that w ≥v1 + v2, so it is also possible to close Figure 4: Case study water distribution system. the hose to prevent overflowing either tank. The objective of the controller is to constantly keep the water volume xt above some time-invariant critical threshold rt , for t = 1, 2. We assume that the water volumes are both initially above their respective thresholds. For this purpose, the controller is designed to switch the inflow to T1 whenever x1 < r1 and to T2 whenever x2 < r2. The process has three modes: mode q1 where T1 is filling, mode q2 where T2 is filling, and mode q0 where the hose is closed. The process evolves as follows when in mode q1 ( x1(k + 1) = x1(k) + w −v1, x2(k + 1) = x2(k) −v2, (23) as follows when in mode q2 ( x1(k + 1) = x1(k) −v1, x2(k + 1) = x2(k) + w −v2, (24) and as follows when in mode q0 ( x1(k + 1) = x1(k) −v1, x2(k + 1) = x2(k) −v2. (25) At time k, the process outputs the vector of analog measurements (y1(k),y2(k)), defined by (y1(k),y2(k)) = (x1(k),x2(k)); (26) in this case, we see that the state is fully observable (though this might not always be the case). The sensor, at time k, receives the vector of measurements (y1(k),y2(k)) and outputs the vector of digital inputs (i1(k),i2(k)), defined by (i1(k),i2(k)) = (y1(k),y2(k)); (27) Although it may seem redundant, we explicitly define the semantics of the sensor for the sake of clarity. In other scenarios, the sensor may have more complicated semantics; e.g., it could encrypt or sign the data before sending it to the controller. The controller, which has a unique state ∗, receives the vector of inputs (i1,i2) and outputs a command o, defined by if i1 < r1 then o := q1, (28) elseif i2 < r2 then o := q2, (29) else o := q0. (30) Then, at time k, the actuator translates the command o(k) into the analog control signal u(k) defined by u(k) = o(k). (31) The physical process works as a state machine that reacts to the control signal u(k) and switches on it to decide how to update the 6 Preprint state; i.e., if u(k) = qj, then update the state using the equations of mode qj, for j = 0, 1, 2. This can be clearly represented as a hybrid automaton (see [28, Figure 1.17]). We now define some attackers and analyse their impact over this CPS. 4.1 Attackers We want to determine whether the an attacker can violate the integrity of the process (according to Definition 3.8), and if so, quantify the potential impact. This impact will be a natural number, that will allow us to compare various attackers and assess differ- ent controllers in terms of security. In order to complement this quantitative analysis, we will also consider consider four classes of critical states: for t = 1, 2, when tank Tt is empty (classes E1 and E2), and when tank i is full (classes F1 and F2). According to Definition 3.1, states of this CPS have the structure ((i1,i2,o,u), (∗, (x1,x2)), (y1,y2)). For t = 1, 2, we say that the state σ is a critical state of class Et at time k (i.e., a state where tank i is empty) iff yt (k) = 0; similarly, we say that σ is a critical state of class Ft at time k iff yt (k) = L. The system does not reach any critical state under normal circumstances since the controller is designed to stabilise the values of y1 and y2 around the thresholds r1 and r2, respectively. Thus, for t = 1, 2 the normal range of values for yt is [r− t ,r+ t ], with r− t ≤rt ≤r+ t , and we will assume for the initial value yt (0) that rt ≤yt (0) ≤r+ t . Note that in general our assumption is that the controller under analysis has been already verified to comply with their functional and safety requirements under normal circumstances (no attackers present). Let us consider three different attackers: Attacker α1, who controls the digital control signal o(k), for all k. Attacker α2, who controls the digital measurement i2(k), for all k. Attacker α3, who controls the digital measurement i1(k), for all k. Note that since we want to protect the physical state of the system (level of the tanks), we do not consider attackers that can directly tamper with this state, since they will trivially achieve their goal. 4.2 Quantification and analysis Using the formal semantics and security properties described above, it is now possible to carry out a mathematical analysis on the impact that the three attackers considered can have on the system. Table 1 summarises the results of this analysis. Attacker α1 is very powerful since she can drive the system to any class of critical state she wants. Attackers α2 and α3 are very similar to each other in terms of capabilities, but α3 is more powerful than α2, since α3 can take the system to three out of the four classes of critical states, while α2 can only take the system to two classes of critical states. In the following we sketch manual proofs of these results, and we start to build arguments for the discussion we carry out in Section 5. Proof outline: We assume that the initial state is σ(0) = ((r+ 1 ,r+ 2 ),q0,q0), (∗, (r+ 1 ,r+ 2 )), (r+ 1 ,r+ 2 )), (32) i.e., a state where the water level of the tank Tt is r+ t , for t = 1, 2, and the hose is closed. Attacker α1 has control over the digital output o, and she can choose to replace o(k) with an element of {q1,q2,q0 } for all k. In Attacker Controls Quantification Vulnerable? y1 y2 E1 E2 F1 F2 α1 o [0, L] [0, L] ✓ ✓ ✓ ✓ α2 i2 [r − 1 , r + 1 ] [0, L] ✗ ✓ ✗ ✓ α3 i1 [0, L] [0, r + 2 ] ✓ ✓ ✓ ✗ Table 1: Quantification of k-controllability of the attackers for a k that is large enough. We quantify by using ranges to over-approximate the sets Σkα for the different attackers. one cycle, we have Σ1 α1 = n J((r+ 1 ,r+ 2 ),q0,q0), (∗, (r+ 1 ,r+ 2 )), (r+ 1 ,r+ 2 ))K, J((r+ 1 ,r+ 2 ),q1,q0), (∗, (r+ 1 ,r+ 2 )), (r+ 1 ,r+ 2 ))K, J((r+ 1 ,r+ 2 ),q2,q0), (∗, (r+ 1 ,r+ 2 )), (r+ 1 ,r+ 2 ))K o = n ((r+ 1 ,r+ 2 ),q0,q0), (∗, (r+ 1 −v1,r+ 2 −v2)), (r+ 1 −v1,r+ 2 −v1)), ((r+ 1 ,r+ 2 ),q0,q1), (∗, (r+ 1 −v1,r+ 2 −v2)), (r+ 1 −v1,r+ 2 −v1)), ((r+ 1 ,r+ 2 ),q0,q2), (∗, (r+ 1 −v1,r+ 2 −v2)), (r+ 1 −v1,r+ 2 −v1)) o . (We underline the differences.) Since πy1(Σ1α1) = { r+ 1 −v1 } and πy2(Σ1α1) = { r+ 2 −v2 }, we see that the attack issued by α1 has not had enough time to propagate to the process. However, after considering one more cycle, we see that πy1(Σ2 α1) = { r+ 1 −2v1,r+ 1 −2v1 + w } , (33) πy2(Σ2 α1) = { r+ 2 −2v2,r+ 2 −2v2 + w } . (34) We see two different values for y1 because the control signals q0 and q2 prevent the tank T1 from filling (thus making the level of water r+ 1 −2v1) while the control signal q1 causes the hose to fill T1 (making the level of water r+ 1 −v1 + w). As we extend our analysis over several cycles (a large enough k), we see that we obtain linear combinations of vt and w, for t = 1, 2, as follows πy1(Σk α1) = {y | y = r+ 1 −βv1 + ϵw, 0 ≤y ≤L, β,ϵ ∈N } , (35) πy2(Σk α1) = {y | y = r+ 2 −βv2 + ϵw, 0 ≤y ≤L, β,ϵ ∈N } . (36) For t = 1, 2, the set πyt (Σkα1) includes 0 and L, because there will be linear combinations r+ t −βvt +ϵw that would be less than 0 and greater than L; however, the level of water in the tank Tt cannot go below 0 and it cannot go above L. For simplicity of exposition, we over-estimate these sets using the interval [0, L] (see Table 1). Attacker α2 has control over the digital input i2, and she could choose to replacei2(k) with an element from the range [0, L] for allk. However, we see that many choices for i2 trigger similar behaviours inside the controller. Since there is only one branching depending on whether i2 < r2, we take 0 and L as the two representative values for i2 that α2 could use to trigger different behaviours. Thus 7 Preprint in one cycle, we have Σ1 α2 = n J((r+ 1 ,r+ 2 ),q0,q0), (∗, (r+ 1 ,r+ 2 )), (r+ 1 ,r+ 2 ))K, J((r+ 1 , 0),q0,q0), (∗, (r+ 1 ,r+ 2 )), (r+ 1 ,r+ 2 ))K, J((r+ 1 , L),q0,q0), (∗, (r+ 1 ,r+ 2 )), (r+ 1 ,r+ 2 ))K = n ((r+ 1 ,r+ 2 ),q0,q0), (∗, (r+ 1 −v1,r+ 2 −v2)), (r+ 1 −v1,r+ 2 −v1)), ((r+ 1 ,r+ 2 ),q2,q0), (∗, (r+ 1 −v1,r+ 2 −v2)), (r+ 1 −v1,r+ 2 −v1)), ((r+ 1 ,r+ 2 ),q0,q0), (∗, (r+ 1 −v1,r+ 2 −v2)), (r+ 1 −v1,r+ 2 −v1)) o . At k = 3, there is finally an impact on the value of y2, and we have πy1(Σ3 α2) = { r+ 1 −3v1 } , (37) πy2(Σ3 α2) = { r+ 2 −3v2,r+ 2 −3v2 + w } . (38) In this case, the attacker α2 has the same degree of control over y2 that α1 had: α2 can cause the water level of T2 to rise or decrease at will (although with slightly more delay). However, there is no flow of information from i2 to y1, so we the values for y1 correspond to the values that we we see during the normal execution of the system over k cycles, i.e., values between r− 1 and r+ 1 . Attacker α3 has control over the digital input i1, and she could choose to replace i1(k) with an element from the range [0, L] for all k. The influence of α3 overy1 is clear, since it is dual to the influence that the attacker α2 had over y2, but we also see an indirect flow from i1 to y2 due to the way the controller is programmed. More precisely, whenever i1 < r1, the controller never outputs q2, so it is possible for the attacker to continuously inhibit the mode q2 by, for example, making i1 = 0. Consequently, the value r+ 2 −kv2 would be an element of πy2(Σkα3). Since the value r+ 2 −kv2 cannot be less than 0, whenever r+ 2 ≤kv2, we conclude that 0 is an element of πy2(Σkα3) and that α3 can empty T2. However, no value of i1 forces the controller to output q2, so α3 cannot arbitrarily increase the value of y2 beyond its operational upper bound r+ 2 . 4.3 Redesigning the Controller In this section, we show how we can reduce the controllability of the attacker α3 by adding a fairness mechanism to the controller. Instead of a single mode ∗, we now say that the controller has two. Let m be a variable that denotes the current mode of the controller, whose value is either m1 or m2. The new controller receives the vector of inputs (i1,i2) and outputs a command o depending on its current mode. If the mode is m1 then if i1 < r1 ∧i2 < r2 then o := q1,m := m2, (39) elseif i1 < r1then o := q1, (40) elseif i2 < r2 then o := q2, (41) else o := q0, (42) and if the mode is m2 then if i1 < r1 ∧i2 < r2 then o := q2,m := m1, (43) elseif i1 < r1then o := q1, (44) elseif i2 < r2 then o := q2, (45) else o := q0. (46) If the new controller sees that both tanks are below the thresholds r1 and r2, it will not always prioritise the filling ofT1 as it did before. We informally justify that the controllability of α3 over the variable y2 changed, as it is now not possible for the attacker to empty T2. The tank T2 does not run out of water, because the controller will fill T2 when it sees that the water level of T2 is below r2 even if the attacker always tricks the controller into thinking that T1 is empty, a behaviour that was not present before. However, if the water level of tank T2 is at its minimum normal value, r− 2 , it could happen that the controller’s mode is m1 at that time, and it will take an extra cycle to switch from mode m1 to m2. During that extra cycle, the tank T2 loses v2 units of water, so the attacker’s control over y2 is lower bounded by r− 2 −v2, as shown in Table 2. Consequently, the attacker α3 cannot make the system reach a state where the tank T2 is empty. Attacker Controls Quantification Vulnerable? y1 y2 E1 E2 F1 F2 α3 i1 [0, L] [r − 2 −v2, r + 2 ] ✓ ✗ ✓ ✗ Table 2: Quantification of the k-controllability of attacker α3 with the new controller in place. 5 DISCUSSION AND FUTURE WORK In this section, we list and discuss some fundamental discussion points, some of which represent the basis for interesting future work. 5.1 Modelling and Abstraction level Although we provide a formal framework for the modelling of CPSs, their states, and their execution semantics, and showed evidence that it can be used to derive useful analysis, it will be a subject of future work to assess how appropriate our modelling is when applied to other, perhaps more complex, scenarios. In essence, our formalism defines a deterministic transition system whose transi- tion function is given by the one-step cycle semantics function J·K (see Definition 3.3). Consequently, the notion of integrity that we use in Definition 3.8 is not probabilistic. However, we believe that this approach is good enough to carry out an approximate analysis if the probabilistic nature of the process is due to the existence of zero-mean noises. Hybrid automata [7] are state-based models for dynamical sys- tems. These automata allow the description of invariants and side- effects on transitions, making them suitable for the description of many continuous physical processes. There are also model check- ing techniques for hybrid automata. It would be interesting to see if these automata provide any advantages when it comes to automatic verification of the proposed properties. 5.2 Quantification of Interference For our case study, we came up with a notion of quantification of integrity violations which, ultimately, was related to how many unwanted states could an attacker force the system to be in, in other words the level of corruption an attacker can induce. These metrics are in a sense dual to metrics used in quantitative information flow, which measure the entropy of information, and how likely it is 8 Preprint for an attacker to guess a secret based on the information she has already observed. Ideally, we want to quantify the k-controllability of attackers for large values of k in order to cover as many possible states. Unfor- tunately, the larger k is, the more computationally demanding the analysis becomes. However, we want to highlight that the behaviour of CPSs is usually regular; i.e., the operation modes of CPSs often follow the same sequence over and over, revisiting similar states each time the process repeats. Due to this regular nature, it may be possible to restrict the analysis to the length of this “behavioural loop” instead of analysing arbitrarily long traces, therefore consid- ering a well defined and relatively small ¯k ∈N. 5.3 A Theory of Attacks and Attackers From our case study, we observe that it is possible for different attackers to drive the CPS to critical states by using different attack strategies. Thus, it may be convenient to define more abstract no- tions of attack and attacker that are more related to the effects that we want to avoid on the system in order to bundle together attacks and attackers that are equally powerful. While we think that our attacker model is at least as powerful as attacker models commonly used in control theory (see Section 2.2), further work is indeed to formally compare them. 5.4 Properties There are many well-known properties in control theory that char- acterise important aspects of dynamical systems, including control- lability, observability, stability and stabilizability. Since we focus on the protection of the integrity of the process, we want to reduce the degree of controllability that the attacker has over the system. A focus on the protection of the confidentiality of data would aim to reduce the degree of observability that the attacker has over the system. Krotofil and Larsen acknowledge the importance of rea- soning about controllability and observability, but they highlight that it is also important to reason about operability, which is “the ability to achieve acceptable operations” [30]. Determining how our notion of process integrity relates to these other properties is an interesting research topic. 5.5 Proof Automation From the manual analysis of the case study CPS in the previous seciton, we foresee that it will be possible to use existing tech- niques (such as automated theorem proving, model-checking and SMT solving) to automate parts of the verification. The definition of the measures for quantification, the notions of critical state, the attackers and the initial states must naturally be done manually, but the problem of verification can be reduced to the quantification of the reachable states by an attacker (similar as in quantitative infor- mation flow analysis for confidentiality). It would be also interesting to determine whether our approach together with state-of-the-art verification tools has advantages over existing control-theoretical reasoning techniques, which focus on solving algebraic representa- tions of CPSs. 5.6 Redesign In our case study, we determined that the attacker α3 was more powerful than the attacker α2 due to the way the controller was designed. Thus, we believe that it would be interesting to develop a methodology for (re)designing controllers that helps avoiding these type of unsafe behaviours. 6 RELATED WORK There is a wealth of work related to the modelling and verification of Cyber-Physical Systems, mostly with focus on traditional safety properties: both in the formal sense (properties of a single trace) and the informal sense (resilience against random faults), see for instance [6] for a good survey on this field. In the following thus we focus mostly on formal models of CPSs security. The applicability of Information Flow Analysis (IFA) to CPS security has been reinforced by several authors, although many of the works do not provide definitive results, and other authors focus on protecting confidentiality and not integrity. In [26], the Gollmann and Krotofil state that “Physical relation- ships between the variables in an industrial process, e.g. between volume, pressure, and temperature in a vessel, can be viewed as information flows from a modelling perspective,” acknowledging that it is possible to model physical aspects of CPS using an IF setting; however, they do not explicitly say how. Gamage et al. [22] use IFA to illustrate how to prevent attacks on confidentiality of CPS though the notion of compensating pair (a,ac), where ac is an action that cancels the physical manifestation of the earlier occurring action a so that attackers do not infer that a took place. Other authors have proposed attacker models for CPSs. For ex- ample, Howser and McMillin provide in [29] an IF-based attacker model that builds on top of nondeducibility [51], where attackers aim to hide information relevant to attacks or faults to the monitor- ing systems, preventing operators from realising that the behaviour of the system is anomalous. Given that we our ultimate goal is to make CPSs more resilient by design, we consider all possible behaviours that attackers could trigger, so our analysis is not aimed at deciding whether the attacker hides information from the opera- tor. Rocchetto and Tippenhauer [46] extend the Dolev-Yao attacker model [21] to the Cyber-Physical Dolev-Yao (CPDY) model, where attackers can interact with the physical domain through orthog- onal channels. We believe that their attackers can be modelled in our framework with attackers that control the components of the control vector u, but a formal justification is still missing. From the perspective of control theory, the work by Weerakkody et al. [56] proposes the Kullback-Leibler divergence between the distributions of the attacked and attack-free residuals as a measure for informa- tion flow to determine to which extent the actions of an attacker interfere with the system. However, their definition of security is ultimately tied to a maximum deviation from a prediction model, while ours is based on semantics-based information flow. Thus, although we can imagine that both measures are somehow related, it is not evident what their exact relationship is. In the work by Murguia et al. [41], the authors characterise the states of the system that can be reached when under attack, and they try to minimise this set of reachable states by modifying the mathematical model of the controller. Their approach to characterise the security of 9 Preprint the system is purely control-theoretical, while ours is based on information-flow analysis. Finally, verification tools like McLaughlin et al.’s [34] that use symbolic execution for model checking safety properties in PLCs could help our approach by helping to quantify the interference of attackers. In sum, to the best of our knowledge, we are the first to propose the use of a semantics-based approach, inspired by traditional information-flow analysis techniques used for soft- ware security, to quantify the impact of attacker actions on process variables at design time in CPSs. 7 CONCLUSIONS In this work we proposed a formal technique to reason about the security of Cyber-Physical Systems based on information-flow anal- ysis and focused on integrity properties. Our basic observation is that in CPS security often the worst case scenario is related with the integrity of some physical state (i.e. level of water or chemical concentration in tank, temperature etc.) and that an attacker’s goal is to reach that state by manipulating certain digital or physical inputs to a system (by tampering with one or more sensor or ac- tuators). As such, we can cast this problem as an information flow problem and leverage on well-known principles to perform this analysis. We have illustrated our approach by means of a realistic case study and showed that we can identify and quantify non-trivial harmful flows. In the future we plan to perform this reasoning in a semi-automatic fashion by leveraging on model-checking technology, and we plan to apply our methodology to a wider range of CPS from various domains (such as electricity, water treatment). ACKNOWLEDGMENTS We would like to thank Bruce McMillin for his insightful comments and suggestions to earlier versions of this draft. REFERENCES [1] 2011. The Man Who Found Stuxnet – Sergey Ulasen in the Spotlight. https://eugene.kaspersky.com/2011/11/02/ the-man-who-found-stuxnet-sergey-ulasen-in-the-spotlight/. (2011). Accessed: 2017-01-09. [2] 2014. Bericht zur Lage der IT-Sicherheit in Deutschland 2014. https://www.bsi.bund.de/SharedDocs/Downloads/DE/BSI/Publikationen/ \Lageberichte/Lagebericht2014.pdf. (2014). In German, Accessed: 2017-01-09. [3] 2015. A Cyberattack Has Caused Confirmed Physical Dam- age for the Second Time Ever. https://www.wired.com/2015/01/ german-steel-mill-hack-destruction/. (2015). Accessed: 2017-01-09. [4] B.M. Adams, W.H. Woodall, and C.A. Lowry. 1992. The use (and misuse) of false alarm probabilities in control chart design. Frontiers in Statistical Quality Control 4 (1992), 155–168. [5] Sridhar Adepu and Aditya Mathur. 2016. Using Process Invariants to Detect Cyber Attacks on a Water Treatment System. Springer International Publishing, Cham, 91–104. https://doi.org/10.1007/978-3-319-33630-5_7 [6] Rajeev Alur. 2015. Principles of cyber-physical systems. MIT Press. [7] R. Alur, C. Courcoubetis, N. Halbwachs, T.A. Henzinger, P.-H. Ho, X. Nicollin, A. Olivero, J. Sifakis, and S. Yovine. 1995. The algorithmic analysis of hybrid systems. Theoretical Computer Science 138, 1 (1995), 3 – 34. https://doi.org/10. 1016/0304-3975(94)00202-T Hybrid Systems. [8] Karl J. Aström and Björn Wittenmark. 1997. Computer-controlled Systems (3rd Ed.). Prentice-Hall, Inc., Upper Saddle River, NJ, USA. [9] C. Z. Bai, F. Pasqualetti, and V. Gupta. 2015. Security in stochastic control systems: Fundamental limitations and performance bounds. In American Control Conference (ACC), 2015. 195–200. [10] M. Basseville. 1988. Detecting changes in signals and systems - a survey. Auto- matica 24 (1988), 309 – 326. [11] D.E. Bell and L.J.L. Padula. 1973. Secure Computer Systems: Mathematical Foun- dations and Model. Number v. 1. Mitre Corp. [12] Kenneth J Biba. 1977. Integrity considerations for secure computer systems. Tech- nical Report MTR-3153. MITRE Corporation. [13] Alvaro A. Cárdenas, Saurabh Amin, Zong-Syun Lin, Yu-Lun Huang, Chi-Yen Huang, and Shankar Sastry. 2011. Attacks Against Process Control Systems: Risk Assessment, Detection, and Response. In Proceedings of the 6th ACM Symposium on Information, Computer and Communications Security (ASIACCS ’11). ACM, New York, NY, USA, 355–366. https://doi.org/10.1145/1966913.1966959 [14] Jie Chen and Ron J. Patton. 1999. Robust Model-based Fault Diagnosis for Dynamic Systems. Kluwer Academic Publishers, Norwell, MA, USA. [15] David Clark, Sebastian Hunt, and Pasquale Malacaria. 2002. Quantitative Analysis of the Leakage of Confidential Data. Electronic Notes in Theoretical Computer Science 59, 3 (2002). [16] D. D. Clark and D. R. Wilson. 1987. A Comparison of Commercial and Military Computer Security Policies. In 1987 IEEE Symposium on Security and Privacy. 184–194. https://doi.org/10.1109/SP.1987.10001 [17] Michael R. Clarkson and Fred B. Schneider. 2010. Hyperproperties. Journal of Computer Security 18, 6 (2010), 1157–1210. https://doi.org/10.3233/JCS-2009-0393 [18] M. R. Clarkson and F. B. Schneider. 2010. Quantification of Integrity. In 2010 23rd IEEE Computer Security Foundations Symposium. 28–43. https://doi.org/10.1109/ CSF.2010.10 [19] G. Dan and H. Sandberg. 2010. Stealth Attacks and Protection Schemes for State Estimators in Power Systems. In 2010 First IEEE International Conference on Smart Grid Communications. 214–219. https://doi.org/10.1109/SMARTGRID. 2010.5622046 [20] Dorothy Elizabeth Robling Denning. 1975. Secure Information Flow in Computer Systems. Ph.D. Dissertation. West Lafayette, IN, USA. AAI7600514. [21] D. Dolev and A. Yao. 1983. On the security of public key protocols. IEEE Transactions on Information Theory 29, 2 (Mar 1983), 198–208. https://doi.org/10. 1109/TIT.1983.1056650 [22] T. T. Gamage, B. M. McMillin, and T. P. Roth. 2010. Enforcing Information Flow Security Properties in Cyber-Physical Systems: A Generalized Framework Based on Compensation. In 2010 IEEE 34th Annual Computer Software and Applications Conference Workshops. 158–163. https://doi.org/10.1109/COMPSACW.2010.36 [23] J. Gertler. 1988. Survey of model-based failure detection and isolation in complex plants. Control Systems Magazine, IEEE 8 (1988), 3–11. [24] Joseph A. Goguen and José Meseguer. 1982. Security Policies and Security Models. In 1982 IEEE Symposium on Security and Privacy, Oakland, CA, USA, April 26-28, 1982. 11–20. https://doi.org/10.1109/SP.1982.10014 [25] Dieter Gollmann, Pavel Gurikov, Alexander Isakov, Marina Krotofil, Jason Larsen, and Alexander Winnicki. 2015. Cyber-Physical Systems Security: Experimental Analysis of a Vinyl Acetate Monomer Plant. In Proceedings of the 1st ACM Workshop on Cyber-Physical System Security (CPSS ’15). ACM, New York, NY, USA, 1–12. https://doi.org/10.1145/2732198.2732208 [26] Dieter Gollmann and Marina Krotofil. 2016. Cyber-Physical Systems Security. Springer Berlin Heidelberg, Berlin, Heidelberg, 195–204. https://doi.org/10.1007/ 978-3-662-49301-4_14 [27] F. Gustafsson. 2000. Adaptive Filtering and Change Detection. John Wiley and Sons, LTD, West Sussex, Chichester, England. [28] Maurice Heemels and Bart De Schutter. 2007. Modeling and Control of Hybrid Dynamical Systems. Einhoven University of Technology. [29] G. Howser and B. McMillin. 2014. A Modal Model of Stuxnet Attacks on Cyber- physical Systems: A Matter of Trust. In 2014 Eighth International Conference on Software Security and Reliability (SERE). 225–234. https://doi.org/10.1109/SERE. 2014.36 [30] Marina Krotofil and Jason Larsen. 2015. Rocking the pocket book: Hacking chemical plants. In DefCon Conference, DEFCON. [31] Elias Kyriakides and Marios M. Polycarpou (Eds.). 2015. Intelligent Monitoring, Control, and Security of Critical Infrastructure Systems. Studies in Computational Intelligence, Vol. 565. Springer. [32] R. Langner. 2011. Stuxnet: Dissecting a Cyberwarfare Weapon. IEEE Security Privacy 9, 3 (May 2011), 49–51. https://doi.org/10.1109/MSP.2011.67 [33] D. Luenberger. 1966. Observers for multivariable systems. IEEE Trans. Automat. Control 11 (1966), 190–197. [34] Stephen E. McLaughlin, Saman A. Zonouz, Devin J. Pohly, and Patrick D. Mc- Daniel. 2014. A Trusted Safety Verifier for Process Controller Code. In 21st Annual Network and Distributed System Security Symposium, NDSS 2014, San Diego, California, USA, February 23-26, 2014. http://www.internetsociety.org/ doc/trusted-safety-verifier-process-controller-code [35] Y. Mo, R. Chabukswar, and B. Sinopoli. 2014. Detecting Integrity Attacks on SCADA Systems. IEEE Transactions on Control Systems Technology 22, 4 (July 2014), 1396–1407. https://doi.org/10.1109/TCST.2013.2280899 [36] Y. Mo and B. Sinopoli. 2009. Secure control against replay attacks. In 2009 47th Annual Allerton Conference on Communication, Control, and Computing (Allerton). 911–918. https://doi.org/10.1109/ALLERTON.2009.5394956 10 Preprint [37] Y. Mo and B. Sinopoli. 2010. False data injection attacks in control systems. In Proc. 1st Workshop Secure Control Syst., Stocholm, Sweeden. [38] Yilin Mo and Bruno Sinopoli. 2012. Integrity Attacks on Cyber-physical Systems. In Proceedings of the 1st International Conference on High Confidence Networked Systems (HiCoNS ’12). ACM, New York, NY, USA, 47–54. https://doi.org/10.1145/ 2185505.2185514 [39] Carlos Murguia and Justin Ruths. 2016. Characterization of a CUSUM Model- Based Sensor Attack Detector. In proceedings of the 55th IEEE Conference on Decision and Control (CDC). [40] Carlos Murguia and Justin Ruths. 2016. CUSUM and Chi-Squared Attack De- tection of Compromised Sensors. In proceedings of the IEEE Multi-Conference on Systems and Control (MSC). [41] Carlos Murguia, Nathan van de Wouw, and Justin Ruths. 2017. Reachable Sets of Hidden CPS Sensor Attacks: Analysis and Synthesis Tools. In IFAC 2017 World Congress. [42] Carlos Murguia, Nathan van de Wouw, and Justin Ruths. accepred, 2016. Reach- able Sets of Hidden CPS Sensor Attacks: Analysis and Synthesis Tools. In pro- ceedings of the IFAC World Congress. [43] E. Page. 1954. Continuous Inspection Schemes. Biometrika 41 (1954), 100–115. [44] F. Pasqualetti, F. Dörfler, and F. Bullo. 2013. Attack Detection and Identification in Cyber-Physical Systems. IEEE Trans. Automat. Control 58, 11 (Nov 2013), 2715–2729. https://doi.org/10.1109/TAC.2013.2266831 [45] P. J. Ramadge and W. M. Wonham. 1987. Supervisory Control of a Class of Discrete Event Processes. SIAM Journal on Control and Optimization 25, 1 (1987), 206–230. https://doi.org/10.1137/0325013 arXiv:https://doi.org/10.1137/0325013 [46] Marco Rocchetto and Nils Ole Tippenhauer. 2016. CPDY: Extending the Dolev-Yao Attacker with Physical-Layer Interactions. Springer International Publishing, Cham, 175–192. https://doi.org/10.1007/978-3-319-47846-3_12 [47] M. Ross. 2006. Introduction to Probability Models, Ninth Edition. Academic Press, Inc., Orlando, FL, USA. [48] John Rushby. 1992. Noninterference, Transitivity, and Channel-Control Security Policies. Technical Report. SRI International. http://www.csl.sri.com/papers/ csl-92-2/ [49] A. Sabelfeld and A. C. Myers. 2003. Language-based information-flow security. IEEE Journal on Selected Areas in Communications 21, 1 (Jan 2003), 5–19. https: //doi.org/10.1109/JSAC.2002.806121 [50] Roy S. Smith. 2011. A Decoupled Feedback Structure for Covertly Appropriating Networked Control Systems. IFAC Proceedings Volumes 44, 1 (2011), 90 – 95. https://doi.org/10.3182/20110828-6-IT-1002.01721 18th IFAC World Congress. [51] D. Sutherland. 1986. A Model of Information. In Proccedings of the National Computer Security Conference. 175–183. [52] David I. Urbina, Jairo A. Giraldo, Alvaro A. Cardenas, Nils Ole Tippenhauer, Junia Valente, Mustafa Faisal, Justin Ruths, Richard Candell, and Henrik Sandberg. 2016. Limiting the Impact of Stealthy Attacks on Industrial Control Systems. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS ’16). ACM, New York, NY, USA, 1092–1105. https://doi.org/10. 1145/2976749.2978388 [53] C.S. van Dobben de Bruyn. 1968. Cumulative sum tests : theory and practice. London : Griffin. [54] A. Wald. 1945. Sequential Tests of Statistical Hypotheses. Ann. Math. Statist. 16 (1945), 117–186. [55] S. Weerakkody and B. Sinopoli. 2015. Detecting integrity attacks on control systems using a moving target approach. In 2015 54th IEEE Conference on Decision and Control (CDC). 5820–5826. [56] S. Weerakkody, B. Sinopoli, S. Kar, and A. Datta. 2016. Information flow for security in control systems. In 2016 IEEE 55th Conference on Decision and Control (CDC). 5065–5072. https://doi.org/10.1109/CDC.2016.7799044 [57] A. Willsky. 1976. A survey of design methods for failure detection in dynamic systems. Automatica 12 (1976), 601 – 611. A APPENDIX A.1 Residuals and Hypothesis Testing Under certain properties of the matrices A and C (detectability [8]), the observer gain L can be designed such that E[ek] →0 as k →∞(where E[·] denotes expectation) in the absence of attacks. Moreover, under the same properties, the covariance matrix Pk := E[ekeT k ] converges to steady state (in the absence of attacks) in the sense that limk→∞Pk = P exists, see [8]. For δu k = δy k = 0 and appropriately selected L, it can be verified that the asymptotic covariance matrix P = limk→∞Pk is given by the solution P of the following Lyapunov equation: (F −LC)P(F −LC)T −P + R1 + LR2LT = 0, (47) where 0 denotes the zero matrix of appropriate dimensions. It is assumed that the system has reached steady state before an attack occurs. Consider the residual difference equation in (6). Then, it can be easily shown that, if there are no attacks, the mean of the residual is E[rk+1] = CE[ek+1] + E[ηk+1] = 0m×1, (48) and its asymptotic covariance matrix is given by Σ := E[rk+1rT k+1] = CPCT + R2, (49) where P is the asymptotic covariance matrix of the estimation error ek solution of (47) and R2 is the sensor noise covariance matrix. For this residual, we identify two hypotheses to be tested: H0 the normal mode (no attacks) and H1 the faulty mode (with faults/attacks). Then, we have H0 : ( E[rk] = 0, and E[rkrT k ] = Σ, H1 : ( E[rk] , 0, or E[rkrT k ] , Σ, The objective of hypothesis testing is to distinguish between H0 and H1. Several hypothesis testing techniques can be used to exam- ine the residual and subsequently detect faults/attacks. For instance, Sequential Probability Ratio Testing (SPRT) [54, 57], Cumulative Sum (CUSUM) [27, 43], Generalized Likelihood Ratio (GLR) testing [10], Compound Scalar Testing (CST) [23], etc. Each of these tech- niques has its own advantages and disadvantages depending on the scenario. Arguably, the most utilised one, due to its simplicity, is a particular case of CST, called bad-data change detection procedure. A.2 Attack Detection via Bad-Data Procedure The input to any detection procedure is a distance measure zk ∈R, with k ∈N, i.e., a measure of how deviated the estimator is from the sensor measurements. We employ distance measures any time we test to distinguish between H0 and H1. Here, we assume there is a dedicated detector on each sensor. Throughout the rest of this section we reserve the index i to denote the sensor/detector, i ∈I := {1, 2, . . . ,m}. Thus, we can partition the attacked output vector as ¯yk = col(¯yk,1, . . . , ¯yk,m) where ¯yk,i ∈R denotes the i-th entry of ¯yk ∈Rm; then ¯yk,i = Cixk + ηk,i + δy k,i, (50) with Ci being the i-th row of C and ηk,i and δy k,i denoting the i-th entries of ηk and δy k , respectively. The bad-data procedure uses the absolute value of the entries of the residual sequence as distance measures: zk,i := |rk,i | = |Ciek + ηk,i + δy k,i |. (51) If there are no attacks, rk,i follows a normal distribution with zero mean and variance σ2 i , where σ2 i denotes the i-th entry of the diagonal of the covariance matrix Σ. Hence, δy k = δu k = 0 implies that zk,i = |rk,i | follows a half-normal distribution [47] with E  |rk,i |  = √ 2 √π σi and var  |rk,i |  = σ2 i  1 −2 π . (52) Next, having presented the notion of distance measure, we intro- duce the bad-data procedure. 11 Preprint Bad-Data Procedure: If |rk,i | > αi, ˜ki = k, i ∈I. (53) Design parameter: threshold αi ∈R>0. Output: alarm time(s) ˜ki. The idea is that alarms are triggered if |rk,i | exceeds the threshold αi. The parameter αi is selected to satisfy a required false alarm rate A∗ i . The occurrence of an alarm in the bad-data when there are no attacks to the CPS is referred to as a false alarm. Operators need to tune this false alarm rate depending on the application. To do this, the thresholds αi must be selected to fulfill a desired false alarm rate A∗ i . Let A ∈[0, 1] denote the false alarm rate of the bad- data procedure defined as the expected proportion of observations which are false alarms, i.e., Ai := pr[zk,i ≥αi], where pr[·] denotes probability, see [53] and [4]. 12